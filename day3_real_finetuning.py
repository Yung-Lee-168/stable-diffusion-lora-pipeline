#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Day 3: Real Stable Diffusion v1.5 Fine-tuning Pipeline
çœŸæ­£çš„ SD v1.5 æ¨¡å‹å¾®èª¿è¨“ç·´æµç¨‹

ğŸ¯ é‡è¦ç‰¹æ€§ï¼š
- çœŸæ­£çš„æ¨¡å‹æ¬Šé‡æ›´æ–°èˆ‡åƒæ•¸è¨“ç·´
- åŸºæ–¼ FashionCLIP ç‰¹å¾µçš„ç›£ç£å­¸ç¿’
- LoRA (Low-Rank Adaptation) é«˜æ•ˆå¾®èª¿
- æ”¯æŒ Dreambooth å’Œ Custom Dataset è¨“ç·´
- è‡ªå‹•ä¿å­˜æª¢æŸ¥é»èˆ‡æ¢å¾©è¨“ç·´

æŠ€è¡“æ¶æ§‹:
ä¾†æºåœ– â†’ FashionCLIPç‰¹å¾µ â†’ æ–‡æœ¬åµŒå…¥ â†’ SD UNetå¾®èª¿ â†’ æ¬Šé‡æ›´æ–° â†’ æ¨¡å‹ä¿å­˜
"""

import os
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image
from datetime import datetime
import logging
from tqdm import tqdm
from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor
from diffusers import (
    StableDiffusionPipeline, 
    UNet2DConditionModel,
    DDPMScheduler,
    AutoencoderKL
)
from diffusers.loaders import AttnProcsLayers
from diffusers.models.attention_processor import LoRAAttnProcessor
import cv2
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings("ignore")

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FashionDataset(Dataset):
    """æ™‚å°šåœ–ç‰‡æ•¸æ“šé›†"""
    
    def __init__(self, image_paths, captions, tokenizer, size=512):
        self.image_paths = image_paths
        self.captions = captions
        self.tokenizer = tokenizer
        self.size = size
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # è¼‰å…¥åœ–ç‰‡
        image = Image.open(self.image_paths[idx]).convert("RGB")
        image = image.resize((self.size, self.size), Image.LANCZOS)
        image = np.array(image).astype(np.float32) / 255.0
        image = (image - 0.5) / 0.5  # æ­£è¦åŒ–åˆ° [-1, 1]
        image = torch.from_numpy(image).permute(2, 0, 1)
        
        # æ–‡æœ¬æ¨™è¨˜åŒ–
        text = self.captions[idx]
        text_inputs = self.tokenizer(
            text,
            padding="max_length",
            max_length=77,
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "images": image,
            "text_input_ids": text_inputs.input_ids[0],
            "text_attention_mask": text_inputs.attention_mask[0]
        }

class FashionSDFineTuner:
    """SD v1.5 Fashion Fine-tuning é¡"""
    
    def __init__(self, model_id="runwayml/stable-diffusion-v1-5"):
        """åˆå§‹åŒ–å¾®èª¿å™¨"""
        print("ğŸš€ åˆå§‹åŒ– Stable Diffusion v1.5 Fine-tuner...")
        
        self.model_id = model_id
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # è¨­å®šç›®éŒ„
        self.source_dir = "day1_results"
        self.output_dir = "day3_finetuning_results"
        self.checkpoint_dir = os.path.join(self.output_dir, "checkpoints")
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # è¨“ç·´é…ç½®
        self.config = {
            "learning_rate": 1e-4,
            "batch_size": 1,  # æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´
            "num_epochs": 50,
            "save_steps": 10,
            "validation_steps": 5,
            "max_grad_norm": 1.0,
            "use_lora": True,  # ä½¿ç”¨ LoRA é«˜æ•ˆå¾®èª¿
            "lora_rank": 4,
            "lora_alpha": 32,
            "image_size": 512,
            "mixed_precision": True  # æ··åˆç²¾åº¦è¨“ç·´
        }
        
        # åˆå§‹åŒ–æ¨¡å‹çµ„ä»¶
        self.init_models()
        
        # åˆå§‹åŒ– FashionCLIP
        self.init_fashion_clip()
        
        # è¨“ç·´ç‹€æ…‹
        self.global_step = 0
        self.epoch = 0
        
    def init_models(self):
        """åˆå§‹åŒ– SD æ¨¡å‹çµ„ä»¶"""
        print("ğŸ”§ åˆå§‹åŒ– Stable Diffusion çµ„ä»¶...")
        
        # è¼‰å…¥å®Œæ•´ç®¡é“ç”¨æ–¼æ¨ç†
        self.pipeline = StableDiffusionPipeline.from_pretrained(
            self.model_id,
            torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        ).to(self.device)
        
        # æå–å€‹åˆ¥çµ„ä»¶ç”¨æ–¼è¨“ç·´
        self.vae = self.pipeline.vae
        self.tokenizer = self.pipeline.tokenizer
        self.text_encoder = self.pipeline.text_encoder
        self.unet = self.pipeline.unet
        self.scheduler = DDPMScheduler.from_pretrained(self.model_id, subfolder="scheduler")
        
        # è¨­å®šæ¨¡å‹æ¨¡å¼
        self.vae.requires_grad_(False)
        self.text_encoder.requires_grad_(False)
        self.unet.requires_grad_(True)  # åªè¨“ç·´ UNet
        
        # å•Ÿç”¨ LoRA å¾®èª¿
        if self.config["use_lora"]:
            self.setup_lora()
            
        print("âœ… SD çµ„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def setup_lora(self):
        """è¨­ç½® LoRA å¾®èª¿"""
        print("ğŸ”§ è¨­ç½® LoRA å¾®èª¿...")
        
        # ç‚º UNet æ·»åŠ  LoRA é©é…å™¨
        lora_attn_procs = {}
        for name in self.unet.attn_processors.keys():
            cross_attention_dim = None if name.endswith("attn1.processor") else self.unet.config.cross_attention_dim
            if name.startswith("mid_block"):
                hidden_size = self.unet.config.block_out_channels[-1]
            elif name.startswith("up_blocks"):
                block_id = int(name[len("up_blocks.")])
                hidden_size = list(reversed(self.unet.config.block_out_channels))[block_id]
            elif name.startswith("down_blocks"):
                block_id = int(name[len("down_blocks.")])
                hidden_size = self.unet.config.block_out_channels[block_id]
            
            lora_attn_procs[name] = LoRAAttnProcessor(
                hidden_size=hidden_size,
                cross_attention_dim=cross_attention_dim,
                rank=self.config["lora_rank"],
                scale=self.config["lora_alpha"] / self.config["lora_rank"]
            )
        
        self.unet.set_attn_processor(lora_attn_procs)
        
        # åªè¨“ç·´ LoRA åƒæ•¸
        self.lora_layers = AttnProcsLayers(self.unet.attn_processors)
        self.trainable_params = list(self.lora_layers.parameters())
        
        print(f"ğŸ“Š LoRA å¯è¨“ç·´åƒæ•¸æ•¸é‡: {sum(p.numel() for p in self.trainable_params):,}")
        
    def init_fashion_clip(self):
        """åˆå§‹åŒ– FashionCLIP"""
        print("ğŸ”§ åˆå§‹åŒ– FashionCLIP...")
        
        try:
            # è¼‰å…¥ FashionCLIP æ¨¡å‹
            self.fashion_clip_model = CLIPModel.from_pretrained(
                "patrickjohncyh/fashion-clip",
                torch_dtype=torch.float32
            ).to(self.device)
            
            self.fashion_clip_processor = CLIPProcessor.from_pretrained(
                "patrickjohncyh/fashion-clip"
            )
            
            # è¨­ç‚ºè©•ä¼°æ¨¡å¼
            self.fashion_clip_model.eval()
            
            print("âœ… FashionCLIP åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ FashionCLIP åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def extract_fashion_features(self, image_path):
        """ä½¿ç”¨ FashionCLIP æå–ç‰¹å¾µ"""
        try:
            # è¼‰å…¥åœ–ç‰‡
            image = Image.open(image_path).convert("RGB")
            
            # è™•ç†åœ–ç‰‡
            inputs = self.fashion_clip_processor(
                images=image,
                return_tensors="pt"
            ).to(self.device)
            
            # æå–ç‰¹å¾µ
            with torch.no_grad():
                image_features = self.fashion_clip_model.get_image_features(**inputs)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            return image_features.cpu().numpy()
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾µæå–å¤±æ•—: {e}")
            return None
    
    def generate_fashion_caption(self, image_path):
        """åŸºæ–¼ FashionCLIP ç”Ÿæˆæ™‚å°šæè¿°"""
        # é€™è£¡å¯ä»¥æ ¹æ“š day2_csv_generator.py çš„é‚è¼¯
        # ç”Ÿæˆçµæ§‹åŒ–çš„æ™‚å°šæè¿°
        
        # ç°¡å–®ç¤ºä¾‹ - å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰è©²ä½¿ç”¨æ›´è¤‡é›œçš„é‚è¼¯
        captions = [
            "fashionable outfit with elegant styling",
            "trendy clothing with modern design",
            "stylish fashion piece with contemporary look",
            "sophisticated attire with refined details"
        ]
        
        # é€™è£¡å¯ä»¥åŠ å…¥æ›´è¤‡é›œçš„é‚è¼¯ä¾†ç”Ÿæˆå€‹æ€§åŒ–æè¿°
        return np.random.choice(captions)
    
    def prepare_dataset(self):
        """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
        print("ğŸ“ æº–å‚™è¨“ç·´æ•¸æ“šé›†...")
        
        # æœå°‹åœ–ç‰‡æ–‡ä»¶
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            image_files.extend([
                os.path.join(self.source_dir, f) 
                for f in os.listdir(self.source_dir) 
                if f.lower().endswith(ext)
            ])
        
        if not image_files:
            raise ValueError(f"åœ¨ {self.source_dir} ä¸­æ‰¾ä¸åˆ°åœ–ç‰‡æª”æ¡ˆ")
        
        print(f"ğŸ“· æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡")
        
        # ç”Ÿæˆæ¨™è¨»
        captions = []
        for image_path in tqdm(image_files, desc="ç”Ÿæˆæ¨™è¨»"):
            caption = self.generate_fashion_caption(image_path)
            captions.append(caption)
        
        # å‰µå»ºæ•¸æ“šé›†
        dataset = FashionDataset(
            image_paths=image_files,
            captions=captions,
            tokenizer=self.tokenizer,
            size=self.config["image_size"]
        )
        
        # å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=self.config["batch_size"],
            shuffle=True,
            num_workers=0  # Windows å»ºè­°è¨­ç‚º 0
        )
        
        print(f"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆï¼Œå…± {len(dataset)} å€‹æ¨£æœ¬")
        return dataloader
    
    def compute_loss(self, batch):
        """è¨ˆç®—è¨“ç·´æå¤±"""
        images = batch["images"].to(self.device)
        text_input_ids = batch["text_input_ids"].to(self.device)
        
        # ç·¨ç¢¼æ–‡æœ¬
        text_embeddings = self.text_encoder(text_input_ids)[0]
        
        # å°‡åœ–ç‰‡ç·¨ç¢¼åˆ°æ½›åœ¨ç©ºé–“
        with torch.no_grad():
            latents = self.vae.encode(images).latent_dist.sample()
            latents = latents * self.vae.config.scaling_factor
        
        # æ·»åŠ å™ªè²
        noise = torch.randn_like(latents)
        timesteps = torch.randint(0, self.scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()
        noisy_latents = self.scheduler.add_noise(latents, noise, timesteps)
        
        # é æ¸¬å™ªè²
        model_pred = self.unet(noisy_latents, timesteps, text_embeddings).sample
        
        # è¨ˆç®—æå¤±
        if self.scheduler.config.prediction_type == "epsilon":
            target = noise
        elif self.scheduler.config.prediction_type == "v_prediction":
            target = self.scheduler.get_velocity(latents, noise, timesteps)
        else:
            raise ValueError(f"Unknown prediction type {self.scheduler.config.prediction_type}")
        
        loss = nn.functional.mse_loss(model_pred.float(), target.float(), reduction="mean")
        
        return loss
    
    def train_epoch(self, dataloader, optimizer, epoch):
        """è¨“ç·´ä¸€å€‹ epoch"""
        self.unet.train()
        epoch_loss = 0.0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        
        for step, batch in enumerate(progress_bar):
            # å‰å‘å‚³æ’­
            loss = self.compute_loss(batch)
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config["max_grad_norm"] > 0:
                torch.nn.utils.clip_grad_norm_(self.trainable_params, self.config["max_grad_norm"])
            
            # æ›´æ–°åƒæ•¸
            optimizer.step()
            optimizer.zero_grad()
            
            # è¨˜éŒ„æå¤±
            epoch_loss += loss.item()
            self.global_step += 1
            
            # æ›´æ–°é€²åº¦æ¢
            progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            # ä¿å­˜æª¢æŸ¥é»
            if self.global_step % self.config["save_steps"] == 0:
                self.save_checkpoint(epoch, self.global_step)
        
        avg_loss = epoch_loss / len(dataloader)
        print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æå¤±: {avg_loss:.4f}")
        
        return avg_loss
    
    def save_checkpoint(self, epoch, step):
        """ä¿å­˜è¨“ç·´æª¢æŸ¥é»"""
        checkpoint_path = os.path.join(self.checkpoint_dir, f"checkpoint_epoch_{epoch+1}_step_{step}.pt")
        
        checkpoint = {
            "epoch": epoch,
            "global_step": step,
            "lora_state_dict": self.lora_layers.state_dict() if self.config["use_lora"] else self.unet.state_dict(),
            "config": self.config
        }
        
        torch.save(checkpoint, checkpoint_path)
        print(f"ğŸ’¾ æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_path}")
    
    def save_final_model(self):
        """ä¿å­˜æœ€çµ‚æ¨¡å‹"""
        print("ğŸ’¾ ä¿å­˜æœ€çµ‚æ¨¡å‹...")
        
        # ä¿å­˜ LoRA æ¬Šé‡
        if self.config["use_lora"]:
            lora_path = os.path.join(self.output_dir, "fashion_lora_weights.pt")
            torch.save(self.lora_layers.state_dict(), lora_path)
            print(f"âœ… LoRA æ¬Šé‡å·²ä¿å­˜: {lora_path}")
        
        # ä¿å­˜å®Œæ•´ç®¡é“
        model_path = os.path.join(self.output_dir, "fashion_sd_model")
        self.pipeline.save_pretrained(model_path)
        print(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def validate_model(self, validation_prompts=None):
        """é©—è­‰æ¨¡å‹æ•ˆæœ"""
        print("ğŸ§ª é©—è­‰æ¨¡å‹æ•ˆæœ...")
        
        if validation_prompts is None:
            validation_prompts = [
                "elegant fashion outfit with modern styling",
                "trendy casual wear with contemporary design",
                "sophisticated formal attire with refined details",
                "stylish street fashion with creative elements"
            ]
        
        self.pipeline.unet.eval()
        
        validation_dir = os.path.join(self.output_dir, "validation_images")
        os.makedirs(validation_dir, exist_ok=True)
        
        for i, prompt in enumerate(validation_prompts):
            try:
                # ç”Ÿæˆåœ–ç‰‡
                with torch.no_grad():
                    image = self.pipeline(
                        prompt=prompt,
                        num_inference_steps=20,
                        guidance_scale=7.5,
                        height=512,
                        width=512
                    ).images[0]
                
                # ä¿å­˜åœ–ç‰‡
                image_path = os.path.join(validation_dir, f"validation_{i+1}_{self.global_step}.png")
                image.save(image_path)
                print(f"âœ… é©—è­‰åœ–ç‰‡å·²ä¿å­˜: {image_path}")
                
            except Exception as e:
                print(f"âŒ é©—è­‰åœ–ç‰‡ {i+1} ç”Ÿæˆå¤±æ•—: {e}")
    
    def train(self):
        """åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹"""
        print("ğŸš€ é–‹å§‹ Stable Diffusion v1.5 Fine-tuning è¨“ç·´")
        print("=" * 60)
        
        try:
            # æº–å‚™æ•¸æ“š
            dataloader = self.prepare_dataset()
            
            # è¨­å®šå„ªåŒ–å™¨
            if self.config["use_lora"]:
                optimizer = optim.AdamW(self.trainable_params, lr=self.config["learning_rate"])
            else:
                optimizer = optim.AdamW(self.unet.parameters(), lr=self.config["learning_rate"])
            
            # è¨“ç·´å¾ªç’°
            train_losses = []
            
            for epoch in range(self.config["num_epochs"]):
                print(f"\nğŸ”„ é–‹å§‹ Epoch {epoch+1}/{self.config['num_epochs']}")
                
                # è¨“ç·´ä¸€å€‹ epoch
                avg_loss = self.train_epoch(dataloader, optimizer, epoch)
                train_losses.append(avg_loss)
                
                # é©—è­‰æ¨¡å‹
                if (epoch + 1) % self.config["validation_steps"] == 0:
                    self.validate_model()
                
                self.epoch = epoch
            
            # ä¿å­˜æœ€çµ‚æ¨¡å‹
            self.save_final_model()
            
            # ç”Ÿæˆè¨“ç·´å ±å‘Š
            self.generate_training_report(train_losses)
            
            print(f"\nğŸ‰ è¨“ç·´å®Œæˆï¼çµæœä¿å­˜åœ¨: {self.output_dir}")
            
        except Exception as e:
            print(f"âŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
    
    def generate_training_report(self, train_losses):
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(self.output_dir, f"finetuning_report_{timestamp}.md")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Stable Diffusion v1.5 Fashion Fine-tuning å ±å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ™‚é–“**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## è¨“ç·´é…ç½®\n\n")
            
            for key, value in self.config.items():
                f.write(f"- **{key}**: {value}\n")
            
            f.write("\n## è¨“ç·´çµæœ\n\n")
            f.write(f"- **ç¸½ Epochs**: {len(train_losses)}\n")
            f.write(f"- **ç¸½è¨“ç·´æ­¥æ•¸**: {self.global_step}\n")
            f.write(f"- **æœ€çµ‚æå¤±**: {train_losses[-1]:.4f}\n")
            f.write(f"- **æœ€ä½æå¤±**: {min(train_losses):.4f}\n")
            
            f.write("\n## æå¤±æ›²ç·š\n\n")
            for i, loss in enumerate(train_losses):
                f.write(f"Epoch {i+1}: {loss:.4f}\n")
        
        print(f"ğŸ“„ è¨“ç·´å ±å‘Šå·²ä¿å­˜: {report_path}")

def main():
    """ä¸»å‡½æ•¸"""
    try:
        # æª¢æŸ¥ CUDA å¯ç”¨æ€§
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        else:
            print("âš ï¸  ä½¿ç”¨ CPU é€²è¡Œè¨“ç·´ï¼ˆé€Ÿåº¦è¼ƒæ…¢ï¼‰")
        
        # å‰µå»ºå¾®èª¿å™¨
        finetuner = FashionSDFineTuner()
        
        # é–‹å§‹è¨“ç·´
        finetuner.train()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
