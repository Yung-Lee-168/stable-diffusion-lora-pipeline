#!/usr/bin/env python3
"""
è§£æ±º PyTorch ç‰ˆæœ¬å®‰å…¨æ€§å•é¡Œ
ä½¿ç”¨ safetensors æ ¼å¼è¼‰å…¥æ¨¡å‹ï¼Œé¿å… torch.load å®‰å…¨æ€§å•é¡Œ
"""

import os
import time
from transformers import CLIPModel, CLIPProcessor

def download_models_with_safetensors():
    print("ğŸ”§ ä½¿ç”¨ SafeTensors æ ¼å¼è§£æ±º PyTorch å®‰å…¨æ€§å•é¡Œ")
    print("=" * 60)
    
    try:
        import torch
        print(f"ç•¶å‰ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å‡ç´š
        torch_version = tuple(map(int, torch.__version__.split('.')[:2]))
        if torch_version < (2, 6):
            print("âš ï¸ PyTorch ç‰ˆæœ¬è¼ƒèˆŠï¼Œä½¿ç”¨ safetensors æ ¼å¼è¼‰å…¥")
        else:
            print("âœ… PyTorch ç‰ˆæœ¬ç¬¦åˆè¦æ±‚")
    except:
        print("âŒ ç„¡æ³•æª¢æŸ¥ PyTorch ç‰ˆæœ¬")
    
    print()
    
    # ä¸‹è¼‰æ¨™æº– CLIP - ä½¿ç”¨ safetensors
    print("1ï¸âƒ£ ä¸‹è¼‰æ¨™æº– CLIP (ä½¿ç”¨ SafeTensors æ ¼å¼)")
    try:
        start_time = time.time()
        print("   ğŸ“¥ æ­£åœ¨ä¸‹è¼‰æ¨¡å‹æª”æ¡ˆ (SafeTensors æ ¼å¼)...")
        
        # å¼·åˆ¶ä½¿ç”¨ safetensors æ ¼å¼
        model = CLIPModel.from_pretrained(
            "openai/clip-vit-base-patch32",
            use_safetensors=True,  # å¼·åˆ¶ä½¿ç”¨ safetensors
            trust_remote_code=False  # å®‰å…¨æ€§è¨­ç½®
        )
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
        download_time = time.time() - start_time
        param_count = sum(p.numel() for p in model.parameters()) / 1e6
        
        print(f"   âœ… æ¨™æº– CLIP ä¸‹è¼‰æˆåŠŸï¼")
        print(f"   â±ï¸ ä¸‹è¼‰æ™‚é–“: {download_time:.1f} ç§’")
        print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {param_count:.1f}M")
        print(f"   ğŸ”’ ä½¿ç”¨ SafeTensors æ ¼å¼ (æ›´å®‰å…¨)")
        
        # æ¸¬è©¦åŠŸèƒ½
        from PIL import Image
        test_image = Image.new('RGB', (224, 224), color='red')
        test_texts = ["a red image", "a blue image"]
        
        inputs = processor(text=test_texts, images=test_image, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
        
        print(f"   ğŸ§ª åŠŸèƒ½æ¸¬è©¦: é€šé")
        
        # æ¸…ç†è¨˜æ†¶é«”
        del model, processor
        
    except Exception as e:
        print(f"   âŒ æ¨™æº– CLIP ä¸‹è¼‰å¤±æ•—: {e}")
        
        # å˜—è©¦å‚™ç”¨æ–¹æ¡ˆ
        try:
            print("   ğŸ”„ å˜—è©¦å‚™ç”¨ä¸‹è¼‰æ–¹æ¡ˆ...")
            model = CLIPModel.from_pretrained(
                "openai/clip-vit-base-patch32",
                torch_dtype=torch.float32,  # ä½¿ç”¨ float32
                low_cpu_mem_usage=True      # ä½è¨˜æ†¶é«”ä½¿ç”¨
            )
            processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            print(f"   âœ… å‚™ç”¨æ–¹æ¡ˆæˆåŠŸï¼")
            del model, processor
        except Exception as e2:
            print(f"   âŒ å‚™ç”¨æ–¹æ¡ˆä¹Ÿå¤±æ•—: {e2}")
            return False
    
    print()
    
    # ä¸‹è¼‰ FashionCLIP - ä½¿ç”¨ safetensors
    print("2ï¸âƒ£ ä¸‹è¼‰ FashionCLIP (ä½¿ç”¨ SafeTensors æ ¼å¼)")
    try:
        start_time = time.time()
        print("   ğŸ“¥ æ­£åœ¨ä¸‹è¼‰å°ˆæ¥­æ™‚å°šæ¨¡å‹...")
        
        # å˜—è©¦è¼‰å…¥ FashionCLIP
        try:
            fashion_model = CLIPModel.from_pretrained(
                "patrickjohncyh/fashion-clip",
                use_safetensors=True,
                trust_remote_code=False
            )
            fashion_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
            
            download_time = time.time() - start_time
            fashion_param_count = sum(p.numel() for p in fashion_model.parameters()) / 1e6
            
            print(f"   âœ… FashionCLIP ä¸‹è¼‰æˆåŠŸï¼")
            print(f"   â±ï¸ ä¸‹è¼‰æ™‚é–“: {download_time:.1f} ç§’")
            print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {fashion_param_count:.1f}M")
            print(f"   ğŸ”’ ä½¿ç”¨ SafeTensors æ ¼å¼")
            print(f"   ğŸ‘— å°ˆæ¥­é ˜åŸŸ: æ™‚å°šåœ–ç‰‡åˆ†æ")
            
            del fashion_model, fashion_processor
            
        except Exception as e:
            print(f"   âš ï¸ FashionCLIP ä¸‹è¼‰å¤±æ•—: {e}")
            print(f"   ğŸ’¡ å°‡åœ¨æ¸¬è©¦ä¸­ä½¿ç”¨æ¨™æº– CLIP ä½œç‚ºå‚™ç”¨")
        
    except Exception as e:
        print(f"   âŒ FashionCLIP è™•ç†å¤±æ•—: {e}")
    
    print()
    print("=" * 60)
    print("ğŸ‰ æ¨¡å‹æº–å‚™å®Œæˆï¼")
    print("=" * 60)
    
    return True

def create_updated_test_script():
    """å‰µå»ºæ›´æ–°ç‰ˆçš„æ¸¬è©¦è…³æœ¬ï¼Œè§£æ±º PyTorch å®‰å…¨æ€§å•é¡Œ"""
    
    updated_content = '''#!/usr/bin/env python3
"""
å®‰å…¨ç‰ˆç¬¬2å¤©æ¸¬è©¦ï¼šCLIP vs FashionCLIP æ¯”è¼ƒ
è§£æ±º PyTorch å®‰å…¨æ€§å•é¡Œï¼Œä½¿ç”¨ SafeTensors æ ¼å¼
"""

import requests
import json
import base64
import os
from datetime import datetime
from PIL import Image
import numpy as np

class SafeEnhancedDay2Tester:
    def __init__(self):
        self.api_url = "http://localhost:7860"
        self.output_dir = "day2_enhanced_results"
        os.makedirs(self.output_dir, exist_ok=True)
        
    def install_requirements(self):
        """æª¢æŸ¥ä¸¦å®‰è£å¿…è¦çš„å¥—ä»¶"""
        print("ğŸ” æª¢æŸ¥æ¨¡å‹ä¾è³´...")
        
        try:
            import torch
            import transformers
            print("âœ… åŸºç¤å¥—ä»¶å·²å®‰è£")
            print(f"ğŸ”§ PyTorch ç‰ˆæœ¬: {torch.__version__}")
        except ImportError as e:
            print(f"âŒ ç¼ºå°‘åŸºç¤å¥—ä»¶: {e}")
            return False
            
        return True
    
    def load_standard_clip_safe(self):
        """å®‰å…¨è¼‰å…¥æ¨™æº– CLIP æ¨¡å‹ - ä½¿ç”¨ SafeTensors"""
        try:
            from transformers import CLIPProcessor, CLIPModel
            import torch
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ“¥ å®‰å…¨è¼‰å…¥æ¨™æº– CLIP (è¨­å‚™: {device})...")
            
            # ä½¿ç”¨ SafeTensors æ ¼å¼ï¼Œé¿å… torch.load å®‰å…¨æ€§å•é¡Œ
            if device == "cuda":
                model = CLIPModel.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    torch_dtype=torch.float16,
                    use_safetensors=True,       # ä½¿ç”¨å®‰å…¨æ ¼å¼
                    trust_remote_code=False     # å®‰å…¨æ€§è¨­ç½®
                )
            else:
                model = CLIPModel.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    use_safetensors=True,
                    trust_remote_code=False
                )
                
            processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            model.to(device)
            
            print("âœ… æ¨™æº– CLIP æ¨¡å‹å®‰å…¨è¼‰å…¥æˆåŠŸ")
            return model, processor, "standard_clip"
            
        except Exception as e:
            print(f"âŒ æ¨™æº– CLIP æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨è¼ƒèˆŠçš„è¼‰å…¥æ–¹å¼
            try:
                print("ğŸ”„ å˜—è©¦å‚™ç”¨è¼‰å…¥æ–¹æ¡ˆ...")
                model = CLIPModel.from_pretrained(
                    "openai/clip-vit-base-patch32",
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True
                )
                processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                model.to(device)
                print("âœ… å‚™ç”¨æ–¹æ¡ˆè¼‰å…¥æˆåŠŸ")
                return model, processor, "standard_clip"
            except Exception as e2:
                print(f"âŒ å‚™ç”¨æ–¹æ¡ˆä¹Ÿå¤±æ•—: {e2}")
                return None, None, None
    
    def load_fashion_clip_safe(self):
        """å®‰å…¨è¼‰å…¥ FashionCLIP æ¨¡å‹"""
        try:
            from transformers import CLIPModel, CLIPProcessor
            import torch
            
            fashion_models = [
                "patrickjohncyh/fashion-clip",
                "openai/clip-vit-base-patch32"  # å‚™ç”¨
            ]
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"ğŸ® ä½¿ç”¨è¨­å‚™: {device}")
            
            for model_name in fashion_models:
                try:
                    print(f"ğŸ“¥ å®‰å…¨è¼‰å…¥ {model_name}...")
                    
                    if device == "cuda":
                        model = CLIPModel.from_pretrained(
                            model_name,
                            torch_dtype=torch.float16,
                            use_safetensors=True,
                            trust_remote_code=False,
                            device_map="auto"
                        )
                    else:
                        model = CLIPModel.from_pretrained(
                            model_name,
                            use_safetensors=True,
                            trust_remote_code=False
                        )
                    
                    processor = CLIPProcessor.from_pretrained(model_name)
                    model.to(device)
                    
                    print(f"âœ… FashionCLIP å®‰å…¨è¼‰å…¥æˆåŠŸ: {model_name}")
                    print(f"   è¨­å‚™: {device}")
                    print(f"   ç²¾åº¦: {'float16' if device == 'cuda' else 'float32'}")
                    print(f"   ğŸ”’ ä½¿ç”¨ SafeTensors æ ¼å¼")
                    
                    return model, processor, "fashion_clip"
                    
                except Exception as e:
                    print(f"âš ï¸ è¼‰å…¥ {model_name} å¤±æ•—: {e}")
                    continue
                    
            print("âš ï¸ å°ˆæ¥­ FashionCLIP ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨™æº– CLIP")
            return self.load_standard_clip_safe()
            
        except Exception as e:
            print(f"âŒ FashionCLIP è¼‰å…¥å¤±æ•—: {e}")
            return None, None, None

# å…¶ä»–æ–¹æ³•ä¿æŒä¸è®Š...
'''
    
    with open("day2_safe_test.py", "w", encoding="utf-8") as f:
        f.write(updated_content)
    
    print("ğŸ“ å·²å‰µå»ºå®‰å…¨ç‰ˆæ¸¬è©¦è…³æœ¬: day2_safe_test.py")

if __name__ == "__main__":
    print("ğŸ”§ PyTorch å®‰å…¨æ€§å•é¡Œè§£æ±ºæ–¹æ¡ˆ")
    print("=" * 60)
    
    # ä¸‹è¼‰æ¨¡å‹
    success = download_models_with_safetensors()
    
    if success:
        print("\nâœ… ç¾åœ¨å¯ä»¥åŸ·è¡Œå®‰å…¨ç‰ˆæ¸¬è©¦:")
        print("   python day2_enhanced_test.py")
        print("\nğŸ’¡ å¦‚æœä»æœ‰å•é¡Œï¼Œå¯ä»¥å˜—è©¦:")
        print("   pip install torch>=2.6.0 --upgrade")
    else:
        print("\nâŒ æ¨¡å‹ä¸‹è¼‰é‡åˆ°å•é¡Œ")
        print("ğŸ’¡ å»ºè­°å‡ç´š PyTorch:")
        print("   pip install torch>=2.6.0 transformers --upgrade")
