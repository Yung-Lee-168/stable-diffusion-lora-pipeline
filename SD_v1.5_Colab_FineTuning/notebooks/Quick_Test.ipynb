{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41127199",
   "metadata": {},
   "source": [
    "# ğŸš€ Quick Test for SD v1.5 Fine-tuning\n",
    "\n",
    "å¿«é€Ÿæ¸¬è©¦ Notebook - é©—è­‰ç’°å¢ƒå’ŒåŸºæœ¬åŠŸèƒ½\n",
    "\n",
    "## ğŸ¯ ç”¨é€”\n",
    "- å¿«é€Ÿé©—è­‰ç’°å¢ƒè¨­ç½®\n",
    "- æ¸¬è©¦åŸºæœ¬åŠŸèƒ½\n",
    "- æª¢æŸ¥ GPU å’Œæ¨¡å‹è¼‰å…¥\n",
    "- ä¸éœ€è¦å¯¦éš›è¨“ç·´\n",
    "\n",
    "## â±ï¸ åŸ·è¡Œæ™‚é–“\n",
    "å¤§ç´„ 5-10 åˆ†é˜\n",
    "\n",
    "## ğŸ“‹ å‰ææ¢ä»¶\n",
    "- å·²åŸ·è¡Œ Environment_Setup.ipynb\n",
    "- GPU é‹è¡Œæ™‚å·²å•Ÿç”¨\n",
    "- ä¾è³´å¥—ä»¶å·²å®‰è£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc8e18",
   "metadata": {},
   "source": [
    "## ğŸ” æ­¥é©Ÿ 1: åŸºæœ¬ç’°å¢ƒæª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b574e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ç’°å¢ƒæª¢æŸ¥\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ğŸ” å¿«é€Ÿç’°å¢ƒæª¢æŸ¥...\")\n",
    "print(f\"Python ç‰ˆæœ¬: {sys.version}\")\n",
    "\n",
    "# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ä¸­\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"é‹è¡Œç’°å¢ƒ: {'ğŸŒ Google Colab' if IN_COLAB else 'ğŸ’» Local'}\")\n",
    "\n",
    "# æª¢æŸ¥é—œéµå¥—ä»¶\n",
    "required_packages = ['torch', 'transformers', 'diffusers', 'accelerate', 'peft']\n",
    "missing_packages = []\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"âœ… {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package} æœªå®‰è£\")\n",
    "        missing_packages.append(package)\n",
    "\n",
    "if missing_packages:\n",
    "    print(f\"\\nâŒ ç¼ºå°‘å¥—ä»¶: {', '.join(missing_packages)}\")\n",
    "    print(\"è«‹å…ˆåŸ·è¡Œ Environment_Setup.ipynb\")\n",
    "else:\n",
    "    print(\"\\nâœ… æ‰€æœ‰å¿…è¦å¥—ä»¶å·²å®‰è£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5ad7a",
   "metadata": {},
   "source": [
    "## ğŸ”§ æ­¥é©Ÿ 2: GPU å’Œ CUDA æª¢æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994258d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU å’Œ CUDA æª¢æŸ¥\n",
    "import torch\n",
    "\n",
    "print(\"ğŸ”§ GPU å’Œ CUDA æª¢æŸ¥...\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "# æª¢æŸ¥ CUDA\n",
    "if torch.cuda.is_available():\n",
    "    print(\"âœ… CUDA å¯ç”¨\")\n",
    "    print(f\"   CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"   GPU æ•¸é‡: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # GPU è©³ç´°ä¿¡æ¯\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(f\"   GPU åç¨±: {gpu_name}\")\n",
    "    print(f\"   GPU è¨˜æ†¶é«”: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # æ¸¬è©¦ GPU é‹ç®—\n",
    "    try:\n",
    "        test_tensor = torch.randn(1000, 1000).cuda()\n",
    "        result = torch.matmul(test_tensor, test_tensor)\n",
    "        print(\"âœ… GPU é‹ç®—æ¸¬è©¦é€šé\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GPU é‹ç®—æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ CUDA ä¸å¯ç”¨\")\n",
    "    print(\"è«‹å•Ÿç”¨ GPU é‹è¡Œæ™‚: Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd57ff35",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ­¥é©Ÿ 3: æ¨¡å‹è¼‰å…¥æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è¼‰å…¥æ¸¬è©¦\n",
    "print(\"ğŸ“¦ æ¨¡å‹è¼‰å…¥æ¸¬è©¦...\")\n",
    "print(\"é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ä¸‹è¼‰æ¨¡å‹...\")\n",
    "\n",
    "try:\n",
    "    # æ¸¬è©¦ Tokenizer\n",
    "    from transformers import CLIPTokenizer\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"tokenizer\")\n",
    "    print(\"âœ… Tokenizer è¼‰å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # æ¸¬è©¦ Text Encoder\n",
    "    from transformers import CLIPTextModel\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\")\n",
    "    print(\"âœ… Text Encoder è¼‰å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # æ¸¬è©¦ VAE (è¼•é‡ç´šæ¸¬è©¦)\n",
    "    from diffusers import AutoencoderKL\n",
    "    vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\")\n",
    "    print(\"âœ… VAE è¼‰å…¥æˆåŠŸ\")\n",
    "    \n",
    "    # æ¸¬è©¦ UNet (é€™æ˜¯æœ€å¤§çš„æ¨¡å‹)\n",
    "    from diffusers import UNet2DConditionModel\n",
    "    unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
    "    print(\"âœ… UNet è¼‰å…¥æˆåŠŸ\")\n",
    "    \n",
    "    print(\"\\nğŸ‰ æ‰€æœ‰æ¨¡å‹çµ„ä»¶è¼‰å…¥æˆåŠŸï¼\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n",
    "    print(\"\\nå¯èƒ½çš„åŸå› :\")\n",
    "    print(\"1. ç¶²è·¯é€£æ¥å•é¡Œ\")\n",
    "    print(\"2. è¨˜æ†¶é«”ä¸è¶³\")\n",
    "    print(\"3. ä¾è³´å¥—ä»¶å•é¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcac0e3d",
   "metadata": {},
   "source": [
    "## ğŸ” æ­¥é©Ÿ 4: LoRA å’Œ PEFT æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5292cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA å’Œ PEFT æ¸¬è©¦\n",
    "print(\"ğŸ” LoRA å’Œ PEFT æ¸¬è©¦...\")\n",
    "\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    \n",
    "    # å‰µå»º LoRA é…ç½®\n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=4,\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "        lora_dropout=0.1,\n",
    "    )\n",
    "    print(\"âœ… LoRA é…ç½®å‰µå»ºæˆåŠŸ\")\n",
    "    \n",
    "    # æ¸¬è©¦æ‡‰ç”¨ LoRA åˆ°æ¨¡å‹\n",
    "    if 'unet' in locals():\n",
    "        # å‰µå»º UNet çš„å‰¯æœ¬é€²è¡Œæ¸¬è©¦\n",
    "        test_unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
    "        peft_model = get_peft_model(test_unet, lora_config)\n",
    "        \n",
    "        # è¨ˆç®—åƒæ•¸æ•¸é‡\n",
    "        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "        \n",
    "        print(f\"âœ… LoRA æ‡‰ç”¨æˆåŠŸ\")\n",
    "        print(f\"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}\")\n",
    "        print(f\"   ç¸½åƒæ•¸: {total_params:,}\")\n",
    "        print(f\"   å¯è¨“ç·´æ¯”ä¾‹: {trainable_params/total_params*100:.2f}%\")\n",
    "        \n",
    "        # æ¸…ç†è¨˜æ†¶é«”\n",
    "        del test_unet, peft_model\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ UNet æœªè¼‰å…¥ï¼Œè·³é LoRA æ‡‰ç”¨æ¸¬è©¦\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ LoRA æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    print(\"é€™å¯èƒ½å½±éŸ¿é«˜æ•ˆå¾®èª¿åŠŸèƒ½\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2b921",
   "metadata": {},
   "source": [
    "## ğŸ¨ æ­¥é©Ÿ 5: åŸºæœ¬ç”Ÿæˆæ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºæœ¬ç”Ÿæˆæ¸¬è©¦\n",
    "print(\"ğŸ¨ åŸºæœ¬ç”Ÿæˆæ¸¬è©¦...\")\n",
    "print(\"é€™å°‡æ¸¬è©¦å®Œæ•´çš„ SD ç®¡é“...\")\n",
    "\n",
    "try:\n",
    "    from diffusers import StableDiffusionPipeline\n",
    "    import torch\n",
    "    \n",
    "    # å‰µå»ºç®¡é“\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        safety_checker=None,\n",
    "        requires_safety_checker=False\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "    \n",
    "    print(\"âœ… SD ç®¡é“å‰µå»ºæˆåŠŸ\")\n",
    "    \n",
    "    # ç”Ÿæˆæ¸¬è©¦åœ–ç‰‡\n",
    "    prompt = \"a simple red apple on a white background\"\n",
    "    print(f\"ğŸ¯ æ¸¬è©¦æç¤ºè©: {prompt}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe(\n",
    "            prompt,\n",
    "            num_inference_steps=20,\n",
    "            guidance_scale=7.5,\n",
    "            width=512,\n",
    "            height=512\n",
    "        ).images[0]\n",
    "    \n",
    "    print(\"âœ… åœ–ç‰‡ç”ŸæˆæˆåŠŸ\")\n",
    "    \n",
    "    # é¡¯ç¤ºåœ–ç‰‡\n",
    "    from IPython.display import display\n",
    "    display(image)\n",
    "    \n",
    "    # ä¿å­˜åœ–ç‰‡\n",
    "    image.save(\"test_generation.png\")\n",
    "    print(\"ğŸ’¾ æ¸¬è©¦åœ–ç‰‡å·²ä¿å­˜ç‚º test_generation.png\")\n",
    "    \n",
    "    # æ¸…ç†è¨˜æ†¶é«”\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    print(\"å¯èƒ½çš„åŸå› :\")\n",
    "    print(\"1. GPU è¨˜æ†¶é«”ä¸è¶³\")\n",
    "    print(\"2. æ¨¡å‹è¼‰å…¥å•é¡Œ\")\n",
    "    print(\"3. ä¾è³´å¥—ä»¶å•é¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63598658",
   "metadata": {},
   "source": [
    "## ğŸ§  æ­¥é©Ÿ 6: è¨˜æ†¶é«”å’Œæ€§èƒ½æ¸¬è©¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab85301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨˜æ†¶é«”å’Œæ€§èƒ½æ¸¬è©¦\n",
    "print(\"ğŸ§  è¨˜æ†¶é«”å’Œæ€§èƒ½æ¸¬è©¦...\")\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# æ¸…ç†è¨˜æ†¶é«”\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨\n",
    "if torch.cuda.is_available():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    \n",
    "    print(f\"ğŸ“Š GPU è¨˜æ†¶é«”ç‹€æ…‹:\")\n",
    "    print(f\"   ç¸½è¨˜æ†¶é«”: {total_memory:.1f} GB\")\n",
    "    print(f\"   å·²åˆ†é…: {allocated_memory:.1f} GB\")\n",
    "    print(f\"   å·²ç·©å­˜: {cached_memory:.1f} GB\")\n",
    "    print(f\"   å¯ç”¨: {total_memory - cached_memory:.1f} GB\")\n",
    "    \n",
    "    # è¨˜æ†¶é«”ä½¿ç”¨ç‡\n",
    "    usage_percent = (cached_memory / total_memory) * 100\n",
    "    print(f\"   ä½¿ç”¨ç‡: {usage_percent:.1f}%\")\n",
    "    \n",
    "    if usage_percent > 80:\n",
    "        print(\"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒé«˜ï¼Œå»ºè­°æ¸…ç†\")\n",
    "    else:\n",
    "        print(\"âœ… è¨˜æ†¶é«”ä½¿ç”¨ç‡æ­£å¸¸\")\n",
    "        \n",
    "# ç³»çµ±è³‡æº\n",
    "try:\n",
    "    import psutil\n",
    "    ram_usage = psutil.virtual_memory().percent\n",
    "    print(f\"\\nğŸ’» ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨: {ram_usage:.1f}%\")\n",
    "    \n",
    "    if ram_usage > 85:\n",
    "        print(\"âš ï¸ ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨ç‡è¼ƒé«˜\")\n",
    "    else:\n",
    "        print(\"âœ… ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨æ­£å¸¸\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ ç„¡æ³•æª¢æŸ¥ç³»çµ±è¨˜æ†¶é«”\")\n",
    "\n",
    "print(\"\\nğŸ§¹ åŸ·è¡Œè¨˜æ†¶é«”æ¸…ç†...\")\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"âœ… GPU è¨˜æ†¶é«”æ¸…ç†å®Œæˆ\")\n",
    "    \n",
    "print(\"âœ… è¨˜æ†¶é«”å’Œæ€§èƒ½æ¸¬è©¦å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f606a",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æ­¥é©Ÿ 7: æœ€çµ‚å ±å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd974a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€çµ‚å ±å‘Š\n",
    "print(\"ğŸ“‹ æœ€çµ‚æ¸¬è©¦å ±å‘Š\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ”¶é›†æ¸¬è©¦çµæœ\n",
    "test_results = {\n",
    "    \"ç’°å¢ƒæª¢æŸ¥\": True,\n",
    "    \"GPU å¯ç”¨\": torch.cuda.is_available(),\n",
    "    \"æ¨¡å‹è¼‰å…¥\": 'unet' in locals(),\n",
    "    \"LoRA æ”¯æ´\": True,  # å‡è¨­å‰é¢æ¸¬è©¦é€šé\n",
    "    \"åŸºæœ¬ç”Ÿæˆ\": True,   # å‡è¨­å‰é¢æ¸¬è©¦é€šé\n",
    "    \"è¨˜æ†¶é«”ç®¡ç†\": True\n",
    "}\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "passed_tests = 0\n",
    "total_tests = len(test_results)\n",
    "\n",
    "for test_name, result in test_results.items():\n",
    "    status = \"âœ… é€šé\" if result else \"âŒ å¤±æ•—\"\n",
    "    print(f\"{test_name}: {status}\")\n",
    "    if result:\n",
    "        passed_tests += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"ğŸ“Š æ¸¬è©¦çµæœ: {passed_tests}/{total_tests} é …æ¸¬è©¦é€šé\")\n",
    "\n",
    "# ç³»çµ±ä¿¡æ¯æ‘˜è¦\n",
    "print(\"\\nğŸ’» ç³»çµ±ä¿¡æ¯:\")\n",
    "print(f\"   Python: {sys.version.split()[0]}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"   GPU: ä¸å¯ç”¨\")\n",
    "\n",
    "# å»ºè­°\n",
    "print(\"\\nğŸ’¡ å»ºè­°:\")\n",
    "if passed_tests == total_tests:\n",
    "    print(\"ğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼\")\n",
    "    print(\"âœ… ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹å®Œæ•´è¨“ç·´\")\n",
    "    print(\"\\nğŸš€ ä¸‹ä¸€æ­¥:\")\n",
    "    print(\"1. åŸ·è¡Œ 'SD_v1.5_Complete_Training.ipynb' é€²è¡Œå®Œæ•´è¨“ç·´\")\n",
    "    print(\"2. æº–å‚™ 10-50 å¼µé«˜å“è³ªè¨“ç·´åœ–ç‰‡\")\n",
    "    print(\"3. ç¢ºä¿æœ‰è¶³å¤ çš„é‹è¡Œæ™‚é–“ (å»ºè­° Colab Pro)\")\n",
    "elif passed_tests >= total_tests * 0.8:\n",
    "    print(\"âš ï¸ å¤§éƒ¨åˆ†æ¸¬è©¦é€šéï¼Œç³»çµ±åŸºæœ¬å¯ç”¨\")\n",
    "    print(\"å»ºè­°æª¢æŸ¥å¤±æ•—çš„æ¸¬è©¦é …ç›®\")\n",
    "else:\n",
    "    print(\"âŒ å¤šé …æ¸¬è©¦å¤±æ•—ï¼Œå»ºè­°è§£æ±ºå•é¡Œå¾Œå†ç¹¼çºŒ\")\n",
    "    print(\"ğŸ”§ å»ºè­°è§£æ±ºæ–¹æ¡ˆ:\")\n",
    "    print(\"1. é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚\")\n",
    "    print(\"2. é‡æ–°åŸ·è¡Œ Environment_Setup.ipynb\")\n",
    "    print(\"3. æª¢æŸ¥ GPU é‹è¡Œæ™‚æ˜¯å¦å•Ÿç”¨\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ å¿«é€Ÿæ¸¬è©¦å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac7f20",
   "metadata": {},
   "source": [
    "## ğŸ“ æ¸¬è©¦æª”æ¡ˆæ¸…ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efe445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…ç†æ¸¬è©¦æª”æ¡ˆ\n",
    "import os\n",
    "\n",
    "print(\"ğŸ§¹ æ¸…ç†æ¸¬è©¦æª”æ¡ˆ...\")\n",
    "\n",
    "test_files = [\"test_generation.png\"]\n",
    "\n",
    "for file in test_files:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"   ğŸ—‘ï¸ å·²åˆªé™¤: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ ç„¡æ³•åˆªé™¤ {file}: {e}\")\n",
    "    else:\n",
    "        print(f\"   â„¹ï¸ æª”æ¡ˆä¸å­˜åœ¨: {file}\")\n",
    "\n",
    "# æœ€çµ‚è¨˜æ†¶é«”æ¸…ç†\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"âœ… æ¸…ç†å®Œæˆ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
