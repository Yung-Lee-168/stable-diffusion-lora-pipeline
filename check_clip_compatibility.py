#!/usr/bin/env python3
"""
CLIP ç‰ˆæœ¬ç›¸å®¹æ€§æª¢æŸ¥å·¥å…·
å¹«åŠ©ç”¨æˆ¶é¸æ“‡æœ€é©åˆçš„ CLIP ç‰ˆæœ¬
"""

import sys
import platform
import subprocess
import time

class CLIPCompatibilityChecker:
    def __init__(self):
        self.recommendations = []
        
    def check_system_info(self):
        """æª¢æŸ¥ç³»çµ±åŸºæœ¬è³‡è¨Š"""
        print("ğŸ–¥ï¸ ç³»çµ±è³‡è¨Šæª¢æŸ¥")
        print("=" * 50)
        
        # ä½œæ¥­ç³»çµ±
        os_info = platform.system()
        print(f"ä½œæ¥­ç³»çµ±: {os_info} {platform.release()}")
        
        # Python ç‰ˆæœ¬
        python_version = sys.version
        print(f"Python ç‰ˆæœ¬: {python_version}")
        
        # è¨˜æ†¶é«” (ç°¡å–®ä¼°ç®—)
        try:
            import psutil
            memory = psutil.virtual_memory()
            print(f"ç¸½è¨˜æ†¶é«”: {memory.total // (1024**3)} GB")
            print(f"å¯ç”¨è¨˜æ†¶é«”: {memory.available // (1024**3)} GB")
            
            if memory.total < 8 * (1024**3):  # å°‘æ–¼ 8GB
                self.recommendations.append("è¨˜æ†¶é«”è¼ƒå°‘ï¼Œå»ºè­°ä½¿ç”¨è¼•é‡ç‰ˆ CLIP æ¨¡å‹")
        except ImportError:
            print("æœªå®‰è£ psutilï¼Œç„¡æ³•æª¢æŸ¥è¨˜æ†¶é«”è³‡è¨Š")
        
        print()
    
    def check_pytorch_gpu(self):
        """æª¢æŸ¥ PyTorch å’Œ GPU æ”¯æ´"""
        print("ğŸ”¥ PyTorch å’Œ GPU æª¢æŸ¥")
        print("=" * 50)
        
        try:
            import torch
            print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
            
            # CUDA æª¢æŸ¥
            if torch.cuda.is_available():
                print(f"âœ… CUDA å¯ç”¨: {torch.version.cuda}")
                print(f"âœ… GPU æ•¸é‡: {torch.cuda.device_count()}")
                
                for i in range(torch.cuda.device_count()):
                    gpu_name = torch.cuda.get_device_name(i)
                    memory = torch.cuda.get_device_properties(i).total_memory
                    print(f"   GPU {i}: {gpu_name} ({memory // (1024**3)} GB)")
                
                # GPU è¨˜æ†¶é«”å»ºè­°
                total_gpu_memory = sum([
                    torch.cuda.get_device_properties(i).total_memory 
                    for i in range(torch.cuda.device_count())
                ])
                
                if total_gpu_memory >= 8 * (1024**3):  # 8GB+
                    self.recommendations.append("GPU è¨˜æ†¶é«”å……è¶³ï¼Œå¯ä½¿ç”¨å®Œæ•´ç‰ˆ CLIP æ¨¡å‹")
                elif total_gpu_memory >= 4 * (1024**3):  # 4-8GB
                    self.recommendations.append("GPU è¨˜æ†¶é«”ä¸­ç­‰ï¼Œå»ºè­°ä½¿ç”¨æ¨™æº– CLIP æ¨¡å‹")
                else:  # <4GB
                    self.recommendations.append("GPU è¨˜æ†¶é«”è¼ƒå°‘ï¼Œå»ºè­°ä½¿ç”¨ CPU æˆ–å°å‹æ¨¡å‹")
                    
            else:
                print("âš ï¸ æœªåµæ¸¬åˆ° CUDA GPUï¼Œå°‡ä½¿ç”¨ CPU")
                self.recommendations.append("åƒ… CPU å¯ç”¨ï¼Œå»ºè­°ä½¿ç”¨ transformers CLIP (è¼ƒæ…¢ä½†ç©©å®š)")
                
        except ImportError:
            print("âŒ PyTorch æœªå®‰è£")
            self.recommendations.append("éœ€è¦å…ˆå®‰è£ PyTorch")
        
        print()
    
    def check_installed_packages(self):
        """æª¢æŸ¥å·²å®‰è£çš„å¥—ä»¶"""
        print("ğŸ“¦ å·²å®‰è£å¥—ä»¶æª¢æŸ¥")
        print("=" * 50)
        
        packages_to_check = [
            "torch", "transformers", "clip-by-openai", 
            "ftfy", "regex", "tqdm", "pillow"
        ]
        
        installed_packages = {}
        
        for package in packages_to_check:
            try:
                result = subprocess.run([sys.executable, "-m", "pip", "show", package], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    version_line = [line for line in result.stdout.split('\n') if line.startswith('Version:')]
                    if version_line:
                        version = version_line[0].split(':')[1].strip()
                        print(f"âœ… {package}: {version}")
                        installed_packages[package] = version
                    else:
                        print(f"âœ… {package}: å·²å®‰è£")
                        installed_packages[package] = "unknown"
                else:
                    print(f"âŒ {package}: æœªå®‰è£")
            except Exception:
                print(f"âŒ {package}: æª¢æŸ¥å¤±æ•—")
        
        # åŸºæ–¼å·²å®‰è£å¥—ä»¶çµ¦å»ºè­°
        if "transformers" in installed_packages:
            self.recommendations.append("å·²å®‰è£ transformersï¼Œæ¨è–¦ä½¿ç”¨ HuggingFace CLIP")
        
        if "clip-by-openai" in installed_packages:
            self.recommendations.append("å·²å®‰è£ OpenAI CLIPï¼Œå¯ä½¿ç”¨å®˜æ–¹ç‰ˆæœ¬")
        
        print()
        return installed_packages
    
    def test_clip_performance(self):
        """æ¸¬è©¦ä¸åŒ CLIP ç‰ˆæœ¬çš„æ•ˆèƒ½"""
        print("âš¡ CLIP æ•ˆèƒ½æ¸¬è©¦")
        print("=" * 50)
        
        # æ¸¬è©¦ transformers CLIP
        try:
            print("æ¸¬è©¦ HuggingFace transformers CLIP...")
            start_time = time.time()
            
            from transformers import CLIPProcessor, CLIPModel
            from PIL import Image
            import torch
            
            model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
            
            # å‰µå»ºæ¸¬è©¦åœ–ç‰‡å’Œæ–‡å­—
            test_image = Image.new('RGB', (224, 224), color='red')
            test_texts = ["a red image", "a blue image", "a green image"]
            
            inputs = processor(text=test_texts, images=test_image, return_tensors="pt", padding=True)
            
            with torch.no_grad():
                outputs = model(**inputs)
            
            load_time = time.time() - start_time
            print(f"âœ… Transformers CLIP è¼‰å…¥æ™‚é–“: {load_time:.2f} ç§’")
            
            if load_time < 30:
                self.recommendations.append("Transformers CLIP è¼‰å…¥é€Ÿåº¦è‰¯å¥½")
            else:
                self.recommendations.append("Transformers CLIP è¼‰å…¥è¼ƒæ…¢ï¼Œè€ƒæ…®ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
                
        except Exception as e:
            print(f"âŒ Transformers CLIP æ¸¬è©¦å¤±æ•—: {e}")
        
        # æ¸¬è©¦ OpenAI CLIP (å¦‚æœå¯ç”¨)
        try:
            print("æ¸¬è©¦ OpenAI CLIP...")
            import clip
            
            start_time = time.time()
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model, preprocess = clip.load("ViT-B/32", device=device)
            load_time = time.time() - start_time
            
            print(f"âœ… OpenAI CLIP è¼‰å…¥æ™‚é–“: {load_time:.2f} ç§’")
            
            if load_time < 20:
                self.recommendations.append("OpenAI CLIP è¼‰å…¥é€Ÿåº¦å„ªç§€")
            
        except ImportError:
            print("âš ï¸ OpenAI CLIP æœªå®‰è£")
        except Exception as e:
            print(f"âŒ OpenAI CLIP æ¸¬è©¦å¤±æ•—: {e}")
        
        print()
    
    def generate_recommendations(self):
        """ç”Ÿæˆæœ€çµ‚å»ºè­°"""
        print("ğŸ¯ æ¨è–¦æ–¹æ¡ˆ")
        print("=" * 50)
        
        # åŸºæœ¬å»ºè­°
        if not self.recommendations:
            self.recommendations.append("ä½¿ç”¨ HuggingFace transformers CLIP (æœ€ç›¸å®¹)")
        
        # æ ¹æ“šä¸åŒæƒ…æ³çµ¦å‡ºå…·é«”å»ºè­°
        print("åŸºæ–¼æ‚¨çš„ç³»çµ±é…ç½®ï¼Œå»ºè­°ï¼š")
        print()
        
        for i, rec in enumerate(self.recommendations, 1):
            print(f"{i}. {rec}")
        
        print("\nğŸ“‹ å…·é«”å®‰è£å‘½ä»¤ï¼š")
        print()
        
        # GPU ä½¿ç”¨è€…
        try:
            import torch
            if torch.cuda.is_available():
                print("ğŸ”¥ GPU ä½¿ç”¨è€…æ¨è–¦ï¼š")
                print("pip install transformers torch torchvision")
                print("# æˆ–è€…å®‰è£ OpenAI CLIPï¼š")
                print("pip install git+https://github.com/openai/CLIP.git")
        except:
            pass
        
        # CPU ä½¿ç”¨è€…
        print("\nğŸ’» CPU ä½¿ç”¨è€…æ¨è–¦ï¼š")
        print("pip install transformers torch pillow")
        
        # FashionCLIP
        print("\nğŸ‘— æ™‚å°šå°ˆæ¥­éœ€æ±‚ï¼š")
        print("# å˜—è©¦å°ˆæ¥­ FashionCLIP (éœ€è¦ç¶²è·¯ä¸‹è¼‰)ï¼š")
        print("# æ¨¡å‹æœƒè‡ªå‹•å¾ HuggingFace ä¸‹è¼‰")
        
        print("\nâœ¨ åœ¨ç•¶å‰æ¸¬è©¦ä¸­çš„å»ºè­°ï¼š")
        print("ç›´æ¥åŸ·è¡Œ day2_enhanced_test.pyï¼Œç¨‹å¼æœƒè‡ªå‹•é¸æ“‡æœ€é©åˆçš„ç‰ˆæœ¬ï¼")
    
    def run_full_check(self):
        """åŸ·è¡Œå®Œæ•´æª¢æŸ¥"""
        print("ğŸ” CLIP ç‰ˆæœ¬ç›¸å®¹æ€§å®Œæ•´æª¢æŸ¥")
        print("=" * 60)
        print()
        
        self.check_system_info()
        self.check_pytorch_gpu()
        installed = self.check_installed_packages()
        
        # åªæœ‰åœ¨æœ‰åŸºæœ¬å¥—ä»¶æ™‚æ‰åšæ•ˆèƒ½æ¸¬è©¦
        if "transformers" in installed or "torch" in installed:
            self.test_clip_performance()
        
        self.generate_recommendations()

if __name__ == "__main__":
    checker = CLIPCompatibilityChecker()
    checker.run_full_check()
