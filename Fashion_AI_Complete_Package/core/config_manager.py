#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Day 3: Fine-tuning Configuration Manager
å¾®èª¿é…ç½®ç®¡ç†å™¨ - æ”¯æŒä¸åŒçš„è¨“ç·´ç­–ç•¥å’Œåƒæ•¸çµ„åˆ
"""

import json
import os
from datetime import datetime

class FineTuningConfig:
    """å¾®èª¿é…ç½®é¡"""
    
    def __init__(self):
        self.configs = {
            # å¿«é€Ÿæ¸¬è©¦é…ç½® - é©åˆåˆå§‹é©—è­‰
            "quick_test": {
                "learning_rate": 5e-4,
                "batch_size": 1,
                "num_epochs": 5,
                "save_steps": 2,
                "validation_steps": 2,
                "max_grad_norm": 1.0,
                "use_lora": True,
                "lora_rank": 4,
                "lora_alpha": 32,
                "image_size": 512,
                "mixed_precision": True,
                "description": "å¿«é€Ÿæ¸¬è©¦é…ç½®ï¼Œé©åˆé©—è­‰è¨“ç·´æµç¨‹"
            },
            
            # æ¨™æº–è¨“ç·´é…ç½® - å¹³è¡¡æ•ˆæœå’Œé€Ÿåº¦
            "standard": {
                "learning_rate": 1e-4,
                "batch_size": 2,
                "num_epochs": 50,
                "save_steps": 10,
                "validation_steps": 5,
                "max_grad_norm": 1.0,
                "use_lora": True,
                "lora_rank": 8,
                "lora_alpha": 32,
                "image_size": 512,
                "mixed_precision": True,
                "description": "æ¨™æº–è¨“ç·´é…ç½®ï¼Œå¹³è¡¡æ•ˆæœå’Œè¨“ç·´æ™‚é–“"
            },
            
            # é«˜å“è³ªé…ç½® - è¿½æ±‚æœ€ä½³æ•ˆæœ
            "high_quality": {
                "learning_rate": 5e-5,
                "batch_size": 1,
                "num_epochs": 100,
                "save_steps": 20,
                "validation_steps": 10,
                "max_grad_norm": 1.0,
                "use_lora": True,
                "lora_rank": 16,
                "lora_alpha": 64,
                "image_size": 768,
                "mixed_precision": True,
                "description": "é«˜å“è³ªé…ç½®ï¼Œè¿½æ±‚æœ€ä½³ç”Ÿæˆæ•ˆæœ"
            },
            
            # å…¨é‡å¾®èª¿é…ç½® - å®Œæ•´æ¨¡å‹è¨“ç·´
            "full_finetuning": {
                "learning_rate": 1e-5,
                "batch_size": 1,
                "num_epochs": 30,
                "save_steps": 5,
                "validation_steps": 3,
                "max_grad_norm": 1.0,
                "use_lora": False,  # ä¸ä½¿ç”¨ LoRAï¼Œè¨“ç·´å…¨éƒ¨åƒæ•¸
                "lora_rank": None,
                "lora_alpha": None,
                "image_size": 512,
                "mixed_precision": True,
                "description": "å…¨é‡å¾®èª¿ï¼Œè¨“ç·´å®Œæ•´ UNet åƒæ•¸ï¼ˆéœ€è¦å¤§é‡ GPU è¨˜æ†¶é«”ï¼‰"
            },
            
            # DreamBooth é¢¨æ ¼é…ç½®
            "dreambooth": {
                "learning_rate": 2e-6,
                "batch_size": 1,
                "num_epochs": 200,
                "save_steps": 50,
                "validation_steps": 25,
                "max_grad_norm": 1.0,
                "use_lora": True,
                "lora_rank": 4,
                "lora_alpha": 16,
                "image_size": 512,
                "mixed_precision": True,
                "prior_preservation": True,  # DreamBooth ç‰¹æœ‰
                "class_prompt": "fashion outfit",
                "description": "DreamBooth é¢¨æ ¼é…ç½®ï¼Œé©åˆå€‹æ€§åŒ–é¢¨æ ¼å­¸ç¿’"
            }
        }
    
    def get_config(self, config_name="standard"):
        """ç²å–æŒ‡å®šé…ç½®"""
        if config_name not in self.configs:
            print(f"âš ï¸  é…ç½® '{config_name}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜èª 'standard' é…ç½®")
            config_name = "standard"
        
        config = self.configs[config_name].copy()
        config["config_name"] = config_name
        return config
    
    def list_configs(self):
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨é…ç½®"""
        print("ğŸ“‹ å¯ç”¨çš„è¨“ç·´é…ç½®:")
        print("=" * 50)
        
        for name, config in self.configs.items():
            print(f"ğŸ”§ {name}")
            print(f"   æè¿°: {config['description']}")
            print(f"   å­¸ç¿’ç‡: {config['learning_rate']}")
            print(f"   Epochs: {config['num_epochs']}")
            print(f"   LoRA: {'æ˜¯' if config['use_lora'] else 'å¦'}")
            print()
    
    def save_config(self, config, save_path):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ é…ç½®å·²ä¿å­˜: {save_path}")
    
    def create_custom_config(self, base_config="standard", **kwargs):
        """å‰µå»ºè‡ªå®šç¾©é…ç½®"""
        config = self.get_config(base_config)
        config.update(kwargs)
        config["config_name"] = "custom"
        config["description"] = f"åŸºæ–¼ {base_config} çš„è‡ªå®šç¾©é…ç½®"
        return config

class TrainingValidator:
    """è¨“ç·´åƒæ•¸é©—è­‰å™¨"""
    
    @staticmethod
    def validate_config(config):
        """é©—è­‰é…ç½®åƒæ•¸"""
        errors = []
        warnings = []
        
        # æª¢æŸ¥å¿…éœ€åƒæ•¸
        required_params = [
            "learning_rate", "batch_size", "num_epochs", 
            "save_steps", "validation_steps", "image_size"
        ]
        
        for param in required_params:
            if param not in config:
                errors.append(f"ç¼ºå°‘å¿…éœ€åƒæ•¸: {param}")
        
        # æª¢æŸ¥åƒæ•¸åˆç†æ€§
        if config.get("learning_rate", 0) <= 0:
            errors.append("å­¸ç¿’ç‡å¿…é ˆå¤§æ–¼ 0")
        
        if config.get("learning_rate", 0) > 1e-2:
            warnings.append("å­¸ç¿’ç‡å¯èƒ½éé«˜ï¼Œå»ºè­° < 1e-2")
        
        if config.get("batch_size", 0) <= 0:
            errors.append("æ‰¹æ¬¡å¤§å°å¿…é ˆå¤§æ–¼ 0")
        
        if config.get("num_epochs", 0) <= 0:
            errors.append("è¨“ç·´ epochs å¿…é ˆå¤§æ–¼ 0")
        
        if config.get("image_size", 0) not in [512, 768, 1024]:
            warnings.append("åœ–ç‰‡å°ºå¯¸å»ºè­°ä½¿ç”¨ 512, 768 æˆ– 1024")
        
        # LoRA åƒæ•¸æª¢æŸ¥
        if config.get("use_lora", False):
            if config.get("lora_rank", 0) <= 0:
                errors.append("LoRA rank å¿…é ˆå¤§æ–¼ 0")
            
            if config.get("lora_rank", 0) > 64:
                warnings.append("LoRA rank éé«˜å¯èƒ½å½±éŸ¿è¨“ç·´æ•ˆæœ")
        
        # GPU è¨˜æ†¶é«”ä¼°ç®—
        memory_estimate = TrainingValidator.estimate_gpu_memory(config)
        if memory_estimate > 24:  # GB
            warnings.append(f"é ä¼° GPU è¨˜æ†¶é«”éœ€æ±‚: {memory_estimate:.1f} GBï¼Œå¯èƒ½è¶…å‡ºé™åˆ¶")
        
        return errors, warnings
    
    @staticmethod
    def estimate_gpu_memory(config):
        """ä¼°ç®— GPU è¨˜æ†¶é«”éœ€æ±‚"""
        base_memory = 4.0  # SD v1.5 åŸºç¤è¨˜æ†¶é«” (GB)
        
        # åœ–ç‰‡å°ºå¯¸å½±éŸ¿
        size_factor = (config.get("image_size", 512) / 512) ** 2
        image_memory = 2.0 * size_factor
        
        # æ‰¹æ¬¡å¤§å°å½±éŸ¿
        batch_memory = config.get("batch_size", 1) * 1.5
        
        # LoRA vs å…¨é‡å¾®èª¿
        if config.get("use_lora", True):
            training_memory = 1.0  # LoRA è¨˜æ†¶é«”è¼ƒå°‘
        else:
            training_memory = 6.0  # å…¨é‡å¾®èª¿è¨˜æ†¶é«”è¼ƒå¤š
        
        # æ··åˆç²¾åº¦
        if not config.get("mixed_precision", True):
            training_memory *= 1.5
        
        total_memory = base_memory + image_memory + batch_memory + training_memory
        return total_memory

def main():
    """ä¸»å‡½æ•¸ - é…ç½®ç®¡ç†æ¼”ç¤º"""
    config_manager = FineTuningConfig()
    validator = TrainingValidator()
    
    print("ğŸš€ Fine-tuning é…ç½®ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆ—å‡ºæ‰€æœ‰é…ç½®
    config_manager.list_configs()
    
    # æ¸¬è©¦é…ç½®é©—è­‰
    print("ğŸ§ª é…ç½®é©—è­‰æ¸¬è©¦:")
    for config_name in ["quick_test", "standard", "high_quality"]:
        config = config_manager.get_config(config_name)
        errors, warnings = validator.validate_config(config)
        memory = validator.estimate_gpu_memory(config)
        
        print(f"\nğŸ“‹ {config_name} é…ç½®:")
        print(f"   GPU è¨˜æ†¶é«”ä¼°ç®—: {memory:.1f} GB")
        
        if errors:
            print(f"   âŒ éŒ¯èª¤: {', '.join(errors)}")
        if warnings:
            print(f"   âš ï¸  è­¦å‘Š: {', '.join(warnings)}")
        if not errors and not warnings:
            print("   âœ… é…ç½®æœ‰æ•ˆ")
    
    # å‰µå»ºè‡ªå®šç¾©é…ç½®ç¤ºä¾‹
    print(f"\nğŸ”§ è‡ªå®šç¾©é…ç½®ç¤ºä¾‹:")
    custom_config = config_manager.create_custom_config(
        base_config="standard",
        learning_rate=2e-4,
        num_epochs=30,
        lora_rank=12
    )
    
    errors, warnings = validator.validate_config(custom_config)
    memory = validator.estimate_gpu_memory(custom_config)
    
    print(f"   å­¸ç¿’ç‡: {custom_config['learning_rate']}")
    print(f"   Epochs: {custom_config['num_epochs']}")
    print(f"   LoRA Rank: {custom_config['lora_rank']}")
    print(f"   GPU è¨˜æ†¶é«”ä¼°ç®—: {memory:.1f} GB")
    
    if errors:
        print(f"   âŒ éŒ¯èª¤: {', '.join(errors)}")
    if warnings:
        print(f"   âš ï¸  è­¦å‘Š: {', '.join(warnings)}")

if __name__ == "__main__":
    main()
