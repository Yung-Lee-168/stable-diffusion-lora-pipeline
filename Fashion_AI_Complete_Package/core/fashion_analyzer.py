#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Day 3: Fashion Training Pipeline
åŸºæ–¼ FashionCLIP ç‰¹å¾µæå–èˆ‡ SD v1.5 çš„è‡ªç›£ç£å­¸ç¿’ç³»çµ±

ğŸ¯ é‡è¦èªªæ˜ï¼š
- ğŸš« å®Œå…¨ç¦ç”¨æ¨™æº– CLIP æ¨¡å‹
- ğŸ¯ åƒ…ä½¿ç”¨ FashionCLIP é€²è¡Œåœ–ç‰‡ç‰¹å¾µæå–å’Œç›¸ä¼¼åº¦è¨ˆç®—
- æ‰€æœ‰é¡åˆ¥å®šç¾©åƒè€ƒ day2_csv_generator.py

è™•ç†æµç¨‹:
ä¾†æºåœ–è¼¸å…¥ â†’ ğŸ¯FashionCLIPç‰¹å¾µæå– â†’ çµæ§‹åŒ–è¼¸å…¥ â†’ SDåœ–ç”Ÿæˆ â†’ ğŸ¯FashionCLIPç›¸ä¼¼åº¦æ¯”å° â†’ å¾®èª¿ â†’ é¢¨æ ¼åŒ–ç”Ÿæˆ
"""

import os
import json
import requests
import base64
import io
from PIL import Image
import numpy as np
from datetime import datetime
import csv
import torch
from transformers import CLIPProcessor, CLIPModel
import cv2
from sklearn.metrics.pairwise import cosine_similarity

class FashionTrainingPipeline:
    def __init__(self):
        print("ğŸš€ åˆå§‹åŒ– Fashion Training Pipeline...")
        
        # åŸºæœ¬è¨­å®š
        self.api_url = "http://localhost:7860"
        self.source_dir = "day1_results"  # ä¾†æºåœ–ç‰‡ç›®éŒ„
        self.output_dir = "day3_training_results"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.init_models()
        
        # è¨“ç·´é…ç½®
        self.training_config = {
            "epochs": 10,
            "learning_rate": 0.001,
            "batch_size": 1,
            # ğŸ¯ å„ªåŒ–å¾Œçš„æå¤±æ¬Šé‡é…ç½®
            "loss_weights": {
                "visual": 0.2,      # è¦–è¦ºç›¸ä¼¼åº¦ (é™ä½æ¬Šé‡ï¼Œå› ç‚ºç”Ÿæˆåœ–ç‰‡é¢¨æ ¼å¯èƒ½ä¸åŒ)
                "fashion_clip": 0.6, # FashionCLIP èªæ„ç›¸ä¼¼åº¦ (ä¸»è¦æŒ‡æ¨™ï¼Œæé«˜æ¬Šé‡)
                "color": 0.2        # è‰²å½©åˆ†å¸ƒç›¸ä¼¼åº¦ (é©ä¸­æ¬Šé‡ï¼Œé—œæ³¨è‰²å½©åŒ¹é…)
            },
            # ğŸ”„ å‚™é¸æ¬Šé‡æ–¹æ¡ˆ (å¯ä¾›æ¸¬è©¦)
            "alternative_weights": {
                "balanced": {"visual": 0.33, "fashion_clip": 0.34, "color": 0.33},
                "fashion_focused": {"visual": 0.15, "fashion_clip": 0.7, "color": 0.15},
                "visual_enhanced": {"visual": 0.5, "fashion_clip": 0.4, "color": 0.1},
                "color_enhanced": {"visual": 0.3, "fashion_clip": 0.4, "color": 0.3}
            },
            # ğŸ“ æç¤ºè©ç”Ÿæˆé…ç½®
            "prompt_config": {
                "use_detailed_features": True,      # æ˜¯å¦ä½¿ç”¨è©³ç´°ç‰¹å¾µ
                "detailed_confidence_threshold": 0.3,  # è©³ç´°ç‰¹å¾µç½®ä¿¡åº¦é–¾å€¼
                "max_detailed_features": 5,        # æœ€å¤§è©³ç´°ç‰¹å¾µæ•¸é‡
                "use_basic_categories_only": False  # åƒ…ä½¿ç”¨åŸºæœ¬é¡åˆ¥
            },
            # ğŸ§ª å¯¦é©—æ€§é…ç½®
            "experimental_configs": {
                "minimal_prompt": {
                    "use_detailed_features": False,
                    "use_basic_categories_only": True,
                    "description": "åƒ…ä½¿ç”¨æ ¸å¿ƒç‰¹å¾µï¼Œæ¸¬è©¦ç°¡æ½”æç¤ºè©æ•ˆæœ"
                },
                "high_confidence_only": {
                    "use_detailed_features": True,
                    "detailed_confidence_threshold": 0.5,
                    "max_detailed_features": 3,
                    "description": "åƒ…ä½¿ç”¨é«˜ç½®ä¿¡åº¦ç‰¹å¾µ"
                },
                "detailed_focused": {
                    "use_detailed_features": True,
                    "detailed_confidence_threshold": 0.2,
                    "max_detailed_features": 8,
                    "description": "åŒ…å«æ›´å¤šè©³ç´°ç‰¹å¾µ"
                }
            }
        }
        
        # è¨˜éŒ„è¨“ç·´æ­·å²
        self.training_history = {
            "epochs": [],
            "losses": [],
            "similarities": [],
            "generated_images": []
        }
        
    def init_models(self):
        """åˆå§‹åŒ–æ‰€éœ€çš„æ¨¡å‹"""
        print("ğŸ“¦ è¼‰å…¥æ¨¡å‹ä¸­...")
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        torch_dtype = torch.float16 if device == "cuda" else torch.float32
        
        print(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {device}")
        print(f"ğŸ”§ ä½¿ç”¨ç²¾åº¦: {torch_dtype}")
        
        # è¼‰å…¥ FashionCLIP æ¨¡å‹ (å”¯ä¸€ä½¿ç”¨çš„æ¨¡å‹)
        try:
            print("ğŸ“¥ è¼‰å…¥ FashionCLIP (å°ˆæ¥­æ™‚å°šæ¨¡å‹)...")
            self.fashion_clip_model = CLIPModel.from_pretrained(
                "patrickjohncyh/fashion-clip",
                torch_dtype=torch_dtype
            ).to(device)
            self.fashion_clip_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
            print("âœ… FashionCLIP æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ FashionCLIP æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            self.fashion_clip_model = None
            self.fashion_clip_processor = None
            
        # ğŸš« å®Œå…¨ç¦ç”¨æ¨™æº– CLIP
        print("ï¿½ æ¨™æº– CLIP å·²ç¦ç”¨ - åƒ…ä½¿ç”¨ FashionCLIP")
        self.clip_model = None
        self.clip_processor = None
            
        # ä½¿ç”¨ day2_csv_generator.py ä¸­çš„å®Œæ•´åˆ†é¡çµæ§‹
        self.categories = {
            "Gender": ["male", "female"],
            "Age": ["child", "teenager", "young adult", "adult", "senior"],
            "Season": ["spring", "summer", "autumn", "winter"],
            "Occasion": ["casual", "formal", "business", "sport", "party", "beach", "wedding", "date", "travel", "home"],
            "Upper Body": ["t-shirt", "shirt", "jacket", "coat", "sweater", "blazer", "hoodie", "tank top", "blouse", "dress"],
            "Lower Body": ["jeans", "trousers", "shorts", "skirt", "leggings", "cargo pants", "sweatpants", "culottes", "capris", "dress"]
        }
        
        # è©³ç´°æœè£ç‰¹å¾µåˆ†æ - ä¾†è‡ª day2_csv_generator.py
        self.detailed_clothing_features = {
            "Dress Style": ["A-line dress", "sheath dress", "wrap dress", "maxi dress", "midi dress", "mini dress", "bodycon dress", "shift dress", "empire waist dress", "fit and flare dress", "slip dress", "shirt dress", "sweater dress"],
            "Shirt Features": ["button-down shirt", "polo shirt", "henley shirt", "flannel shirt", "dress shirt", "peasant blouse", "crop top", "off-shoulder top", "turtleneck", "v-neck shirt", "crew neck", "collared shirt"],
            "Jacket Types": ["denim jacket", "leather jacket", "bomber jacket", "trench coat", "peacoat", "blazer jacket", "cardigan", "windbreaker", "puffer jacket", "motorcycle jacket", "varsity jacket"],
            "Pants Details": ["skinny jeans", "straight leg jeans", "bootcut jeans", "wide leg pants", "high-waisted pants", "low-rise pants", "cropped pants", "palazzo pants", "joggers", "dress pants", "cargo pants with pockets"],
            "Skirt Varieties": ["pencil skirt", "A-line skirt", "pleated skirt", "wrap skirt", "mini skirt", "maxi skirt", "denim skirt", "leather skirt", "tulle skirt", "asymmetrical skirt"],
            "Fabric Texture": ["cotton fabric", "silk material", "denim texture", "leather finish", "wool texture", "linen fabric", "chiffon material", "velvet texture", "knit fabric", "lace material", "satin finish", "corduroy texture"],
            "Pattern Details": ["solid color", "striped pattern", "floral print", "polka dots", "geometric pattern", "animal print", "plaid pattern", "paisley design", "abstract print", "tie-dye pattern", "checkered pattern"],
            "Color Scheme": ["monochrome outfit", "pastel colors", "bright colors", "earth tones", "neutral colors", "bold colors", "metallic accents", "neon colors", "vintage colors", "gradient colors"],
            "Fit Description": ["loose fit", "tight fit", "oversized", "fitted", "relaxed fit", "tailored fit", "slim fit", "regular fit", "cropped length", "flowing silhouette", "structured shape"],
            "Style Details": ["minimalist style", "vintage style", "bohemian style", "gothic style", "preppy style", "streetwear style", "romantic style", "edgy style", "classic style", "trendy style", "elegant style"]
        }
        
    def extract_fashion_features(self, image_path):
        """ä½¿ç”¨ FashionCLIP æå–åœ–ç‰‡ç‰¹å¾µ - åƒè€ƒ day2_csv_generator.py çš„åˆ†ææ–¹æ³•"""
        print(f"ğŸ” ä½¿ç”¨ FashionCLIP åˆ†æåœ–ç‰‡: {os.path.basename(image_path)}")
        
        if not self.fashion_clip_model or not self.fashion_clip_processor:
            print("âŒ FashionCLIP æ¨¡å‹æœªè¼‰å…¥ï¼Œç„¡æ³•é€²è¡Œç‰¹å¾µæå–")
            return {}
        
        try:
            # è¼‰å…¥åœ–ç‰‡
            image = Image.open(image_path).convert("RGB")
            device = next(self.fashion_clip_model.parameters()).device
            model_dtype = next(self.fashion_clip_model.parameters()).dtype
            
            features = {}
            all_categories = {**self.categories, **self.detailed_clothing_features}
            
            # å°æ¯å€‹é¡åˆ¥é€²è¡Œåˆ†æ
            for category_name, labels in all_categories.items():
                try:
                    # æº–å‚™è¼¸å…¥
                    inputs = self.fashion_clip_processor(
                        text=labels, 
                        images=image, 
                        return_tensors="pt", 
                        padding=True
                    )
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    # è™•ç†æ•¸æ“šé¡å‹
                    if model_dtype == torch.float16:
                        for key in inputs:
                            if inputs[key].dtype == torch.float32:
                                inputs[key] = inputs[key].half()
                    
                    # FashionCLIP æ¨¡å‹æ¨ç†
                    with torch.no_grad():
                        outputs = self.fashion_clip_model(**inputs)
                        logits_per_image = outputs.logits_per_image
                        probs = logits_per_image.softmax(dim=1)
                    
                    # ç²å–å‰3åçµæœ
                    top_indices = probs[0].topk(min(3, len(labels))).indices
                    top_labels = [labels[i] for i in top_indices]
                    top_scores = [probs[0][i].item() for i in top_indices]
                    
                    features[category_name] = {
                        "top_labels": top_labels,
                        "scores": top_scores,
                        "confidence": max(top_scores)
                    }
                    
                    print(f"   {category_name}: {top_labels[0]} (ç½®ä¿¡åº¦: {top_scores[0]:.3f})")
                    
                except Exception as e:
                    print(f"âš ï¸ åˆ†æé¡åˆ¥ {category_name} æ™‚å‡ºéŒ¯: {e}")
                    features[category_name] = {
                        "error": str(e), 
                        "confidence": 0.0
                    }
            
            print(f"âœ… FashionCLIP ç‰¹å¾µæå–å®Œæˆï¼Œå…±åˆ†æ {len(features)} å€‹é¡åˆ¥")
            return features
            
        except Exception as e:
            print(f"âŒ åœ–ç‰‡è¼‰å…¥æˆ–è™•ç†å¤±æ•—: {e}")
            return {}
    
    def structure_features(self, features):
        """å°‡ç‰¹å¾µçµæ§‹åŒ–ç‚º SD å¯ç”¨çš„æ ¼å¼ - åƒè€ƒ day2_csv_generator.py çš„æ ¼å¼"""
        print("ğŸ”§ çµæ§‹åŒ–ç‰¹å¾µæ•¸æ“š...")
        
        structured = {
            "prompt_components": [],
            "style_tags": [],
            "technical_params": {}
        }
        
        # æå–åŸºæœ¬ç‰¹å¾µ
        gender = self._get_best_feature(features, "Gender", "person")
        age = self._get_best_feature(features, "Age", "adult")
        upper_body = self._get_best_feature(features, "Upper Body", "clothing")
        lower_body = self._get_best_feature(features, "Lower Body", "")
        occasion = self._get_best_feature(features, "Occasion", "casual")
        season = self._get_best_feature(features, "Season", "")
        
        # åŸºæœ¬äººç‰©æè¿°
        if gender != "person":
            person_desc = f"{age} {gender}"
        else:
            person_desc = age
            
        structured["prompt_components"].append(person_desc)
        
        # æœè£æè¿°
        clothing_desc = f"wearing {upper_body}"
        if lower_body and lower_body != upper_body and lower_body != "dress":
            clothing_desc += f" and {lower_body}"
        structured["prompt_components"].append(clothing_desc)
        
        # é¢¨æ ¼æ¨™ç±¤
        if occasion:
            structured["style_tags"].append(occasion)
        if season:
            structured["style_tags"].append(f"{season} fashion")
        
        # è©³ç´°ç‰¹å¾µ - æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦ä½¿ç”¨
        prompt_config = self.training_config.get("prompt_config", {})
        use_detailed = prompt_config.get("use_detailed_features", True)
        confidence_threshold = prompt_config.get("detailed_confidence_threshold", 0.3)
        max_features = prompt_config.get("max_detailed_features", 5)
        
        if use_detailed and not prompt_config.get("use_basic_categories_only", False):
            detailed_features = []
            for category_name in self.detailed_clothing_features.keys():
                feature_value = self._get_best_feature(features, category_name, "")
                confidence = features.get(category_name, {}).get("confidence", 0)
                
                # åªæ·»åŠ é«˜ç½®ä¿¡åº¦ä¸”æœ‰æ„ç¾©çš„ç‰¹å¾µ
                if feature_value and confidence > confidence_threshold:
                    detailed_features.append((feature_value, confidence))
            
            # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–å‰ N å€‹
            detailed_features.sort(key=lambda x: x[1], reverse=True)
            selected_features = [feat[0] for feat in detailed_features[:max_features]]
            
            # æ·»åŠ åˆ°é¢¨æ ¼æ¨™ç±¤
            structured["style_tags"].extend(selected_features)
            
            print(f"ğŸ” é¸æ“‡äº† {len(selected_features)} å€‹è©³ç´°ç‰¹å¾µ (é–¾å€¼: {confidence_threshold})")
        else:
            print("ğŸ“‹ åƒ…ä½¿ç”¨åŸºæœ¬é¡åˆ¥ï¼Œè·³éè©³ç´°ç‰¹å¾µ")
        
        # æŠ€è¡“åƒæ•¸
        structured["technical_params"] = {
            "steps": 25,
            "cfg_scale": 7.5,
            "width": 512,
            "height": 512,
            "sampler": "DPM++ 2M Karras"
        }
        
        print(f"âœ… çµæ§‹åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(structured['prompt_components'])} å€‹ä¸»è¦çµ„ä»¶å’Œ {len(structured['style_tags'])} å€‹é¢¨æ ¼æ¨™ç±¤")
        return structured
    
    def _get_best_feature(self, features, category_name, default=""):
        """ç²å–æœ€ä½³ç‰¹å¾µå€¼"""
        if category_name in features and "top_labels" in features[category_name]:
            return features[category_name]["top_labels"][0]
        return default
    
    def features_to_prompt(self, structured_features):
        """å°‡çµæ§‹åŒ–ç‰¹å¾µè½‰æ›ç‚º SD æç¤ºè©"""
        prompt_parts = []
        
        # ä¸»è¦æè¿°
        main_desc = ", ".join(structured_features["prompt_components"])
        prompt_parts.append(main_desc)
        
        # é¢¨æ ¼æ¨™ç±¤
        if structured_features["style_tags"]:
            style_desc = ", ".join(structured_features["style_tags"])
            prompt_parts.append(style_desc)
        
        # ğŸš« ç§»é™¤ç„¡ç”¨çš„é€šç”¨å“è³ªè©ï¼Œå°ˆæ³¨æ–¼ FashionCLIP ç‰¹å¾µ
        # é€™äº›è©å°æ™‚å°šç‰¹å¾µè¨“ç·´æ²’æœ‰å¹«åŠ©ï¼Œåè€Œç¨€é‡‹é‡è¦ç‰¹å¾µ
        
        final_prompt = ", ".join(prompt_parts)
        
        # è² é¢æç¤ºè© - ä¿æŒç°¡æ½”ï¼Œå°ˆæ³¨æ–¼é¿å…è®Šå½¢
        negative_prompt = "deformed, bad anatomy, blurry"
        
        return {
            "prompt": final_prompt,
            "negative_prompt": negative_prompt
        }
    
    def generate_image_with_sd(self, prompt_data, structured_features):
        """ä½¿ç”¨ SD v1.5 ç”Ÿæˆåœ–ç‰‡"""
        print("ğŸ¨ ä½¿ç”¨ Stable Diffusion ç”Ÿæˆåœ–ç‰‡...")
        
        payload = {
            "prompt": prompt_data["prompt"],
            "negative_prompt": prompt_data["negative_prompt"],
            "steps": structured_features["technical_params"]["steps"],
            "cfg_scale": structured_features["technical_params"]["cfg_scale"],
            "width": structured_features["technical_params"]["width"],
            "height": structured_features["technical_params"]["height"],
            "sampler_name": structured_features["technical_params"]["sampler"],
            "batch_size": 1,
            "n_iter": 1
        }
        
        try:
            response = requests.post(
                f"{self.api_url}/sdapi/v1/txt2img",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get("images"):
                    # è§£ç¢¼ç”Ÿæˆçš„åœ–ç‰‡
                    img_data = base64.b64decode(result["images"][0])
                    generated_image = Image.open(io.BytesIO(img_data))
                    print("âœ… åœ–ç‰‡ç”ŸæˆæˆåŠŸ")
                    return generated_image
                    
        except Exception as e:
            print(f"âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
            
        return None
    
    def calculate_image_similarity(self, generated_img, source_img):
        """è¨ˆç®—ç”Ÿæˆåœ–ç‰‡èˆ‡åŸåœ–çš„ç›¸ä¼¼åº¦ - ä¸»è¦ä½¿ç”¨ FashionCLIP"""
        print("ğŸ“Š è¨ˆç®—åœ–ç‰‡ç›¸ä¼¼åº¦...")
        
        similarities = {}
        
        try:
            # 1. åŸºæœ¬è¦–è¦ºç›¸ä¼¼åº¦ (ä½¿ç”¨çµæ§‹ç›¸ä¼¼æ€§)
            gen_array = np.array(generated_img.resize((256, 256)))
            src_array = np.array(source_img.resize((256, 256)))
            
            # è½‰æ›ç‚ºç°åº¦åœ–è¨ˆç®— SSIM
            gen_gray = cv2.cvtColor(gen_array, cv2.COLOR_RGB2GRAY)
            src_gray = cv2.cvtColor(src_array, cv2.COLOR_RGB2GRAY)
            
            ssim_score = cv2.matchTemplate(gen_gray, src_gray, cv2.TM_CCOEFF_NORMED)[0][0]
            similarities["visual_ssim"] = float(max(0, ssim_score))
            
            # 2. FashionCLIP èªæ„ç›¸ä¼¼åº¦ (å”¯ä¸€ä½¿ç”¨çš„èªæ„æ¨¡å‹)
            if self.fashion_clip_model and self.fashion_clip_processor:
                device = next(self.fashion_clip_model.parameters()).device
                model_dtype = next(self.fashion_clip_model.parameters()).dtype
                
                # è™•ç†åœ–ç‰‡
                inputs = self.fashion_clip_processor(
                    images=[generated_img, source_img], 
                    return_tensors="pt"
                )
                inputs = {k: v.to(device) for k, v in inputs.items()}
                
                # ç¢ºä¿è¼¸å…¥æ•¸æ“šé¡å‹èˆ‡æ¨¡å‹ä¸€è‡´
                if model_dtype == torch.float16:
                    for key in inputs:
                        if inputs[key].dtype == torch.float32:
                            inputs[key] = inputs[key].half()
                
                with torch.no_grad():
                    image_features = self.fashion_clip_model.get_image_features(**inputs)
                    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                    from sklearn.metrics.pairwise import cosine_similarity
                    fashion_similarity = cosine_similarity(
                        image_features[0:1].cpu().numpy(), 
                        image_features[1:2].cpu().numpy()
                    )[0][0]
                    
                similarities["fashion_clip"] = float(fashion_similarity)
                print(f"   âœ… FashionCLIP ç›¸ä¼¼åº¦: {fashion_similarity:.3f}")
            else:
                print("   âŒ FashionCLIP æ¨¡å‹æœªè¼‰å…¥")
                similarities["fashion_clip"] = 0.0
            
            # ï¿½ æ¨™æº– CLIP å·²å®Œå…¨ç¦ç”¨
            similarities["standard_clip"] = 0.0
            
            # 4. è‰²å½©åˆ†å¸ƒç›¸ä¼¼åº¦
            gen_hist = cv2.calcHist([gen_array], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])
            src_hist = cv2.calcHist([src_array], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])
            
            color_similarity = cv2.compareHist(gen_hist, src_hist, cv2.HISTCMP_CORREL)
            similarities["color_distribution"] = float(max(0, color_similarity))
            
            print(f"ç›¸ä¼¼åº¦åˆ†æ•¸: è¦–è¦º={similarities.get('visual_ssim', 0):.3f}, "
                  f"ğŸ¯FashionCLIP={similarities.get('fashion_clip', 0):.3f}, "
                  f"è‰²å½©={similarities.get('color_distribution', 0):.3f}")
            print(f"ğŸš« æ¨™æº– CLIP å·²ç¦ç”¨")
            
        except Exception as e:
            print(f"âš ï¸ è¨ˆç®—ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {e}")
            # è¨­å®šé è¨­å€¼
            similarities = {
                "visual_ssim": 0.5,
                "fashion_clip": 0.5,
                "standard_clip": 0.5,
                "color_distribution": 0.5
            }
        
        return similarities
    
    def calculate_combined_loss(self, similarities):
        """è¨ˆç®—çµ„åˆæå¤±å‡½æ•¸ - å„ªåŒ–æ¬Šé‡åˆ†é…ï¼Œä¸»è¦åŸºæ–¼ FashionCLIP"""
        weights = self.training_config["loss_weights"]
        
        # å°‡ç›¸ä¼¼åº¦è½‰æ›ç‚ºæå¤± (1 - similarity)
        visual_loss = 1.0 - similarities.get("visual_ssim", 0)
        fashion_clip_loss = 1.0 - similarities.get("fashion_clip", 0)  # ğŸ¯ ä¸»è¦æŒ‡æ¨™
        color_loss = 1.0 - similarities.get("color_distribution", 0)
        
        # ğŸš« æ¨™æº– CLIP å·²ç¦ç”¨ï¼Œè¨­ç‚º 0
        standard_clip_loss = 0.0
        
        # ğŸ¯ å„ªåŒ–å¾Œçš„åŠ æ¬Šçµ„åˆ
        total_loss = (
            weights["visual"] * visual_loss +           # è¦–è¦ºçµæ§‹ç›¸ä¼¼åº¦
            weights["fashion_clip"] * fashion_clip_loss +  # FashionCLIP èªæ„ç›¸ä¼¼åº¦ (ä¸»è¦)
            weights["color"] * color_loss               # è‰²å½©åˆ†å¸ƒç›¸ä¼¼åº¦
        )
        
        # ğŸ“Š è©³ç´°æå¤±åˆ†æ
        print(f"ğŸ“Š æå¤±åˆ†æ:")
        print(f"   ç¸½æå¤±: {total_loss:.4f}")
        print(f"   ğŸ¯ FashionCLIPæå¤±: {fashion_clip_loss:.4f} (æ¬Šé‡: {weights['fashion_clip']})")
        print(f"   ï¿½ï¸ è¦–è¦ºæå¤±: {visual_loss:.4f} (æ¬Šé‡: {weights['visual']})")
        print(f"   ğŸ¨ è‰²å½©æå¤±: {color_loss:.4f} (æ¬Šé‡: {weights['color']})")
        print(f"   ï¿½ğŸš« æ¨™æº– CLIP: å·²ç¦ç”¨")
        
        return {
            "total_loss": total_loss,
            "visual_loss": visual_loss,
            "fashion_clip_loss": fashion_clip_loss,  # ä¸»è¦ä¸”å”¯ä¸€èªæ„æŒ‡æ¨™
            "standard_clip_loss": standard_clip_loss,  # å·²ç¦ç”¨
            "color_loss": color_loss,
            # ğŸ’¡ æ¬Šé‡ä¿¡æ¯
            "weight_distribution": weights,
            "loss_breakdown": {
                "visual_weighted": weights["visual"] * visual_loss,
                "fashion_clip_weighted": weights["fashion_clip"] * fashion_clip_loss,
                "color_weighted": weights["color"] * color_loss
            }
        }
    
    def process_single_image(self, image_path):
        """è™•ç†å–®ä¸€åœ–ç‰‡çš„å®Œæ•´æµç¨‹"""
        print(f"\nğŸ¯ è™•ç†åœ–ç‰‡: {os.path.basename(image_path)}")
        print("=" * 50)
        
        try:
            # è¼‰å…¥åŸå§‹åœ–ç‰‡
            source_image = Image.open(image_path).convert("RGB")
            
            # 1. ç‰¹å¾µæå–
            features = self.extract_fashion_features(image_path)
            
            # 2. çµæ§‹åŒ–ç‰¹å¾µ
            structured_features = self.structure_features(features)
            
            # 3. ç”Ÿæˆæç¤ºè©
            prompt_data = self.features_to_prompt(structured_features)
            
            # ğŸ“ åˆ†ææç¤ºè©çµ„æˆ (æ–°å¢)
            prompt_analysis = self.analyze_prompt_composition(structured_features, features)
            
            print(f"ğŸ“ ç”Ÿæˆçš„æç¤ºè©: {prompt_data['prompt']}")
            
            # 4. ç”Ÿæˆåœ–ç‰‡
            generated_image = self.generate_image_with_sd(prompt_data, structured_features)
            
            if generated_image is None:
                print("âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—ï¼Œè·³éæ­¤åœ–ç‰‡")
                return None
            
            # 5. è¨ˆç®—ç›¸ä¼¼åº¦
            similarities = self.calculate_image_similarity(generated_image, source_image)
            
            # 6. è¨ˆç®—æå¤±
            losses = self.calculate_combined_loss(similarities)
            
            # ğŸ“Š è©³ç´°æå¤±åˆ†æ (æ–°å¢)
            self.analyze_loss_performance(losses, similarities)
            
            # 7. ä¿å­˜çµæœ
            result_data = {
                "source_image": os.path.basename(image_path),
                "features": features,
                "structured_features": structured_features,
                "prompt": prompt_data,
                "similarities": similarities,
                "losses": losses,
                "timestamp": datetime.now().isoformat()
            }
            
            # ä¿å­˜ç”Ÿæˆçš„åœ–ç‰‡
            output_filename = f"generated_{os.path.splitext(os.path.basename(image_path))[0]}.png"
            output_path = os.path.join(self.output_dir, output_filename)
            generated_image.save(output_path)
            
            print(f"âœ… çµæœå·²ä¿å­˜: {output_filename}")
            print(f"ğŸ“Š ç¸½æå¤±: {losses['total_loss']:.4f}")
            
            return result_data
            
        except Exception as e:
            print(f"âŒ è™•ç†åœ–ç‰‡æ™‚å‡ºéŒ¯: {e}")
            return None
    
    def run_training_pipeline(self):
        """åŸ·è¡Œå®Œæ•´çš„è¨“ç·´æµç¨‹"""
        print("ğŸš€ é–‹å§‹ Fashion Training Pipeline")
        print("=" * 60)
        
        # æª¢æŸ¥ä¾†æºç›®éŒ„
        if not os.path.exists(self.source_dir):
            print(f"âŒ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {self.source_dir}")
            return
        
        # æœå°‹åœ–ç‰‡æª”æ¡ˆ
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
            image_files.extend([
                os.path.join(self.source_dir, f) 
                for f in os.listdir(self.source_dir) 
                if f.lower().endswith(ext)
            ])
        
        if not image_files:
            print(f"âŒ åœ¨ {self.source_dir} ä¸­æ‰¾ä¸åˆ°åœ–ç‰‡æª”æ¡ˆ")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡")
        
        # è™•ç†æ¯å¼µåœ–ç‰‡
        all_results = []
        total_loss = 0
        
        for i, image_path in enumerate(image_files, 1):
            print(f"\nğŸ“· è™•ç†ç¬¬ {i}/{len(image_files)} å¼µåœ–ç‰‡")
            
            result = self.process_single_image(image_path)
            if result:
                all_results.append(result)
                total_loss += result["losses"]["total_loss"]
        
        # è¨ˆç®—å¹³å‡æå¤±
        if all_results:
            avg_loss = total_loss / len(all_results)
            print(f"\nğŸ“Š å¹³å‡æå¤±: {avg_loss:.4f}")
        
        # ä¿å­˜å®Œæ•´çµæœ
        self.save_training_results(all_results)
        
        # ç”Ÿæˆå ±å‘Š
        self.generate_training_report(all_results)
        
        print(f"\nğŸ‰ è¨“ç·´æµç¨‹å®Œæˆï¼çµæœä¿å­˜åœ¨: {self.output_dir}")
    
    def save_training_results(self, results):
        """ä¿å­˜è¨“ç·´çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ JSON æ ¼å¼çµæœ
        json_path = os.path.join(self.output_dir, f"training_results_{timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ CSV æ ¼å¼çµæœ
        csv_path = os.path.join(self.output_dir, f"training_summary_{timestamp}.csv")
        with open(csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # å¯«å…¥æ¨™é¡Œ
            writer.writerow([
                "Source Image", "Generated Prompt", "Total Loss",
                "Visual Loss", "FashionCLIP Loss", "Color Loss",
                "Visual Similarity", "FashionCLIP Similarity", "Color Similarity",
                "Visual Weight", "FashionCLIP Weight", "Color Weight"
            ])
            
            # å¯«å…¥æ•¸æ“š
            for result in results:
                weights = result["losses"].get("weight_distribution", {})
                writer.writerow([
                    result["source_image"],
                    result["prompt"]["prompt"][:100] + "...",  # æˆªæ–·é•·æç¤ºè©
                    f"{result['losses']['total_loss']:.4f}",
                    f"{result['losses']['visual_loss']:.4f}",
                    f"{result['losses']['fashion_clip_loss']:.4f}",
                    f"{result['losses']['color_loss']:.4f}",
                    f"{result['similarities'].get('visual_ssim', 0):.4f}",
                    f"{result['similarities'].get('fashion_clip', 0):.4f}",
                    f"{result['similarities'].get('color_distribution', 0):.4f}",
                    f"{weights.get('visual', 0):.2f}",
                    f"{weights.get('fashion_clip', 0):.2f}",
                    f"{weights.get('color', 0):.2f}"
                ])
        
        print(f"ğŸ“„ çµæœå·²ä¿å­˜: {json_path}")
        print(f"ğŸ“Š CSV æ‘˜è¦å·²ä¿å­˜: {csv_path}")
    
    def generate_training_report(self, results):
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        if not results:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(self.output_dir, f"training_report_{timestamp}.html")
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        total_images = len(results)
        avg_total_loss = sum(r["losses"]["total_loss"] for r in results) / total_images
        avg_visual_sim = sum(r["similarities"].get("visual_ssim", 0) for r in results) / total_images
        avg_fashion_sim = sum(r["similarities"].get("fashion_clip", 0) for r in results) / total_images
        
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Fashion Training Pipeline Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #2c3e50; color: white; padding: 20px; border-radius: 10px; }}
        .summary {{ background: #ecf0f1; padding: 15px; margin: 20px 0; border-radius: 8px; }}
        .result-item {{ border: 1px solid #bdc3c7; margin: 10px 0; padding: 15px; border-radius: 5px; }}
        .prompt {{ background: #e8f6f3; padding: 10px; border-radius: 5px; font-family: monospace; }}
        .metrics {{ display: flex; justify-content: space-around; margin: 10px 0; }}
        .metric {{ text-align: center; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #2980b9; }}
        .generated-image {{ max-width: 300px; border-radius: 5px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ¨ Fashion Training Pipeline Report</h1>
        <p>è¨“ç·´æ™‚é–“: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š è¨“ç·´æ‘˜è¦</h2>
        <div class="metrics">
            <div class="metric">
                <div class="metric-value">{total_images}</div>
                <div>è™•ç†åœ–ç‰‡æ•¸</div>
            </div>
            <div class="metric">
                <div class="metric-value">{avg_total_loss:.3f}</div>
                <div>å¹³å‡ç¸½æå¤±</div>
            </div>
            <div class="metric">
                <div class="metric-value">{avg_visual_sim:.3f}</div>
                <div>å¹³å‡è¦–è¦ºç›¸ä¼¼åº¦</div>
            </div>
            <div class="metric">
                <div class="metric-value">{avg_fashion_sim:.3f}</div>
                <div>å¹³å‡FashionCLIPç›¸ä¼¼åº¦</div>
            </div>
        </div>
    </div>
    
    <h2>ğŸ–¼ï¸ è©³ç´°çµæœ</h2>
"""
        
        # æ·»åŠ æ¯å€‹çµæœçš„è©³ç´°ä¿¡æ¯
        for i, result in enumerate(results, 1):
            generated_img_name = f"generated_{os.path.splitext(result['source_image'])[0]}.png"
            
            html_content += f"""
    <div class="result-item">
        <h3>åœ–ç‰‡ {i}: {result['source_image']}</h3>
        
        <div class="prompt">
            <strong>ç”Ÿæˆæç¤ºè©:</strong><br>
            {result['prompt']['prompt']}
        </div>
        
        <div class="metrics">
            <div class="metric">
                <div class="metric-value">{result['losses']['total_loss']:.3f}</div>
                <div>ç¸½æå¤±</div>
            </div>
            <div class="metric">
                <div class="metric-value">{result['similarities'].get('visual_ssim', 0):.3f}</div>
                <div>è¦–è¦ºç›¸ä¼¼åº¦</div>
            </div>
            <div class="metric">
                <div class="metric-value">{result['similarities'].get('fashion_clip', 0):.3f}</div>
                <div>FashionCLIPç›¸ä¼¼åº¦</div>
            </div>
            <div class="metric">
                <div class="metric-value">{result['similarities'].get('color_distribution', 0):.3f}</div>
                <div>è‰²å½©ç›¸ä¼¼åº¦</div>
            </div>
        </div>
        
        <p><strong>ç”Ÿæˆåœ–ç‰‡:</strong> {generated_img_name}</p>
        
        <details>
            <summary>è©³ç´°ç‰¹å¾µåˆ†æ</summary>
            <pre>{json.dumps(result['features'], ensure_ascii=False, indent=2)}</pre>
        </details>
    </div>
"""
        
        html_content += """
</body>
</html>
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"ğŸ“‹ è¨“ç·´å ±å‘Šå·²ç”Ÿæˆ: {report_path}")

    def set_loss_weights(self, weight_name="default"):
        """å‹•æ…‹è¨­ç½®æå¤±æ¬Šé‡é…ç½®"""
        if weight_name == "default":
            # ä½¿ç”¨å„ªåŒ–å¾Œçš„é»˜èªæ¬Šé‡
            return self.training_config["loss_weights"]
        elif weight_name in self.training_config["alternative_weights"]:
            # ä½¿ç”¨å‚™é¸æ¬Šé‡æ–¹æ¡ˆ
            self.training_config["loss_weights"] = self.training_config["alternative_weights"][weight_name]
            print(f"ğŸ”„ åˆ‡æ›åˆ°æ¬Šé‡æ–¹æ¡ˆ: {weight_name}")
            print(f"   æ–°æ¬Šé‡: {self.training_config['loss_weights']}")
            return self.training_config["loss_weights"]
        else:
            print(f"âŒ æœªçŸ¥çš„æ¬Šé‡æ–¹æ¡ˆ: {weight_name}")
            return self.training_config["loss_weights"]
    
    def compare_weight_schemes(self, image_path, schemes=["default", "balanced", "fashion_focused"]):
        """æ¯”è¼ƒä¸åŒæ¬Šé‡æ–¹æ¡ˆçš„æ•ˆæœ"""
        print(f"\nğŸ§ª æ¬Šé‡æ–¹æ¡ˆæ¯”è¼ƒå¯¦é©—: {os.path.basename(image_path)}")
        print("=" * 60)
        
        # è¼‰å…¥åŸå§‹åœ–ç‰‡ä¸¦æå–ç‰¹å¾µï¼ˆåªåšä¸€æ¬¡ï¼‰
        source_image = Image.open(image_path).convert("RGB")
        features = self.extract_fashion_features(image_path)
        structured_features = self.structure_features(features)
        prompt_data = self.features_to_prompt(structured_features)
        
        # ç”Ÿæˆåœ–ç‰‡ï¼ˆåªåšä¸€æ¬¡ï¼‰
        generated_image = self.generate_image_with_sd(prompt_data, structured_features)
        if generated_image is None:
            print("âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—ï¼Œç„¡æ³•é€²è¡Œæ¯”è¼ƒ")
            return None
        
        # è¨ˆç®—ç›¸ä¼¼åº¦ï¼ˆåªåšä¸€æ¬¡ï¼‰
        similarities = self.calculate_image_similarity(generated_image, source_image)
        
        # æ¯”è¼ƒä¸åŒæ¬Šé‡æ–¹æ¡ˆ
        comparison_results = {}
        original_weights = self.training_config["loss_weights"].copy()
        
        for scheme in schemes:
            print(f"\nğŸ¯ æ¸¬è©¦æ¬Šé‡æ–¹æ¡ˆ: {scheme}")
            print("-" * 30)
            
            # è¨­ç½®æ¬Šé‡
            self.set_loss_weights(scheme)
            
            # è¨ˆç®—æå¤±
            losses = self.calculate_combined_loss(similarities)
            
            comparison_results[scheme] = {
                "weights": self.training_config["loss_weights"].copy(),
                "total_loss": losses["total_loss"],
                "losses": losses,
                "similarities": similarities
            }
        
        # æ¢å¾©åŸå§‹æ¬Šé‡
        self.training_config["loss_weights"] = original_weights
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        self._generate_weight_comparison_report(comparison_results, os.path.basename(image_path))
        
        return comparison_results
    
    def _generate_weight_comparison_report(self, results, image_name):
        """ç”Ÿæˆæ¬Šé‡æ¯”è¼ƒå ±å‘Š"""
        print(f"\nğŸ“Š æ¬Šé‡æ–¹æ¡ˆæ¯”è¼ƒå ±å‘Š: {image_name}")
        print("=" * 60)
        
        # æŒ‰ç¸½æå¤±æ’åº
        sorted_results = sorted(results.items(), key=lambda x: x[1]["total_loss"])
        
        print("ğŸ† æ¬Šé‡æ–¹æ¡ˆæ’å (æŒ‰ç¸½æå¤±å¾ä½åˆ°é«˜):")
        for i, (scheme, data) in enumerate(sorted_results, 1):
            weights = data["weights"]
            total_loss = data["total_loss"]
            
            print(f"\n{i}. ğŸ“‹ æ–¹æ¡ˆ: {scheme}")
            print(f"   ç¸½æå¤±: {total_loss:.4f}")
            print(f"   æ¬Šé‡é…ç½®: è¦–è¦º={weights.get('visual', 0):.2f}, "
                  f"FashionCLIP={weights.get('fashion_clip', 0):.2f}, "
                  f"è‰²å½©={weights.get('color', 0):.2f}")
            
            if i == 1:
                print("   ğŸ¯ **æœ€ä½³æ–¹æ¡ˆ**")
        
        # ä¿å­˜è©³ç´°æ¯”è¼ƒçµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comparison_file = os.path.join(
            self.output_dir, 
            f"weight_comparison_{image_name}_{timestamp}.json"
        )
        
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°æ¯”è¼ƒçµæœå·²ä¿å­˜: {comparison_file}")

    def set_prompt_config(self, config_name="default"):
        """è¨­ç½®æç¤ºè©ç”Ÿæˆé…ç½®"""
        if config_name == "default":
            # ä¿æŒç•¶å‰é…ç½®
            return self.training_config["prompt_config"]
            
        elif config_name in self.training_config["experimental_configs"]:
            # ä½¿ç”¨å¯¦é©—æ€§é…ç½®
            exp_config = self.training_config["experimental_configs"][config_name]
            
            # æ›´æ–°æç¤ºè©é…ç½®
            for key, value in exp_config.items():
                if key != "description":
                    self.training_config["prompt_config"][key] = value
            
            print(f"ğŸ”„ åˆ‡æ›åˆ°æç¤ºè©é…ç½®: {config_name}")
            print(f"   æè¿°: {exp_config.get('description', '')}")
            print(f"   é…ç½®: {self.training_config['prompt_config']}")
            
            return self.training_config["prompt_config"]
        else:
            print(f"âŒ æœªçŸ¥çš„é…ç½®: {config_name}")
            return self.training_config["prompt_config"]
    
    def compare_prompt_configs(self, image_path, configs=["default", "minimal_prompt", "high_confidence_only"]):
        """æ¯”è¼ƒä¸åŒæç¤ºè©é…ç½®çš„æ•ˆæœ"""
        print(f"\nğŸ§ª æç¤ºè©é…ç½®æ¯”è¼ƒå¯¦é©—: {os.path.basename(image_path)}")
        print("=" * 60)
        
        # è¼‰å…¥åŸå§‹åœ–ç‰‡ä¸¦æå–ç‰¹å¾µï¼ˆåªåšä¸€æ¬¡ï¼‰
        source_image = Image.open(image_path).convert("RGB")
        features = self.extract_fashion_features(image_path)
        
        comparison_results = {}
        original_config = self.training_config["prompt_config"].copy()
        
        for config_name in configs:
            print(f"\nğŸ¯ æ¸¬è©¦é…ç½®: {config_name}")
            print("-" * 30)
            
            # è¨­ç½®é…ç½®
            self.set_prompt_config(config_name)
            
            # ç”Ÿæˆæç¤ºè©
            structured_features = self.structure_features(features)
            prompt_data = self.features_to_prompt(structured_features)
            
            print(f"ğŸ“ ç”Ÿæˆçš„æç¤ºè©: {prompt_data['prompt']}")
            print(f"ğŸ“ æç¤ºè©é•·åº¦: {len(prompt_data['prompt'])} å­—ç¬¦")
            
            # è¨˜éŒ„çµæœ
            comparison_results[config_name] = {
                "config": self.training_config["prompt_config"].copy(),
                "prompt": prompt_data["prompt"],
                "prompt_length": len(prompt_data["prompt"]),
                "structured_features": structured_features
            }
        
        # æ¢å¾©åŸå§‹é…ç½®
        self.training_config["prompt_config"] = original_config
        
        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        self._generate_prompt_comparison_report(comparison_results, os.path.basename(image_path))
        
        return comparison_results
    
    def _generate_prompt_comparison_report(self, results, image_name):
        """ç”Ÿæˆæç¤ºè©æ¯”è¼ƒå ±å‘Š"""
        print(f"\nğŸ“Š æç¤ºè©é…ç½®æ¯”è¼ƒå ±å‘Š: {image_name}")
        print("=" * 60)
        
        # æŒ‰æç¤ºè©é•·åº¦æ’åº
        sorted_results = sorted(results.items(), key=lambda x: x[1]["prompt_length"])
        
        print("ğŸ“ é…ç½®æ¯”è¼ƒ (æŒ‰æç¤ºè©é•·åº¦æ’åº):")
        for i, (config_name, data) in enumerate(sorted_results, 1):
            prompt_length = data["prompt_length"]
            config = data["config"]
            
            print(f"\n{i}. ğŸ“‹ é…ç½®: {config_name}")
            print(f"   æç¤ºè©é•·åº¦: {prompt_length} å­—ç¬¦")
            print(f"   ä½¿ç”¨è©³ç´°ç‰¹å¾µ: {config.get('use_detailed_features', False)}")
            print(f"   ç½®ä¿¡åº¦é–¾å€¼: {config.get('detailed_confidence_threshold', 0.3)}")
            print(f"   æœ€å¤§ç‰¹å¾µæ•¸: {config.get('max_detailed_features', 5)}")
            print(f"   æç¤ºè©: {data['prompt'][:100]}...")
            
            if i == 1:
                print("   ğŸ¯ **æœ€ç°¡æ½”**")
        
        # ä¿å­˜æ¯”è¼ƒçµæœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comparison_file = os.path.join(
            self.output_dir, 
            f"prompt_comparison_{image_name}_{timestamp}.json"
        )
        
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°æ¯”è¼ƒçµæœå·²ä¿å­˜: {comparison_file}")

    def analyze_prompt_composition(self, structured_features, features):
        """åˆ†ææç¤ºè©çµ„æˆèˆ‡ç‰¹å¾µåˆ†å¸ƒ"""
        print("ğŸ” åˆ†ææç¤ºè©çµ„æˆ...")
        
        analysis = {
            "component_count": len(structured_features["prompt_components"]),
            "style_tag_count": len(structured_features["style_tags"]),
            "feature_distribution": {},
            "confidence_analysis": {},
            "optimization_suggestions": []
        }
        
        # åˆ†æç‰¹å¾µåˆ†å¸ƒ
        for category, feature_data in features.items():
            if isinstance(feature_data, dict) and "confidence" in feature_data:
                confidence = feature_data["confidence"]
                analysis["feature_distribution"][category] = confidence
                
                # ä¿¡å¿ƒåº¦åˆ†æ
                if confidence > 0.7:
                    analysis["confidence_analysis"][category] = "é«˜ä¿¡å¿ƒ"
                elif confidence > 0.4:
                    analysis["confidence_analysis"][category] = "ä¸­ä¿¡å¿ƒ"
                else:
                    analysis["confidence_analysis"][category] = "ä½ä¿¡å¿ƒ"
        
        # å„ªåŒ–å»ºè­°
        low_confidence_features = [k for k, v in analysis["feature_distribution"].items() if v < 0.3]
        if len(low_confidence_features) > 5:
            analysis["optimization_suggestions"].append("å»ºè­°æé«˜ç½®ä¿¡åº¦é–¾å€¼ï¼Œæ¸›å°‘ä½ä¿¡å¿ƒç‰¹å¾µ")
        
        if analysis["style_tag_count"] > 8:
            analysis["optimization_suggestions"].append("é¢¨æ ¼æ¨™ç±¤éå¤šï¼Œå»ºè­°é™åˆ¶æœ€å¤§ç‰¹å¾µæ•¸é‡")
        
        print(f"   ğŸ“Š ä¸»è¦çµ„ä»¶: {analysis['component_count']}, é¢¨æ ¼æ¨™ç±¤: {analysis['style_tag_count']}")
        print(f"   ğŸ’¡ å„ªåŒ–å»ºè­°: {len(analysis['optimization_suggestions'])} é …")
        
        return analysis
    
    def analyze_loss_performance(self, losses, similarities):
        """åˆ†ææå¤±æ€§èƒ½èˆ‡å»ºè­°"""
        print("ğŸ“ˆ æå¤±æ€§èƒ½åˆ†æ...")
        
        total_loss = losses["total_loss"]
        fashion_clip_loss = losses["fashion_clip_loss"]
        
        # æ€§èƒ½è©•ä¼°
        if total_loss < 0.3:
            performance = "å„ªç§€"
            emoji = "ğŸ¯"
        elif total_loss < 0.5:
            performance = "è‰¯å¥½"
            emoji = "âœ…"
        elif total_loss < 0.7:
            performance = "ä¸€èˆ¬"
            emoji = "âš ï¸"
        else:
            performance = "éœ€æ”¹å–„"
            emoji = "âŒ"
        
        print(f"   {emoji} æ•´é«”æ€§èƒ½: {performance} (ç¸½æå¤±: {total_loss:.3f})")
        
        # FashionCLIP ä¸»è¦æŒ‡æ¨™åˆ†æ
        if fashion_clip_loss < 0.2:
            print("   ğŸ¯ FashionCLIP è¡¨ç¾å„ªç•°")
        elif fashion_clip_loss < 0.4:
            print("   âœ… FashionCLIP è¡¨ç¾è‰¯å¥½")
        else:
            print("   âš ï¸ FashionCLIP éœ€è¦å„ªåŒ–æç¤ºè©ç­–ç•¥")
        
        # æ¬Šé‡å»ºè­°
        if losses["visual_loss"] > 0.8 and losses["fashion_clip_loss"] < 0.3:
            print("   ğŸ’¡ å»ºè­°: é™ä½è¦–è¦ºæ¬Šé‡ï¼Œå°ˆæ³¨èªæ„ç›¸ä¼¼åº¦")
        
        return {
            "performance": performance,
            "total_loss": total_loss,
            "main_metric_loss": fashion_clip_loss
        }

    # ...existing code...
def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ¯ Day 3: Fashion Training Pipeline")
    print("åŸºæ–¼ FashionCLIP ç‰¹å¾µæå–èˆ‡ SD v1.5 çš„è‡ªç›£ç£å­¸ç¿’ç³»çµ±")
    print("=" * 60)
    
    # åˆå§‹åŒ–è¨“ç·´ç®¡é“
    pipeline = FashionTrainingPipeline()
    
    # åŸ·è¡Œè¨“ç·´æµç¨‹
    pipeline.run_training_pipeline()

if __name__ == "__main__":
    main()

def test_weight_optimization():
    """æ¸¬è©¦æ¬Šé‡å„ªåŒ–åŠŸèƒ½"""
    print("ğŸ§ª æ¬Šé‡å„ªåŒ–æ¸¬è©¦")
    print("=" * 40)
    
    # åˆå§‹åŒ–è¨“ç·´ç®¡é“
    pipeline = FashionTrainingPipeline()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æ¸¬è©¦åœ–ç‰‡
    test_image = None
    if os.path.exists("day1_results"):
        image_files = [f for f in os.listdir("day1_results") 
                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        if image_files:
            test_image = os.path.join("day1_results", image_files[0])
    
    if test_image:
        # é€²è¡Œæ¬Šé‡æ–¹æ¡ˆæ¯”è¼ƒ
        results = pipeline.compare_weight_schemes(
            test_image, 
            schemes=["default", "balanced", "fashion_focused", "visual_enhanced"]
        )
        
        if results:
            print("\nâœ… æ¬Šé‡å„ªåŒ–æ¸¬è©¦å®Œæˆï¼")
            print("ğŸ’¡ å»ºè­°æŸ¥çœ‹ç”Ÿæˆçš„æ¯”è¼ƒå ±å‘Šä»¥é¸æ“‡æœ€ä½³æ¬Šé‡æ–¹æ¡ˆ")
        else:
            print("âŒ æ¬Šé‡æ¸¬è©¦å¤±æ•—")
    else:
        print("âŒ æ‰¾ä¸åˆ°æ¸¬è©¦åœ–ç‰‡")
