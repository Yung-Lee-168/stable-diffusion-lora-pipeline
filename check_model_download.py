#!/usr/bin/env python3
"""
æª¢æŸ¥ CLIP å’Œ FashionCLIP æ¨¡å‹ä¸‹è¼‰ç‹€æ…‹
"""

import os
import torch
from transformers import CLIPModel, CLIPProcessor

def check_model_download_status():
    print("ğŸ” æª¢æŸ¥æ¨¡å‹ä¸‹è¼‰ç‹€æ…‹")
    print("=" * 60)
    
    # æª¢æŸ¥ HuggingFace cache ç›®éŒ„
    cache_dir = os.path.expanduser("~/.cache/huggingface/transformers")
    print(f"ğŸ“ HuggingFace å¿«å–ç›®éŒ„: {cache_dir}")
    print(f"ğŸ“ ç›®éŒ„å­˜åœ¨: {os.path.exists(cache_dir)}")
    
    if os.path.exists(cache_dir):
        cache_size = sum(
            os.path.getsize(os.path.join(dirpath, filename))
            for dirpath, dirnames, filenames in os.walk(cache_dir)
            for filename in filenames
        ) / (1024 * 1024 * 1024)  # è½‰æ›ç‚º GB
        print(f"ğŸ“¦ å¿«å–å¤§å°: {cache_size:.2f} GB")
    
    print("\n" + "=" * 60)
    print("ğŸ§ª å¯¦éš›æ¸¬è©¦æ¨¡å‹è¼‰å…¥")
    print("=" * 60)
    
    # æ¸¬è©¦æ¨™æº– CLIP
    print("1ï¸âƒ£ æ¸¬è©¦æ¨™æº– CLIP (openai/clip-vit-base-patch32)")
    try:
        print("   ğŸ“¥ å˜—è©¦è¼‰å…¥...")
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
        # æª¢æŸ¥æ¨¡å‹åƒæ•¸
        param_count = sum(p.numel() for p in model.parameters()) / 1e6
        model_size = param_count * 4 / 1024  # ä¼°ç®— GB
        
        print(f"   âœ… æ¨™æº– CLIP è¼‰å…¥æˆåŠŸ")
        print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {param_count:.1f}M")
        print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: ~{model_size:.2f} GB")
        print(f"   ğŸ® æ¨è–¦è¨­å‚™: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        
        # å¿«é€ŸåŠŸèƒ½æ¸¬è©¦
        from PIL import Image
        test_image = Image.new('RGB', (224, 224), color='red')
        test_texts = ["a red image", "a blue image"]
        
        inputs = processor(text=test_texts, images=test_image, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model(**inputs)
        
        print(f"   ğŸ§ª åŠŸèƒ½æ¸¬è©¦: é€šé")
        standard_clip_ok = True
        
    except Exception as e:
        print(f"   âŒ æ¨™æº– CLIP è¼‰å…¥å¤±æ•—: {e}")
        standard_clip_ok = False
    
    print()
    
    # æ¸¬è©¦ FashionCLIP
    print("2ï¸âƒ£ æ¸¬è©¦ FashionCLIP (patrickjohncyh/fashion-clip)")
    try:
        print("   ğŸ“¥ å˜—è©¦è¼‰å…¥...")
        fashion_model = CLIPModel.from_pretrained("patrickjohncyh/fashion-clip")
        fashion_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
        
        # æª¢æŸ¥æ¨¡å‹åƒæ•¸
        fashion_param_count = sum(p.numel() for p in fashion_model.parameters()) / 1e6
        fashion_model_size = fashion_param_count * 4 / 1024  # ä¼°ç®— GB
        
        print(f"   âœ… FashionCLIP è¼‰å…¥æˆåŠŸ")
        print(f"   ğŸ“Š åƒæ•¸æ•¸é‡: {fashion_param_count:.1f}M")
        print(f"   ğŸ’¾ æ¨¡å‹å¤§å°: ~{fashion_model_size:.2f} GB")
        print(f"   ğŸ® æ¨è–¦è¨­å‚™: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        print(f"   ğŸ‘— å°ˆæ¥­å„ªå‹¢: æ™‚å°šåœ–ç‰‡åˆ†æ")
        
        # å¿«é€ŸåŠŸèƒ½æ¸¬è©¦
        test_image = Image.new('RGB', (224, 224), color='blue')
        fashion_texts = ["elegant dress", "casual outfit", "formal wear"]
        
        inputs = fashion_processor(text=fashion_texts, images=test_image, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = fashion_model(**inputs)
        
        print(f"   ğŸ§ª åŠŸèƒ½æ¸¬è©¦: é€šé")
        fashion_clip_ok = True
        
    except Exception as e:
        print(f"   âŒ FashionCLIP è¼‰å…¥å¤±æ•—: {e}")
        if "connection" in str(e).lower() or "network" in str(e).lower():
            print(f"   ğŸ’¡ å¯èƒ½æ˜¯ç¶²è·¯å•é¡Œï¼Œæ¨¡å‹å°šæœªä¸‹è¼‰")
        fashion_clip_ok = False
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ ä¸‹è¼‰ç‹€æ…‹ç¸½çµ")
    print("=" * 60)
    
    print(f"æ¨™æº– CLIP: {'âœ… å·²ä¸‹è¼‰ä¸¦å¯ç”¨' if standard_clip_ok else 'âŒ æœªä¸‹è¼‰æˆ–æœ‰å•é¡Œ'}")
    print(f"FashionCLIP: {'âœ… å·²ä¸‹è¼‰ä¸¦å¯ç”¨' if fashion_clip_ok else 'âŒ æœªä¸‹è¼‰æˆ–æœ‰å•é¡Œ'}")
    
    if standard_clip_ok and fashion_clip_ok:
        print("\nğŸ‰ å…©å€‹æ¨¡å‹éƒ½å·²æº–å‚™å°±ç·’ï¼")
        print("âœ… å¯ä»¥ç›´æ¥åŸ·è¡Œ: python day2_enhanced_test.py")
    elif standard_clip_ok:
        print("\nâš ï¸ åªæœ‰æ¨™æº– CLIP å¯ç”¨")
        print("ğŸ’¡ ç¨‹å¼æœƒè‡ªå‹•ä½¿ç”¨æ¨™æº– CLIP é€²è¡Œæ¸¬è©¦")
    else:
        print("\nâŒ æ¨¡å‹è¼‰å…¥å•é¡Œ")
        print("ğŸ’¡ è«‹æª¢æŸ¥ç¶²è·¯é€£ç·šä¸¦é‡æ–°å˜—è©¦")
    
    print("\n" + "=" * 60)
    print("ğŸš€ åŸ·è¡Œå»ºè­°")
    print("=" * 60)
    
    if standard_clip_ok or fashion_clip_ok:
        print("1. æ¨¡å‹å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹æ¸¬è©¦")
        print("2. åŸ·è¡Œå‘½ä»¤: python day2_enhanced_test.py")
        print("3. ç¨‹å¼æœƒè‡ªå‹•é¸æ“‡å¯ç”¨çš„æ¨¡å‹é€²è¡Œæ¯”è¼ƒ")
        
        if torch.cuda.is_available():
            print("4. ğŸ® GPU åŠ é€Ÿå·²å•Ÿç”¨ï¼Œæ¸¬è©¦é€Ÿåº¦æœƒæ›´å¿«")
        else:
            print("4. ğŸ’» ä½¿ç”¨ CPU æ¨¡å¼ï¼Œå¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“")
    else:
        print("1. è«‹ç¢ºä¿ç¶²è·¯é€£ç·šæ­£å¸¸")
        print("2. é‡æ–°åŸ·è¡Œæ­¤æª¢æŸ¥è…³æœ¬")
        print("3. å¦‚æœå•é¡ŒæŒçºŒï¼Œå¯ä»¥å…ˆä½¿ç”¨ transformers çš„é è¨­å¿«å–")

if __name__ == "__main__":
    check_model_download_status()
