#!/usr/bin/env python3
"""
æª¢æŸ¥ day2_enhanced_test.py ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬
"""

def check_models():
    print("ğŸ” æª¢æŸ¥ day2_enhanced_test.py ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬")
    print("=" * 60)
    
    # æª¢æŸ¥åŸºç¤ç’°å¢ƒ
    try:
        import torch
        import transformers
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"âœ… Transformers ç‰ˆæœ¬: {transformers.__version__}")
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ… GPU: {torch.cuda.get_device_name(0)}")
            print(f"âœ… GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory // (1024**3)} GB")
        
    except ImportError as e:
        print(f"âŒ åŸºç¤å¥—ä»¶ç¼ºå¤±: {e}")
        return
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ day2_enhanced_test.py é…ç½®çš„æ¨¡å‹ç‰ˆæœ¬")
    print("=" * 60)
    
    # æ¨™æº– CLIP ç‰ˆæœ¬
    print("ğŸ” æ¨™æº– CLIP:")
    print("   æ¨¡å‹: openai/clip-vit-base-patch32")
    print("   ä¾†æº: HuggingFace Transformers")
    print("   ç”¨é€”: é€šç”¨åœ–ç‰‡-æ–‡å­—ç†è§£")
    
    # FashionCLIP ç‰ˆæœ¬
    print("\nğŸ‘— FashionCLIP:")
    print("   ä¸»è¦æ¨¡å‹: patrickjohncyh/fashion-clip")
    print("   å‚™ç”¨æ¨¡å‹: openai/clip-vit-base-patch32")
    print("   ä¾†æº: HuggingFace Transformers")
    print("   ç”¨é€”: å°ˆæ¥­æ™‚å°šåœ–ç‰‡åˆ†æ")
    
    print("\n" + "=" * 60)
    print("âš¡ ç³»çµ±å„ªåŒ–é…ç½®")
    print("=" * 60)
    
    if torch.cuda.is_available():
        print("ğŸ® GPU æ¨¡å¼:")
        print("   è¨­å‚™: CUDA (RTX 3050 Ti)")
        print("   ç²¾åº¦: float16 (ç¯€çœè¨˜æ†¶é«”)")
        print("   è¨˜æ†¶é«”ç®¡ç†: device_map='auto'")
    else:
        print("ğŸ’» CPU æ¨¡å¼:")
        print("   è¨­å‚™: CPU")
        print("   ç²¾åº¦: float32 (æ¨™æº–ç²¾åº¦)")
    
    print("\n" + "=" * 60)
    print("ğŸ§ª æ¸¬è©¦å¯¦éš›è¼‰å…¥")
    print("=" * 60)
    
    # æ¸¬è©¦æ¨™æº– CLIP
    try:
        print("ğŸ“¥ è¼‰å…¥æ¨™æº– CLIP...")
        from transformers import CLIPModel, CLIPProcessor
        
        model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
        param_count = sum(p.numel() for p in model.parameters()) / 1e6
        print(f"âœ… æ¨™æº– CLIP è¼‰å…¥æˆåŠŸ")
        print(f"   åƒæ•¸æ•¸é‡: {param_count:.1f}M")
        print(f"   æ¨¡å‹å¤§å°: ~{param_count * 4 / 1024:.1f} GB (float32)")
        
    except Exception as e:
        print(f"âŒ æ¨™æº– CLIP è¼‰å…¥å¤±æ•—: {e}")
    
    # æ¸¬è©¦ FashionCLIP
    try:
        print("\nğŸ“¥ è¼‰å…¥ FashionCLIP...")
        fashion_model = CLIPModel.from_pretrained("patrickjohncyh/fashion-clip")
        fashion_processor = CLIPProcessor.from_pretrained("patrickjohncyh/fashion-clip")
        
        fashion_param_count = sum(p.numel() for p in fashion_model.parameters()) / 1e6
        print(f"âœ… FashionCLIP è¼‰å…¥æˆåŠŸ")
        print(f"   åƒæ•¸æ•¸é‡: {fashion_param_count:.1f}M")
        print(f"   æ¨¡å‹å¤§å°: ~{fashion_param_count * 4 / 1024:.1f} GB (float32)")
        print(f"   å°ˆæ¥­å„ªå‹¢: æ™‚å°šåœ–ç‰‡åˆ†ææº–ç¢ºåº¦æ›´é«˜")
        
    except Exception as e:
        print(f"âŒ FashionCLIP è¼‰å…¥å¤±æ•—: {e}")
        print("   å°‡å›é€€åˆ°æ¨™æº– CLIP")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ åŸ·è¡Œå»ºè­°")
    print("=" * 60)
    print("1. ç›´æ¥åŸ·è¡Œ: python day2_enhanced_test.py")
    print("2. ç¨‹å¼æœƒè‡ªå‹•é¸æ“‡æœ€é©åˆçš„æ¨¡å‹ç‰ˆæœ¬")
    print("3. å¦‚æœ FashionCLIP ä¸å¯ç”¨ï¼Œæœƒè‡ªå‹•ä½¿ç”¨æ¨™æº– CLIP")
    print("4. GPU åŠ é€Ÿæœƒè‡ªå‹•å•Ÿç”¨ (å¦‚æœå¯ç”¨)")

if __name__ == "__main__":
    check_models()
