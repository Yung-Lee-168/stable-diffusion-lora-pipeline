#!/usr/bin/env python3
"""
ç³»çµ±ç¡¬é«”åˆ†æå·¥å…· - æ¨è–¦æœ€é©åˆçš„ FashionCLIP æ¨¡å‹
"""

import psutil
import platform
import subprocess
import json
from datetime import datetime

class SystemAnalyzer:
    def __init__(self):
        self.results = {}
        
    def check_system_specs(self):
        """æª¢æŸ¥ç³»çµ±åŸºæœ¬è¦æ ¼"""
        print("ğŸ” æª¢æŸ¥ç³»çµ±è¦æ ¼...")
        
        # CPU è³‡è¨Š
        self.results['cpu'] = {
            'cores': psutil.cpu_count(logical=False),
            'threads': psutil.cpu_count(logical=True),
            'frequency': psutil.cpu_freq().max if psutil.cpu_freq() else 'Unknown',
            'usage': psutil.cpu_percent(interval=1)
        }
        
        # è¨˜æ†¶é«”è³‡è¨Š
        memory = psutil.virtual_memory()
        self.results['memory'] = {
            'total_gb': round(memory.total / (1024**3), 2),
            'available_gb': round(memory.available / (1024**3), 2),
            'usage_percent': memory.percent
        }
        
        # ç³»çµ±è³‡è¨Š
        self.results['system'] = {
            'os': platform.system(),
            'version': platform.version(),
            'architecture': platform.architecture()[0]
        }
        
        print(f"âœ… CPU: {self.results['cpu']['cores']}æ ¸å¿ƒ/{self.results['cpu']['threads']}åŸ·è¡Œç·’")
        print(f"âœ… è¨˜æ†¶é«”: {self.results['memory']['total_gb']} GB")
        print(f"âœ… ç³»çµ±: {self.results['system']['os']} {self.results['system']['architecture']}")
    
    def check_gpu_info(self):
        """æª¢æŸ¥ GPU è³‡è¨Š"""
        print("\nğŸ® æª¢æŸ¥ GPU è³‡è¨Š...")
        
        gpu_info = {
            'has_cuda': False,
            'gpu_count': 0,
            'gpu_memory': 0,
            'gpu_name': 'Unknown'
        }
        
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰ CUDA
            import torch
            if torch.cuda.is_available():
                gpu_info['has_cuda'] = True
                gpu_info['gpu_count'] = torch.cuda.device_count()
                gpu_info['gpu_name'] = torch.cuda.get_device_name(0)
                gpu_info['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                print(f"âœ… CUDA GPU: {gpu_info['gpu_name']}")
                print(f"âœ… GPU è¨˜æ†¶é«”: {gpu_info['gpu_memory']:.1f} GB")
            else:
                print("âš ï¸ æœªåµæ¸¬åˆ° CUDA GPUï¼Œå°‡ä½¿ç”¨ CPU")
        except ImportError:
            print("âš ï¸ PyTorch æœªå®‰è£ï¼Œç„¡æ³•æª¢æŸ¥ GPU")
        
        self.results['gpu'] = gpu_info
    
    def check_available_models(self):
        """æª¢æŸ¥å¯ç”¨çš„ FashionCLIP æ¨¡å‹"""
        print("\nğŸ“‹ åˆ†æå¯ç”¨çš„ FashionCLIP æ¨¡å‹...")
        
        models = {
            'lightweight': {
                'name': 'openai/clip-vit-base-patch32',
                'size_mb': 600,
                'memory_requirement_gb': 2,
                'description': 'æ¨™æº– CLIP - è¼•é‡ç´šï¼Œé©åˆå¤§å¤šæ•¸é›»è…¦',
                'fashion_optimized': False
            },
            'fashion_specialized': {
                'name': 'patrickjohncyh/fashion-clip',
                'size_mb': 1200,
                'memory_requirement_gb': 3,
                'description': 'å°ˆæ¥­æ™‚å°š CLIP - é‡å°æœé£¾å„ªåŒ–',
                'fashion_optimized': True
            },
            'large_model': {
                'name': 'openai/clip-vit-large-patch14',
                'size_mb': 1800,
                'memory_requirement_gb': 4,
                'description': 'å¤§å‹ CLIP - æœ€ä½³æ•ˆæœä½†éœ€è¦æ›´å¤šè³‡æº',
                'fashion_optimized': False
            }
        }
        
        self.results['available_models'] = models
        
        for model_type, info in models.items():
            print(f"   {model_type}: {info['name']}")
            print(f"     å¤§å°: {info['size_mb']} MB")
            print(f"     è¨˜æ†¶é«”éœ€æ±‚: {info['memory_requirement_gb']} GB")
    
    def recommend_model(self):
        """åŸºæ–¼ç³»çµ±è¦æ ¼æ¨è–¦æœ€é©åˆçš„æ¨¡å‹"""
        print("\nğŸ¯ æ¨¡å‹æ¨è–¦åˆ†æ...")
        
        memory_gb = self.results['memory']['total_gb']
        has_gpu = self.results['gpu']['has_cuda']
        gpu_memory = self.results['gpu']['gpu_memory']
        
        recommendations = []
        
        # åŸºæ–¼è¨˜æ†¶é«”æ¨è–¦
        if memory_gb >= 8:
            if has_gpu and gpu_memory >= 4:
                recommendations.append({
                    'model': 'fashion_specialized',
                    'reason': 'å……è¶³çš„ç³»çµ±è¨˜æ†¶é«”å’Œ GPUï¼Œæ¨è–¦å°ˆæ¥­æ™‚å°šæ¨¡å‹',
                    'priority': 1
                })
                recommendations.append({
                    'model': 'large_model',
                    'reason': 'é«˜éšé…ç½®å¯å˜—è©¦å¤§å‹æ¨¡å‹',
                    'priority': 2
                })
            else:
                recommendations.append({
                    'model': 'fashion_specialized',
                    'reason': 'å……è¶³è¨˜æ†¶é«”ï¼Œæ¨è–¦å°ˆæ¥­æ™‚å°šæ¨¡å‹ï¼ˆCPU ç‰ˆæœ¬ï¼‰',
                    'priority': 1
                })
        elif memory_gb >= 4:
            recommendations.append({
                'model': 'lightweight',
                'reason': 'ä¸­ç­‰è¨˜æ†¶é«”é…ç½®ï¼Œæ¨è–¦è¼•é‡ç´šæ¨¡å‹',
                'priority': 1
            })
            if has_gpu:
                recommendations.append({
                    'model': 'fashion_specialized',
                    'reason': 'æœ‰ GPU åŠ é€Ÿï¼Œå¯å˜—è©¦å°ˆæ¥­æ¨¡å‹',
                    'priority': 2
                })
        else:
            recommendations.append({
                'model': 'lightweight',
                'reason': 'è¨˜æ†¶é«”è¼ƒå°‘ï¼Œåƒ…æ¨è–¦è¼•é‡ç´šæ¨¡å‹',
                'priority': 1
            })
        
        # æ’åºæ¨è–¦
        recommendations.sort(key=lambda x: x['priority'])
        self.results['recommendations'] = recommendations
        
        print("æ¨è–¦çµæœï¼š")
        for i, rec in enumerate(recommendations, 1):
            model_info = self.results['available_models'][rec['model']]
            print(f"   {i}. {model_info['name']}")
            print(f"      åŸå› : {rec['reason']}")
            print(f"      æ™‚å°šå„ªåŒ–: {'æ˜¯' if model_info['fashion_optimized'] else 'å¦'}")
    
    def generate_optimal_config(self):
        """ç”Ÿæˆæœ€ä½³é…ç½®å»ºè­°"""
        print("\nâš™ï¸ ç”Ÿæˆæœ€ä½³é…ç½®...")
        
        top_recommendation = self.results['recommendations'][0]
        model_key = top_recommendation['model']
        model_info = self.results['available_models'][model_key]
        
        config = {
            'recommended_model': model_info['name'],
            'use_gpu': self.results['gpu']['has_cuda'],
            'batch_size': 1 if self.results['memory']['total_gb'] < 8 else 2,
            'precision': 'float16' if self.results['gpu']['has_cuda'] else 'float32',
            'cache_dir': './models_cache'
        }
        
        self.results['optimal_config'] = config
        
        print("æœ€ä½³é…ç½®:")
        for key, value in config.items():
            print(f"   {key}: {value}")
    
    def save_analysis_report(self):
        """ä¿å­˜åˆ†æå ±å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'system_analysis': self.results,
            'summary': {
                'recommended_model': self.results['recommendations'][0]['model'],
                'model_name': self.results['available_models'][self.results['recommendations'][0]['model']]['name'],
                'reason': self.results['recommendations'][0]['reason']
            }
        }
        
        with open('system_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“Š åˆ†æå ±å‘Šå·²ä¿å­˜: system_analysis_report.json")
        return report
    
    def run_analysis(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print("=" * 60)
        print("ç³»çµ±ç¡¬é«”åˆ†æ & FashionCLIP æ¨¡å‹æ¨è–¦")
        print("=" * 60)
        
        self.check_system_specs()
        self.check_gpu_info()
        self.check_available_models()
        self.recommend_model()
        self.generate_optimal_config()
        
        report = self.save_analysis_report()
        
        print("\n" + "=" * 60)
        print("ğŸ¯ æœ€çµ‚æ¨è–¦")
        print("=" * 60)
        print(f"æ¨è–¦æ¨¡å‹: {report['summary']['model_name']}")
        print(f"æ¨è–¦åŸå› : {report['summary']['reason']}")
        
        return report

if __name__ == "__main__":
    analyzer = SystemAnalyzer()
    analyzer.run_analysis()
