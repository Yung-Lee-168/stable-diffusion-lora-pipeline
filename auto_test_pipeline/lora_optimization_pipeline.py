#!/usr/bin/env python3
"""
LoRA èª¿å„ªå®Œæ•´æµç¨‹è…³æœ¬
æ•´åˆè¨“ç·´ã€æ¨ç†ã€åˆ†æã€å„ªåŒ–å’Œç›£æ§çš„å®Œæ•´è‡ªå‹•åŒ–æµç¨‹

åŠŸèƒ½ï¼š
1. è‡ªå‹•åŒ– LoRA è¨“ç·´æµç¨‹
2. æ¨ç†æ¸¬è©¦åœ–ç‰‡ç”Ÿæˆ
3. çµæœåˆ†æèˆ‡æŒ‡æ¨™è¨ˆç®—
4. åƒæ•¸è‡ªå‹•å„ªåŒ–
5. ç›£æ§èˆ‡é è­¦
6. å¤šè¼ªè¿­ä»£èª¿å„ª
"""

import os
import sys
import json
import time
import datetime
import subprocess
import argparse
import shutil
from typing import Dict, List, Any, Optional
import threading
from pathlib import Path

class LoRAOptimizationPipeline:
    """LoRA èª¿å„ªå®Œæ•´æµç¨‹"""
    
    def __init__(self, base_dir: str = "."):
        self.base_dir = Path(base_dir)
        self.pipeline_dir = self.base_dir / "auto_test_pipeline"
        self.results_dir = self.pipeline_dir / "test_results"
        self.configs_dir = self.pipeline_dir / "optimization_configs"
        self.models_dir = self.base_dir / "models" / "Lora"
        
        # å‰µå»ºå¿…è¦ç›®éŒ„
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.configs_dir.mkdir(parents=True, exist_ok=True)
        
        # æµç¨‹ç‹€æ…‹
        self.current_iteration = 0
        self.max_iterations = 5
        self.target_metrics = {
            "total_loss": 0.4,
            "visual_similarity": 0.5,
            "fashion_clip_similarity": 0.6,
            "overall_score": 0.7
        }
        
        # æ­·å²è¨˜éŒ„
        self.iteration_history = []
        self.best_iteration = None
        self.best_score = 0.0
        
        # è…³æœ¬è·¯å¾‘
        self.scripts = {
            "train": self.pipeline_dir / "train_lora.py",
            "infer": self.pipeline_dir / "infer_lora.py",
            "analyze": self.pipeline_dir / "analyze_results.py",
            "optimize": self.pipeline_dir / "lora_tuning_optimizer.py",
            "monitor": self.pipeline_dir / "lora_tuning_monitor.py"
        }
    
    def check_environment(self) -> bool:
        """æª¢æŸ¥ç’°å¢ƒå’Œä¾è³´"""
        print("ğŸ” æª¢æŸ¥ç’°å¢ƒ...")
        
        # æª¢æŸ¥è…³æœ¬å­˜åœ¨æ€§
        missing_scripts = []
        for name, script_path in self.scripts.items():
            if not script_path.exists():
                missing_scripts.append(f"{name}: {script_path}")
        
        if missing_scripts:
            print("âŒ ç¼ºå°‘å¿…è¦è…³æœ¬ï¼š")
            for script in missing_scripts:
                print(f"   {script}")
            return False
        
        # æª¢æŸ¥è¨“ç·´æ•¸æ“š
        train_data_dir = self.pipeline_dir / "lora_train_set" / "10_test"
        if not train_data_dir.exists():
            print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“šç›®éŒ„ï¼š{train_data_dir}")
            return False
        
        # æª¢æŸ¥åœ–ç‰‡æ•¸é‡
        image_files = list(train_data_dir.glob("*.jpg"))
        if len(image_files) < 5:
            print(f"âŒ è¨“ç·´åœ–ç‰‡ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ 5 å¼µï¼Œç•¶å‰åªæœ‰ {len(image_files)} å¼µ")
            return False
        
        print(f"âœ… ç’°å¢ƒæª¢æŸ¥é€šéï¼Œè¨“ç·´æ•¸æ“šï¼š{len(image_files)} å¼µåœ–ç‰‡")
        return True
    
    def run_script(self, script_name: str, args: List[str] = None) -> tuple:
        """åŸ·è¡Œè…³æœ¬"""
        if script_name not in self.scripts:
            return False, f"æœªçŸ¥è…³æœ¬ï¼š{script_name}"
        
        script_path = self.scripts[script_name]
        command = [sys.executable, str(script_path)]
        
        if args:
            command.extend(args)
        
        print(f"ğŸš€ åŸ·è¡Œï¼š{' '.join(command)}")
        
        try:
            result = subprocess.run(
                command,
                capture_output=True,
                text=True,
                cwd=str(self.base_dir),
                timeout=1800  # 30åˆ†é˜è¶…æ™‚
            )
            
            if result.returncode == 0:
                return True, result.stdout
            else:
                return False, result.stderr
        except subprocess.TimeoutExpired:
            return False, "è…³æœ¬åŸ·è¡Œè¶…æ™‚"
        except Exception as e:
            return False, f"åŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}"
    
    def load_latest_analysis(self) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥æœ€æ–°åˆ†æçµæœ"""
        try:
            json_files = list(self.results_dir.glob("training_report_*.json"))
            if not json_files:
                return None
            
            # æŒ‰æ™‚é–“æ’åº
            json_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest_file = json_files[0]
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ è¼‰å…¥åˆ†æçµæœå¤±æ•—ï¼š{e}")
            return None
    
    def extract_performance_metrics(self, analysis: Dict[str, Any]) -> Dict[str, float]:
        """æå–æ€§èƒ½æŒ‡æ¨™"""
        metrics = {
            "total_loss": 1.0,
            "visual_similarity": 0.0,
            "fashion_clip_similarity": 0.0,
            "color_similarity": 0.0,
            "overall_score": 0.0,
            "training_efficiency": 0.0
        }
        
        # å¾ benchmark_analysis æå–
        if "benchmark_analysis" in analysis:
            benchmark = analysis["benchmark_analysis"]
            avg_metrics = benchmark.get("average_metrics", {})
            
            metrics["total_loss"] = avg_metrics.get("avg_total_loss", 1.0)
            metrics["visual_similarity"] = avg_metrics.get("avg_visual_similarity", 0.0)
            metrics["fashion_clip_similarity"] = avg_metrics.get("avg_fashion_clip_similarity", 0.0)
            metrics["color_similarity"] = avg_metrics.get("avg_color_similarity", 0.0)
        
        # å¾ lora_tuning æå–
        if "lora_tuning" in analysis:
            lora_tuning = analysis["lora_tuning"]
            metrics["overall_score"] = lora_tuning.get("overall_tuning_score", 0.0)
            
            if "training_efficiency" in lora_tuning:
                metrics["training_efficiency"] = lora_tuning["training_efficiency"].get("score", 0.0)
        
        return metrics
    
    def check_convergence(self, metrics: Dict[str, float]) -> bool:
        """æª¢æŸ¥æ˜¯å¦é”åˆ°æ”¶æ–‚æ¢ä»¶"""
        conditions = [
            metrics["total_loss"] <= self.target_metrics["total_loss"],
            metrics["visual_similarity"] >= self.target_metrics["visual_similarity"],
            metrics["fashion_clip_similarity"] >= self.target_metrics["fashion_clip_similarity"],
            metrics["overall_score"] >= self.target_metrics["overall_score"]
        ]
        
        return sum(conditions) >= 3  # è‡³å°‘æ»¿è¶³3å€‹æ¢ä»¶
    
    def load_optimization_config(self, iteration: int) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥å„ªåŒ–é…ç½®"""
        try:
            config_files = list(self.configs_dir.glob(f"lora_config_iter_{iteration}_*.json"))
            if not config_files:
                return None
            
            # å–æœ€æ–°çš„é…ç½®
            config_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            config_file = config_files[0]
            
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ è¼‰å…¥å„ªåŒ–é…ç½®å¤±æ•—ï¼š{e}")
            return None
    
    def backup_best_model(self, iteration: int):
        """å‚™ä»½æœ€ä½³æ¨¡å‹"""
        try:
            # æ‰¾åˆ°æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
            model_files = list(self.models_dir.glob("*.safetensors"))
            if not model_files:
                return
            
            # æŒ‰æ™‚é–“æ’åº
            model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest_model = model_files[0]
            
            # å‚™ä»½åˆ°çµæœç›®éŒ„
            backup_name = f"best_model_iter_{iteration}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.safetensors"
            backup_path = self.results_dir / backup_name
            
            shutil.copy2(latest_model, backup_path)
            print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²å‚™ä»½ï¼š{backup_path}")
            
        except Exception as e:
            print(f"âŒ å‚™ä»½æ¨¡å‹å¤±æ•—ï¼š{e}")
    
    def run_single_iteration(self, iteration: int, config: Dict[str, Any] = None) -> Dict[str, Any]:
        """åŸ·è¡Œå–®æ¬¡è¿­ä»£"""
        print(f"\n{'='*60}")
        print(f"ğŸ”„ é–‹å§‹ç¬¬ {iteration} è¼ªèª¿å„ª")
        print(f"{'='*60}")
        
        iteration_start_time = time.time()
        iteration_result = {
            "iteration": iteration,
            "start_time": datetime.datetime.now().isoformat(),
            "config": config,
            "steps": [],
            "metrics": {},
            "success": False,
            "error": None
        }
        
        # æ­¥é©Ÿ1ï¼šè¨“ç·´
        print(f"\nğŸ¯ æ­¥é©Ÿ1ï¼šLoRA è¨“ç·´...")
        train_args = []
        if config and "training_config" in config:
            train_config = config["training_config"]
            train_args.extend([
                "--learning_rate", str(train_config.get("learning_rate", 0.0005)),
                "--max_train_steps", str(train_config.get("steps", 100)),
                "--resolution", str(train_config.get("resolution", "512x512"))
            ])
        
        success, output = self.run_script("train", train_args)
        iteration_result["steps"].append({
            "step": "train",
            "success": success,
            "output": output[:500] if output else None,
            "error": output if not success else None
        })
        
        if not success:
            print(f"âŒ è¨“ç·´å¤±æ•—ï¼š{output}")
            iteration_result["error"] = f"è¨“ç·´å¤±æ•—ï¼š{output}"
            return iteration_result
        
        print("âœ… è¨“ç·´å®Œæˆ")
        
        # æ­¥é©Ÿ2ï¼šæ¨ç†
        print(f"\nğŸ¨ æ­¥é©Ÿ2ï¼šæ¨ç†æ¸¬è©¦...")
        success, output = self.run_script("infer")
        iteration_result["steps"].append({
            "step": "infer",
            "success": success,
            "output": output[:500] if output else None,
            "error": output if not success else None
        })
        
        if not success:
            print(f"âŒ æ¨ç†å¤±æ•—ï¼š{output}")
            iteration_result["error"] = f"æ¨ç†å¤±æ•—ï¼š{output}"
            return iteration_result
        
        print("âœ… æ¨ç†å®Œæˆ")
        
        # æ­¥é©Ÿ3ï¼šåˆ†æ
        print(f"\nğŸ“Š æ­¥é©Ÿ3ï¼šçµæœåˆ†æ...")
        success, output = self.run_script("analyze")
        iteration_result["steps"].append({
            "step": "analyze",
            "success": success,
            "output": output[:500] if output else None,
            "error": output if not success else None
        })
        
        if not success:
            print(f"âŒ åˆ†æå¤±æ•—ï¼š{output}")
            iteration_result["error"] = f"åˆ†æå¤±æ•—ï¼š{output}"
            return iteration_result
        
        print("âœ… åˆ†æå®Œæˆ")
        
        # è¼‰å…¥åˆ†æçµæœ
        analysis = self.load_latest_analysis()
        if analysis:
            metrics = self.extract_performance_metrics(analysis)
            iteration_result["metrics"] = metrics
            
            print(f"\nğŸ“ˆ ç¬¬ {iteration} è¼ªæ€§èƒ½æŒ‡æ¨™ï¼š")
            print(f"   ç¸½æå¤±ï¼š{metrics['total_loss']:.3f}")
            print(f"   è¦–è¦ºç›¸ä¼¼åº¦ï¼š{metrics['visual_similarity']:.3f}")
            print(f"   FashionCLIP ç›¸ä¼¼åº¦ï¼š{metrics['fashion_clip_similarity']:.3f}")
            print(f"   æ•´é«”åˆ†æ•¸ï¼š{metrics['overall_score']:.3f}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³çµæœ
            if metrics["overall_score"] > self.best_score:
                self.best_score = metrics["overall_score"]
                self.best_iteration = iteration
                self.backup_best_model(iteration)
                print(f"ğŸ‰ æ–°çš„æœ€ä½³çµæœï¼æ•´é«”åˆ†æ•¸ï¼š{self.best_score:.3f}")
        
        # æ­¥é©Ÿ4ï¼šç”Ÿæˆä¸‹ä¸€è¼ªå„ªåŒ–é…ç½®
        if iteration < self.max_iterations:
            print(f"\nğŸ”§ æ­¥é©Ÿ4ï¼šç”Ÿæˆå„ªåŒ–é…ç½®...")
            success, output = self.run_script("optimize")
            iteration_result["steps"].append({
                "step": "optimize",
                "success": success,
                "output": output[:500] if output else None,
                "error": output if not success else None
            })
            
            if not success:
                print(f"âš ï¸ å„ªåŒ–é…ç½®ç”Ÿæˆå¤±æ•—ï¼š{output}")
            else:
                print("âœ… å„ªåŒ–é…ç½®ç”Ÿæˆå®Œæˆ")
        
        iteration_result["success"] = True
        iteration_result["duration"] = time.time() - iteration_start_time
        iteration_result["end_time"] = datetime.datetime.now().isoformat()
        
        return iteration_result
    
    def run_complete_optimization(self, max_iterations: int = 5) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„å„ªåŒ–æµç¨‹"""
        print(f"ğŸš€ é–‹å§‹ LoRA èª¿å„ªå®Œæ•´æµç¨‹")
        print(f"ğŸ“‹ æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼š{max_iterations}")
        print(f"ğŸ¯ ç›®æ¨™æŒ‡æ¨™ï¼š{self.target_metrics}")
        
        self.max_iterations = max_iterations
        optimization_start_time = time.time()
        
        # ç¸½çµå ±å‘Š
        optimization_report = {
            "start_time": datetime.datetime.now().isoformat(),
            "max_iterations": max_iterations,
            "target_metrics": self.target_metrics,
            "iterations": [],
            "best_iteration": None,
            "best_score": 0.0,
            "convergence_achieved": False,
            "total_duration": 0.0,
            "final_metrics": {}
        }
        
        # åŸ·è¡Œè¿­ä»£
        for iteration in range(1, max_iterations + 1):
            # è¼‰å…¥å„ªåŒ–é…ç½®
            config = None
            if iteration > 1:
                config = self.load_optimization_config(iteration)
            
            # åŸ·è¡Œå–®æ¬¡è¿­ä»£
            iteration_result = self.run_single_iteration(iteration, config)
            optimization_report["iterations"].append(iteration_result)
            
            # æª¢æŸ¥æ˜¯å¦æˆåŠŸ
            if not iteration_result["success"]:
                print(f"âŒ ç¬¬ {iteration} è¼ªå¤±æ•—ï¼Œåœæ­¢å„ªåŒ–")
                break
            
            # æª¢æŸ¥æ”¶æ–‚
            if "metrics" in iteration_result and iteration_result["metrics"]:
                metrics = iteration_result["metrics"]
                if self.check_convergence(metrics):
                    optimization_report["convergence_achieved"] = True
                    optimization_report["final_metrics"] = metrics
                    print(f"ğŸ‰ é”åˆ°æ”¶æ–‚æ¢ä»¶ï¼Œæå‰çµæŸå„ªåŒ–")
                    break
            
            # ç­‰å¾…ä¸€æ®µæ™‚é–“å†é€²è¡Œä¸‹ä¸€è¼ª
            if iteration < max_iterations:
                print(f"\nâ³ ç­‰å¾… 10 ç§’å¾Œé–‹å§‹ç¬¬ {iteration + 1} è¼ª...")
                time.sleep(10)
        
        # å®Œæˆå„ªåŒ–
        optimization_report["total_duration"] = time.time() - optimization_start_time
        optimization_report["end_time"] = datetime.datetime.now().isoformat()
        optimization_report["best_iteration"] = self.best_iteration
        optimization_report["best_score"] = self.best_score
        
        # ä¿å­˜å„ªåŒ–å ±å‘Š
        report_file = self.results_dir / f"optimization_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(optimization_report, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ LoRA èª¿å„ªå®Œæˆ")
        print(f"{'='*60}")
        print(f"ğŸ“Š ç¸½è¿­ä»£æ¬¡æ•¸ï¼š{len(optimization_report['iterations'])}")
        print(f"ğŸ† æœ€ä½³è¿­ä»£ï¼šç¬¬ {self.best_iteration} è¼ª")
        print(f"â­ æœ€ä½³åˆ†æ•¸ï¼š{self.best_score:.3f}")
        print(f"ğŸ¯ æ˜¯å¦æ”¶æ–‚ï¼š{'æ˜¯' if optimization_report['convergence_achieved'] else 'å¦'}")
        print(f"â±ï¸ ç¸½è€—æ™‚ï¼š{optimization_report['total_duration']:.1f} ç§’")
        print(f"ğŸ“‹ è©³ç´°å ±å‘Šï¼š{report_file}")
        
        return optimization_report
    
    def generate_final_summary(self, report: Dict[str, Any]) -> str:
        """ç”Ÿæˆæœ€çµ‚ç¸½çµ"""
        summary = f"""
# LoRA èª¿å„ªå®Œæ•´æµç¨‹ç¸½çµå ±å‘Š

## åŸºæœ¬ä¿¡æ¯
- é–‹å§‹æ™‚é–“ï¼š{report['start_time']}
- çµæŸæ™‚é–“ï¼š{report['end_time']}
- ç¸½è€—æ™‚ï¼š{report['total_duration']:.1f} ç§’
- æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼š{report['max_iterations']}
- å¯¦éš›è¿­ä»£æ¬¡æ•¸ï¼š{len(report['iterations'])}

## å„ªåŒ–çµæœ
- æœ€ä½³è¿­ä»£ï¼šç¬¬ {report['best_iteration']} è¼ª
- æœ€ä½³åˆ†æ•¸ï¼š{report['best_score']:.3f}
- æ˜¯å¦æ”¶æ–‚ï¼š{'æ˜¯' if report['convergence_achieved'] else 'å¦'}

## ç›®æ¨™é”æˆæƒ…æ³
"""
        
        if report.get("final_metrics"):
            metrics = report["final_metrics"]
            target = report["target_metrics"]
            
            for metric, value in metrics.items():
                if metric in target:
                    target_value = target[metric]
                    achieved = value >= target_value if metric != "total_loss" else value <= target_value
                    status = "âœ…" if achieved else "âŒ"
                    summary += f"- {metric}: {value:.3f} / {target_value:.3f} {status}\n"
        
        summary += f"""
## è¿­ä»£æ­·å²
| è¼ªæ¬¡ | ç¸½æå¤± | è¦–è¦ºç›¸ä¼¼åº¦ | FashionCLIP | æ•´é«”åˆ†æ•¸ | ç‹€æ…‹ |
|------|--------|------------|-------------|----------|------|
"""
        
        for iteration in report["iterations"]:
            metrics = iteration.get("metrics", {})
            status = "âœ…" if iteration["success"] else "âŒ"
            summary += f"| {iteration['iteration']} | {metrics.get('total_loss', 0):.3f} | {metrics.get('visual_similarity', 0):.3f} | {metrics.get('fashion_clip_similarity', 0):.3f} | {metrics.get('overall_score', 0):.3f} | {status} |\n"
        
        summary += f"""
## å»ºè­°
"""
        
        if report["convergence_achieved"]:
            summary += "ğŸ‰ èª¿å„ªæˆåŠŸé”åˆ°æ”¶æ–‚æ¢ä»¶ï¼Œå»ºè­°ä½¿ç”¨æœ€ä½³è¿­ä»£çš„æ¨¡å‹é€²è¡Œéƒ¨ç½²ã€‚\n"
        else:
            summary += "âš ï¸ èª¿å„ªæœªé”åˆ°æ”¶æ–‚æ¢ä»¶ï¼Œå»ºè­°ï¼š\n"
            summary += "1. æª¢æŸ¥è¨“ç·´æ•¸æ“šè³ªé‡\n"
            summary += "2. èª¿æ•´ç›®æ¨™æŒ‡æ¨™\n"
            summary += "3. å¢åŠ è¿­ä»£æ¬¡æ•¸\n"
            summary += "4. å„ªåŒ–è¶…åƒæ•¸è¨­å®š\n"
        
        return summary

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="LoRA èª¿å„ªå®Œæ•´æµç¨‹")
    parser.add_argument("--max_iterations", type=int, default=5, help="æœ€å¤§è¿­ä»£æ¬¡æ•¸")
    parser.add_argument("--target_loss", type=float, default=0.4, help="ç›®æ¨™ç¸½æå¤±")
    parser.add_argument("--target_visual", type=float, default=0.5, help="ç›®æ¨™è¦–è¦ºç›¸ä¼¼åº¦")
    parser.add_argument("--target_fashion", type=float, default=0.6, help="ç›®æ¨™FashionCLIPç›¸ä¼¼åº¦")
    parser.add_argument("--target_overall", type=float, default=0.7, help="ç›®æ¨™æ•´é«”åˆ†æ•¸")
    parser.add_argument("--base_dir", type=str, default=".", help="åŸºç¤ç›®éŒ„")
    
    args = parser.parse_args()
    
    # å‰µå»ºæµç¨‹å¯¦ä¾‹
    pipeline = LoRAOptimizationPipeline(args.base_dir)
    
    # è¨­å®šç›®æ¨™æŒ‡æ¨™
    pipeline.target_metrics = {
        "total_loss": args.target_loss,
        "visual_similarity": args.target_visual,
        "fashion_clip_similarity": args.target_fashion,
        "overall_score": args.target_overall
    }
    
    # æª¢æŸ¥ç’°å¢ƒ
    if not pipeline.check_environment():
        print("âŒ ç’°å¢ƒæª¢æŸ¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
        return
    
    # åŸ·è¡Œå®Œæ•´å„ªåŒ–
    try:
        report = pipeline.run_complete_optimization(args.max_iterations)
        
        # ç”Ÿæˆæœ€çµ‚ç¸½çµ
        summary = pipeline.generate_final_summary(report)
        summary_file = pipeline.results_dir / f"final_summary_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        print(f"\nğŸ“‹ æœ€çµ‚ç¸½çµå·²ä¿å­˜ï¼š{summary_file}")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨åœæ­¢...")
    except Exception as e:
        print(f"âŒ æµç¨‹åŸ·è¡Œå¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()
