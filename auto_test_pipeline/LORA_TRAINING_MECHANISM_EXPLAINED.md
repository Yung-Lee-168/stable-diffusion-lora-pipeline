# LoRA è¨“ç·´æ ¸å¿ƒæ©Ÿåˆ¶è©³è§£

**æ—¥æœŸ:** 2025å¹´7æœˆ8æ—¥  
**å•é¡Œ:** LoRAåœ¨è¨“ç·´æœŸé–“ï¼Œè®€å–åŸåœ–åŠCLIP dataï¼Œåˆ°åº•åœ¨è¨ˆç®—ä»€éº¼ï¼Œåˆ°åº•åœ¨èª¿ç”šéº¼?

## ğŸ§  LoRA è¨“ç·´çš„æ ¸å¿ƒåŸç†

### ğŸ¯ **LoRA çš„æœ¬è³ª**
LoRA (Low-Rank Adaptation) ä¸æ˜¯åœ¨"ç”Ÿæˆåœ–ç‰‡"ï¼Œè€Œæ˜¯åœ¨**å­¸ç¿’å¦‚ä½•ä¿®æ”¹ Stable Diffusion æ¨¡å‹çš„è¡Œç‚º**ã€‚

## ğŸ“Š LoRA è¨“ç·´æœŸé–“çš„å…·é«”è¨ˆç®—æµç¨‹

### 1. **è¼¸å…¥æ•¸æ“šè™•ç†**
```
åŸåœ– (512x512) â†’ VAE Encoder â†’ Latent Space (64x64x4)
æ–‡å­—æè¿° â†’ CLIP Text Encoder â†’ Text Embeddings (77x768)
```

### 2. **æ ¸å¿ƒè¨“ç·´å¾ªç’°**
```python
# å½ä»£ç¢¼èªªæ˜ LoRA è¨“ç·´éç¨‹
for each_training_step:
    # 1. è¼‰å…¥ä¸€æ‰¹è¨“ç·´æ•¸æ“š
    images = load_batch_images()  # åŸå§‹è¨“ç·´åœ–ç‰‡
    captions = load_batch_captions()  # å°æ‡‰çš„æ–‡å­—æè¿°
    
    # 2. ç·¨ç¢¼åˆ°æ½›åœ¨ç©ºé–“
    latents = vae.encode(images)  # åœ–ç‰‡ â†’ æ½›åœ¨å‘é‡
    text_embeddings = clip.encode(captions)  # æ–‡å­— â†’ åµŒå…¥å‘é‡
    
    # 3. æ·»åŠ å™ªè²ï¼ˆæ“´æ•£éç¨‹çš„é€†å‘ï¼‰
    noise = torch.randn_like(latents)
    timesteps = random_timesteps()
    noisy_latents = scheduler.add_noise(latents, noise, timesteps)
    
    # 4. UNet é æ¸¬å™ªè²ï¼ˆé€™è£¡æ˜¯é—œéµï¼‰
    # ğŸ¯ LoRA åœ¨é€™è£¡ä¿®æ”¹ UNet çš„æ¬Šé‡
    predicted_noise = unet(
        noisy_latents, 
        timesteps, 
        text_embeddings,
        # â­ LoRA æ¬Šé‡åœ¨é€™è£¡èµ·ä½œç”¨
    )
    
    # 5. è¨ˆç®—æå¤±
    loss = mse_loss(predicted_noise, actual_noise)
    
    # 6. åå‘å‚³æ’­ï¼Œåªæ›´æ–° LoRA æ¬Šé‡
    loss.backward()
    optimizer.step()  # åªæ›´æ–° LoRA åƒæ•¸ï¼Œä¸å‹•åŸæ¨¡å‹
```

## ğŸ”§ **LoRA åˆ°åº•åœ¨"èª¿"ä»€éº¼ï¼Ÿ**

### 1. **UNet çš„æ³¨æ„åŠ›å±¤æ¬Šé‡**
```python
# åŸå§‹æ¬Šé‡çŸ©é™£ (ä¾‹å¦‚ 320x320)
W_original = [320, 320]

# LoRA åˆ†è§£ç‚ºå…©å€‹å°çŸ©é™£
W_lora_A = [320, rank]  # rank é€šå¸¸æ˜¯ 16-128
W_lora_B = [rank, 320]

# å¯¦éš›ä½¿ç”¨çš„æ¬Šé‡
W_effective = W_original + (W_lora_A @ W_lora_B) * alpha
```

### 2. **å…·é«”èª¿æ•´çš„æ¨¡çµ„**
- **Cross-Attention**: æ–‡å­—å¦‚ä½•å½±éŸ¿åœ–ç‰‡ç”Ÿæˆ
- **Self-Attention**: åœ–ç‰‡å…§éƒ¨å…ƒç´ å¦‚ä½•ç›¸äº’é—œè¯
- **Feed-Forward**: ç‰¹å¾µçš„éç·šæ€§è®Šæ›

## ğŸ“ˆ **è¨“ç·´æœŸé–“çš„ Loss è¨ˆç®—**

### ğŸ¯ **å¯¦éš›çš„ Total Loss çµ„æˆ**
```python
# train_network.py ä¸­å¯¦éš›è¨ˆç®—çš„æå¤±
def compute_loss(model_pred, target, timesteps):
    # 1. åŸºç¤å™ªè²é æ¸¬æå¤± (MSE)
    mse_loss = F.mse_loss(model_pred, target)
    
    # 2. å¯èƒ½åŒ…å«çš„å…¶ä»–æå¤±é …
    if use_snr_weighting:
        # Signal-to-Noise Ratio åŠ æ¬Š
        snr_weights = compute_snr_weights(timesteps)
        weighted_loss = mse_loss * snr_weights
    
    if use_prior_preservation:
        # å…ˆé©—ä¿æŒæå¤± (é˜²æ­¢éæ“¬åˆ)
        prior_loss = compute_prior_loss()
        total_loss = weighted_loss + prior_loss
    
    return total_loss
```

### ğŸ“Š **å¯¦éš› Loss æ•¸å€¼çš„å«ç¾©**
- **0.127**: å™ªè²é æ¸¬çš„å‡æ–¹èª¤å·®
- **æ•¸å€¼ä¸‹é™**: æ¨¡å‹è¶Šä¾†è¶Šæº–ç¢ºåœ°é æ¸¬å™ªè²
- **æ•¸å€¼å«ç¾©**: è¶Šä½è¡¨ç¤º LoRA è¶Šå¥½åœ°å­¸æœƒäº†ç›®æ¨™é¢¨æ ¼/æ¦‚å¿µ

## ğŸ” **ç‚ºä»€éº¼éœ€è¦åŸåœ–å’Œ CLIP æ•¸æ“šï¼Ÿ**

### 1. **åŸåœ–çš„ä½œç”¨**
```
åŸåœ– â†’ VAEç·¨ç¢¼ â†’ ç›®æ¨™æ½›åœ¨å‘é‡ â†’ åŠ å™ªè² â†’ è¨“ç·´ç›®æ¨™
```
- åŸåœ–æä¾›äº†"æ­£ç¢ºç­”æ¡ˆ"
- LoRA å­¸ç¿’å¦‚ä½•å¾å™ªè²é‡å»ºé€™äº›ç‰¹å®šçš„åœ–ç‰‡

### 2. **CLIP æ•¸æ“šçš„ä½œç”¨**  
```
æ–‡å­—æè¿° â†’ CLIPç·¨ç¢¼ â†’ æ¢ä»¶å‘é‡ â†’ æŒ‡å° UNet ç”Ÿæˆ
```
- æ–‡å­—æè¿°å‘Šè¨´ LoRA "åœ¨ä»€éº¼æ¢ä»¶ä¸‹"è¦ç”Ÿæˆé€™ç¨®æ•ˆæœ
- å»ºç«‹æ–‡å­—èˆ‡è¦–è¦ºç‰¹å¾µçš„å°æ‡‰é—œä¿‚

## ğŸ¨ **å…·é«”ä¾‹å­ï¼šæ™‚å°š LoRA è¨“ç·´**

å‡è¨­æˆ‘å€‘åœ¨è¨“ç·´ä¸€å€‹"å„ªé›…æ´‹è£"çš„ LoRAï¼š

### è¼¸å…¥æ•¸æ“š
```
åœ–ç‰‡: elegant_dress_001.jpg (å„ªé›…æ´‹è£ç…§ç‰‡)
æè¿°: "elegant evening dress, flowing fabric, sophisticated style"
```

### LoRA å­¸ç¿’éç¨‹
1. **åˆ†æåŸåœ–ç‰¹å¾µ**: æ´‹è£çš„å½¢ç‹€ã€è³ªæ„Ÿã€è‰²å½©
2. **é—œè¯æ–‡å­—æè¿°**: "elegant", "evening dress", "flowing fabric" â†’ è¦–è¦ºç‰¹å¾µ
3. **èª¿æ•´ UNet æ¬Šé‡**: ç•¶çœ‹åˆ°é€™äº›é—œéµè©æ™‚ï¼Œå¢å¼·ç”Ÿæˆé€™ç¨®é¢¨æ ¼çš„èƒ½åŠ›
4. **æœ€å°åŒ–é æ¸¬èª¤å·®**: è®“æ¨¡å‹æ›´æº–ç¢ºé æ¸¬å¦‚ä½•å»å™ªå¾—åˆ°ç›®æ¨™åœ–ç‰‡

### è¨“ç·´å¾Œçš„æ•ˆæœ
```
ç”¨æˆ¶è¼¸å…¥: "elegant evening dress"
LoRA èª¿æ•´å¾Œçš„æ¨¡å‹: æ›´å®¹æ˜“ç”Ÿæˆå„ªé›…æ´‹è£çš„åœ–ç‰‡
```

## ğŸ”§ **technical ç´°ç¯€ï¼šæ¬Šé‡ä¿®æ”¹æ–¹å¼**

### LoRA ä¿®æ”¹ Attention æ©Ÿåˆ¶
```python
# åŸå§‹ Attention è¨ˆç®—
Q = input @ W_q  # Query
K = input @ W_k  # Key  
V = input @ W_v  # Value

# LoRA å¢å¼·å¾Œ
Q = input @ (W_q + lora_q_A @ lora_q_B * alpha)
K = input @ (W_k + lora_k_A @ lora_k_B * alpha) 
V = input @ (W_v + lora_v_A @ lora_v_B * alpha)
```

## ğŸ“Š **Loss æ•¸å€¼çš„å¯¦éš›æ„ç¾©**

| Loss ç¯„åœ | æ„ç¾© | è¨“ç·´ç‹€æ…‹ |
|----------|------|---------|
| 0.5+ | åˆå§‹ç‹€æ…‹ï¼Œé æ¸¬å¾ˆä¸æº–ç¢º | å‰›é–‹å§‹è¨“ç·´ |
| 0.2-0.5 | é–‹å§‹å­¸æœƒåŸºæœ¬ç‰¹å¾µ | è¨“ç·´ä¸­æœŸ |
| 0.1-0.2 | å­¸æœƒäº†ä¸»è¦ç‰¹å¾µ | è¨“ç·´å¾ŒæœŸ |
| 0.05-0.1 | éå¸¸æº–ç¢ºçš„é æ¸¬ | å¯èƒ½éæ“¬åˆ |

ä½ çœ‹åˆ°çš„ `Total=0.127000` è¡¨ç¤º LoRA å·²ç¶“ç›¸ç•¶æº–ç¢ºåœ°å­¸æœƒäº†é æ¸¬å™ªè²ï¼Œæ­£åœ¨æœ‰æ•ˆå­¸ç¿’ç›®æ¨™é¢¨æ ¼ã€‚

## ğŸ”§ **æˆ‘å€‘çš„ LoRA è¨“ç·´å…·é«”é…ç½®åˆ†æ**

### å¯¦éš›ä½¿ç”¨çš„ train_network.py åƒæ•¸
```bash
python train_network.py \
  --pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors \
  --train_data_dir=lora_train_set \
  --output_dir=lora_output \
  --logging_dir=training_logs/logs \
  --resolution=512,512 \
  --network_module=networks.lora \
  --network_dim=32 \
  --train_batch_size=1 \
  --max_train_steps=200 \
  --mixed_precision=fp16 \
  --cache_latents \
  --learning_rate=5e-5 \
  --save_every_n_epochs=50 \
  --save_model_as=safetensors \
  --save_state \
  --log_with=tensorboard \
  --gradient_accumulation_steps=1
```

### ğŸ¯ **æ¯å€‹åƒæ•¸çš„å…·é«”ä½œç”¨**

#### 1. **æ¨¡å‹å’Œæ•¸æ“šé…ç½®**
- `--pretrained_model_name_or_path`: è¼‰å…¥ SD 1.5 åŸºç¤æ¨¡å‹
- `--train_data_dir=lora_train_set`: è®€å–æˆ‘å€‘çš„è¨“ç·´åœ–ç‰‡å’Œæ¨™é¡Œ
- `--resolution=512,512`: å°‡æ‰€æœ‰åœ–ç‰‡çµ±ä¸€è™•ç†ç‚º 512x512

#### 2. **LoRA ç‰¹å®šé…ç½®**
- `--network_module=networks.lora`: ä½¿ç”¨ LoRA æ¶æ§‹
- `--network_dim=32`: LoRA çš„ç§© (rank) = 32
  ```python
  # é€™æ„å‘³è‘—æ¯å€‹æ¬Šé‡çŸ©é™£è¢«åˆ†è§£ç‚ºï¼š
  W_original [1024, 1024] + W_lora_A [1024, 32] @ W_lora_B [32, 1024]
  # åƒæ•¸é‡æ¸›å°‘ï¼š1024Â² â†’ 1024Ã—32Ã—2 = åŸä¾†çš„ 6.25%
  ```

#### 3. **è¨“ç·´éç¨‹é…ç½®**
- `--train_batch_size=1`: æ¯æ¬¡åªè™•ç†1å¼µåœ–ç‰‡
- `--max_train_steps=200`: ç¸½å…±è¨“ç·´200æ­¥
- `--learning_rate=5e-5`: å­¸ç¿’ç‡ 0.00005
- `--mixed_precision=fp16`: ä½¿ç”¨åŠç²¾åº¦æµ®é»ï¼Œç¯€çœè¨˜æ†¶é«”

#### 4. **æœ€ä½³åŒ–é…ç½®**
- `--cache_latents`: é å…ˆè¨ˆç®—ä¸¦å¿«å–æ½›åœ¨å‘é‡ï¼ŒåŠ é€Ÿè¨“ç·´
- `--gradient_accumulation_steps=1`: ä¸ç´¯ç©æ¢¯åº¦ï¼Œæ¯æ­¥éƒ½æ›´æ–°

### ğŸ§® **å¯¦éš›è¨ˆç®—éç¨‹è©³è§£**

#### Step 1: æ•¸æ“šè¼‰å…¥
```python
# æ¯å€‹è¨“ç·´æ­¥é©Ÿ
image = load_image("lora_train_set/10_test/fashion_001.jpg")  # è¼‰å…¥åœ–ç‰‡
caption = load_caption("lora_train_set/10_test/fashion_001.txt")  # è¼‰å…¥æè¿°
```

#### Step 2: ç·¨ç¢¼åˆ°æ½›åœ¨ç©ºé–“
```python
# VAE ç·¨ç¢¼ (åªåšä¸€æ¬¡ï¼Œç„¶å¾Œå¿«å–)
latents = vae.encode(image)  # [1, 4, 64, 64] 
text_embeddings = clip_text_encoder.encode(caption)  # [1, 77, 768]
```

#### Step 3: å™ªè²é æ¸¬è¨“ç·´
```python
# éš¨æ©Ÿæ™‚é–“æ­¥ (0-1000)
timestep = random.randint(0, 1000)

# æ·»åŠ å°æ‡‰ç¨‹åº¦çš„å™ªè²
noise = torch.randn_like(latents)
noisy_latents = scheduler.add_noise(latents, noise, timestep)

# UNet é æ¸¬å™ªè² (LoRA åœ¨é€™è£¡ä¿®æ”¹ UNet æ¬Šé‡)
predicted_noise = unet(
    noisy_latents,      # åŠ å™ªè²çš„æ½›åœ¨å‘é‡
    timestep,           # æ™‚é–“æ­¥
    text_embeddings     # æ–‡å­—æ¢ä»¶
)

# è¨ˆç®— MSE Loss
loss = F.mse_loss(predicted_noise, noise)  # é€™å°±æ˜¯æˆ‘å€‘çœ‹åˆ°çš„ 0.127000
```

#### Step 4: åå‘å‚³æ’­
```python
# åªæ›´æ–° LoRA æ¬Šé‡ï¼Œä¸å‹•åŸæ¨¡å‹
loss.backward()
optimizer.step()  # åªæ›´æ–° lora_A å’Œ lora_B åƒæ•¸
```

### ğŸ“Š **Loss æ•¸å€¼ 0.127000 çš„å…·é«”å«ç¾©**

é€™å€‹æ•¸å€¼è¡¨ç¤ºï¼š
- **å™ªè²é æ¸¬çš„å‡æ–¹èª¤å·®**
- **æ•¸å€¼è¶Šå° = LoRA è¶Šæº–ç¢ºåœ°å­¸æœƒäº†å»å™ªéç¨‹**
- **0.127 æ˜¯ç›¸ç•¶ä¸éŒ¯çš„æ•¸å€¼**ï¼Œè¡¨ç¤ºå·²ç¶“å­¸æœƒäº†ä¸å°‘ç›®æ¨™ç‰¹å¾µ

### ğŸ¯ **200 æ­¥è¨“ç·´æœƒå­¸åˆ°ä»€éº¼ï¼Ÿ**

1. **Step 1-50**: å­¸ç¿’åŸºæœ¬çš„åœ–åƒ-æ–‡å­—å°æ‡‰é—œä¿‚
2. **Step 51-100**: é–‹å§‹è­˜åˆ¥ç‰¹å®šçš„è¦–è¦ºç‰¹å¾µ (å¦‚æ´‹è£çš„å½¢ç‹€)
3. **Step 101-150**: ç´°åŒ–é¢¨æ ¼ç‰¹å¾µ (å¦‚å„ªé›…ã€æ™‚å°šçš„è¦–è¦ºå…ƒç´ )
4. **Step 151-200**: å›ºåŒ–å­¸ç¿’åˆ°çš„æ¦‚å¿µï¼Œæé«˜æº–ç¢ºåº¦

### ğŸ” **ç‚ºä»€éº¼ä¸ç”Ÿæˆåœ–ç‰‡ï¼Ÿ**

å› ç‚º LoRA è¨“ç·´æ˜¯ï¼š
- **å­¸ç¿’éç¨‹**ï¼šæ•™ AI å¦‚ä½•ä¿®æ”¹ç”Ÿæˆè¡Œç‚º
- **ä¸æ˜¯ç”Ÿæˆéç¨‹**ï¼šä¸ç›´æ¥ç”¢å‡ºåœ–ç‰‡
- **é¡æ¯”å­¸ç¿’èªè¨€**ï¼šå°±åƒå­¸ç¿’èªæ³•è¦å‰‡ï¼Œä¸æ˜¯åœ¨å¯«æ–‡ç« 

### ğŸ’¡ **è¨“ç·´å®Œæˆå¾Œå¦‚ä½•ä½¿ç”¨ï¼Ÿ**

```python
# è¼‰å…¥è¨“ç·´å¥½çš„ LoRA
model.load_lora_weights("lora_output/fashion_lora.safetensors")

# ç¾åœ¨ç”Ÿæˆåœ–ç‰‡æ™‚æœƒè‡ªå‹•æ‡‰ç”¨å­¸åˆ°çš„é¢¨æ ¼
generated_image = model.generate("elegant evening dress")  # æœƒæ›´å®¹æ˜“ç”Ÿæˆå„ªé›…æ´‹è£
```

é€™å°±æ˜¯ç‚ºä»€éº¼æˆ‘å€‘åœ¨è¨“ç·´æœŸé–“åªèƒ½çœ‹åˆ° loss ä¸‹é™ï¼Œè€Œè¦åœ¨ WebUI ä¸­æ‰èƒ½çœ‹åˆ°å¯¦éš›æ•ˆæœï¼

## ğŸš€ **ç‚ºä»€éº¼ LoRA è¨“ç·´å¦‚æ­¤æœ‰æ•ˆï¼Ÿ**

### 1. **åƒæ•¸æ•ˆç‡æ¥µé«˜**
```python
# SD 1.5 å®Œæ•´æ¨¡å‹: ~860M åƒæ•¸
# LoRA (rank=32): ~25M åƒæ•¸ (åªæœ‰ 2.9%)
# ä½†æ•ˆæœå»éå¸¸é¡¯è‘—ï¼
```

### 2. **ä½ç§©å‡è¨­çš„æ•¸å­¸åŸºç¤**
```python
# å¤§éƒ¨åˆ†æ·±åº¦å­¸ç¿’æ¬Šé‡çŸ©é™£éƒ½æ˜¯"ä½ç§©"çš„
# æ„æ€æ˜¯å¤§éƒ¨åˆ†ä¿¡æ¯å¯ä»¥ç”¨æ›´å°çš„çŸ©é™£è¡¨ç¤º
W_full = U @ Î£ @ V.T  # SVD åˆ†è§£
# LoRA å°±æ˜¯å­¸ç¿’é€™å€‹ä½ç¶­è¡¨ç¤º
```

### 3. **æ³¨æ„åŠ›æ©Ÿåˆ¶çš„å¨åŠ›**
```python
# LoRA ä¸»è¦ä¿®æ”¹ Attention å±¤
# Attention æ±ºå®šäº†"ä»€éº¼ç‰¹å¾µèˆ‡ä»€éº¼ç‰¹å¾µç›¸é—œ"
# å°‘é‡ä¿®æ”¹å°±èƒ½å¤§å¹…æ”¹è®Šç”Ÿæˆçµæœ
```

## ğŸ¨ **å¯¦æˆ°ä¾‹å­ï¼šæ™‚å°š LoRA çš„å­¸ç¿’éç¨‹**

### å‡è¨­æˆ‘å€‘çš„è¨“ç·´æ•¸æ“š
```
åœ–ç‰‡1: elegant_dress.jpg + "elegant evening dress, black, flowing"
åœ–ç‰‡2: casual_dress.jpg + "casual summer dress, floral, light"  
åœ–ç‰‡3: formal_dress.jpg + "formal business dress, navy, tailored"
```

### LoRA å­¸ç¿’çš„éç¨‹
```python
# Step 1-50: åŸºç¤å­¸ç¿’
å­¸ç¿’ï¼šçœ‹åˆ° "dress" â†’ æ‡‰è©²ç”Ÿæˆæ´‹è£å½¢ç‹€
å­¸ç¿’ï¼šçœ‹åˆ° "elegant" â†’ æ‡‰è©²å¢åŠ å„ªé›…å…ƒç´ 

# Step 51-100: ç‰¹å¾µçµ„åˆ
å­¸ç¿’ï¼šçœ‹åˆ° "elegant dress" â†’ çµåˆå„ªé›…+æ´‹è£
å­¸ç¿’ï¼šä¸åŒé¡è‰²è©å½™å°æ‡‰ä¸åŒè‰²èª¿

# Step 101-150: é¢¨æ ¼ç´°åŒ–  
å­¸ç¿’ï¼šå„ªé›…æ´‹è£çš„å…·é«”è¦–è¦ºç‰¹å¾µ (æµç·šå‹ã€è³ªæ„Ÿç­‰)
å­¸ç¿’ï¼šæ­£å¼èˆ‡ä¼‘é–’æ´‹è£çš„å·®ç•°

# Step 151-200: æ¦‚å¿µå›ºåŒ–
å­¸ç¿’ï¼šåœ¨å„ç¨®æƒ…å¢ƒä¸‹ç©©å®šç”Ÿæˆç›®æ¨™é¢¨æ ¼
å­¸ç¿’ï¼šèˆ‡å…¶ä»–å…ƒç´ çš„å”èª¿ (èƒŒæ™¯ã€å§¿å‹¢ç­‰)
```

### ğŸ“Š **Loss ä¸‹é™è»Œè·¡çš„æ„ç¾©**
```
Step 1:   Loss = 0.8    (å®Œå…¨ä¸æœƒ)
Step 50:  Loss = 0.4    (é–‹å§‹ç†è§£)  
Step 100: Loss = 0.2    (åŸºæœ¬å­¸æœƒ)
Step 150: Loss = 0.15   (ç›¸ç•¶ç†Ÿç·´)
Step 200: Loss = 0.127  (éå¸¸æº–ç¢º) â† æˆ‘å€‘çœ‹åˆ°çš„æ•¸å€¼
```

## ğŸ”¬ **æ·±å…¥ï¼šAttention æ¬Šé‡çš„ä¿®æ”¹**

### Cross-Attention ä¿®æ”¹ (æ–‡å­—â†’åœ–åƒ)
```python
# åŸå§‹ï¼šçœ‹åˆ° "dress" å¯èƒ½ç”Ÿæˆå„ç¨®æœè£
# LoRAä¿®æ”¹å¾Œï¼šçœ‹åˆ° "dress" æ›´å‚¾å‘ç”Ÿæˆæˆ‘å€‘è¨“ç·´çš„é¢¨æ ¼

# å…·é«”ä¿®æ”¹ï¼š
Q_text = text_embed @ (W_q + lora_q_A @ lora_q_B)  # æŸ¥è©¢å‘é‡ä¿®æ”¹
K_image = image_feat @ (W_k + lora_k_A @ lora_k_B)  # éµå‘é‡ä¿®æ”¹
attention_weights = softmax(Q_text @ K_image.T)     # æ³¨æ„åŠ›æ¬Šé‡æ”¹è®Š
```

### Self-Attention ä¿®æ”¹ (åœ–åƒå…§éƒ¨)
```python  
# åŸå§‹ï¼šåœ–åƒä¸åŒå€åŸŸçš„é—œè¯æ¯”è¼ƒé€šç”¨
# LoRAä¿®æ”¹å¾Œï¼šå­¸æœƒäº†ç‰¹å®šé¢¨æ ¼çš„ç©ºé–“é—œä¿‚

# ä¾‹å¦‚ï¼šå„ªé›…æ´‹è£çš„è¤¶çšºèˆ‡æ•´é«”å½¢ç‹€çš„é—œä¿‚
# ä¾‹å¦‚ï¼šé¡è‰²åœ¨ä¸åŒéƒ¨ä½çš„åˆ†å¸ƒæ¨¡å¼
```

## ğŸ’¡ **ç‚ºä»€éº¼è¦200æ­¥è€Œä¸æ˜¯æ›´å¤šï¼Ÿ**

### è¨“ç·´æ›²ç·šåˆ†æ
```python
# é€šå¸¸çš„ LoRA è¨“ç·´æ›²ç·š
Steps 1-100:    å¿«é€Ÿä¸‹é™ (å­¸ç¿’ä¸»è¦ç‰¹å¾µ)
Steps 100-200:  ç·©æ…¢ä¸‹é™ (ç´°åŒ–ç‰¹å¾µ)  
Steps 200+:     å¯èƒ½éæ“¬åˆ (è¨˜æ†¶å…·é«”åœ–ç‰‡è€Œéå­¸ç¿’æ¦‚å¿µ)
```

### æœ€ä½³åœæ­¢é»
- **å¤ªå°‘ (<100æ­¥)**: å­¸ä¸å®Œæ•´ï¼Œæ•ˆæœä¸æ˜é¡¯
- **é©ä¸­ (100-300æ­¥)**: å­¸æœƒæ¦‚å¿µï¼Œæ•ˆæœé¡¯è‘—
- **å¤ªå¤š (>500æ­¥)**: å¯èƒ½éæ“¬åˆï¼Œå¤±å»æ³›åŒ–èƒ½åŠ›

## ğŸ¯ **ç¸½çµï¼šLoRA åˆ°åº•åœ¨"èª¿"ä»€éº¼**

1. **èª¿æ³¨æ„åŠ›æ¬Šé‡**: æ”¹è®Šæ–‡å­—èˆ‡è¦–è¦ºç‰¹å¾µçš„å°æ‡‰é—œä¿‚
2. **èª¿ç‰¹å¾µæå–**: è®“æ¨¡å‹æ›´å®¹æ˜“è­˜åˆ¥å’Œç”Ÿæˆç‰¹å®šé¢¨æ ¼
3. **èª¿ç”Ÿæˆåå¥½**: åœ¨çœ‹åˆ°ç›¸é—œæç¤ºè©æ™‚ï¼Œå‚¾å‘ç”Ÿæˆç›®æ¨™é¢¨æ ¼
4. **èª¿ç´°ç¯€è¡¨ç¾**: å­¸æœƒç‰¹å®šé¢¨æ ¼çš„ç´°å¾®è¦–è¦ºç‰¹å¾µ

### é¡æ¯”èªªæ˜
```
å°±åƒæ•™ä¸€å€‹ç•«å®¶æ–°çš„ç•«é¢¨ï¼š
- ä¸æ˜¯æ•™ä»–ç•«å…·é«”çš„åœ– (ä¸ç”Ÿæˆåœ–ç‰‡)
- è€Œæ˜¯æ•™ä»–æ–°çš„æŠ€æ³•å’Œåå¥½ (ä¿®æ”¹æ¬Šé‡)
- å­¸æœƒå¾Œï¼Œä»–ç•«ä»€éº¼éƒ½æœƒå¸¶æœ‰é€™ç¨®é¢¨æ ¼ (LoRAæ•ˆæœ)
```

ä½ çœ‹åˆ°çš„ `Loss=0.127000` å°±æ˜¯åœ¨è¡¡é‡"å­¸æœƒé€™ç¨®é¢¨æ ¼"çš„ç¨‹åº¦ï¼
