#!/usr/bin/env python3
"""
æ‰¹é‡è¨“ç·´åŠ©æ‰‹ - å°‡å¤§é‡åœ–ç‰‡åˆ†æ‰¹é€²è¡Œ LoRA è¨“ç·´
"""

import os
import shutil
import argparse
import subprocess
import time
from pathlib import Path
from typing import List, Tuple
import json
import warnings
warnings.filterwarnings('ignore')

class BatchTrainingHelper:
    def __init__(self, source_dir: str, batch_size: int = 50):
        self.source_dir = Path(source_dir)
        self.batch_size = batch_size
        self.work_dir = Path("batch_training_work")
        self.results_dir = Path("batch_training_results")
        
        # å‰µå»ºå·¥ä½œç›®éŒ„
        self.work_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)
        
    def prepare_batches(self) -> List[Tuple[int, List[str]]]:
        """æº–å‚™åˆ†æ‰¹è¨“ç·´çš„åœ–ç‰‡åˆ—è¡¨"""
        if not self.source_dir.exists():
            raise FileNotFoundError(f"æºç›®éŒ„ä¸å­˜åœ¨: {self.source_dir}")
            
        # ç²å–æ‰€æœ‰åœ–ç‰‡æ–‡ä»¶
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png', '.bmp', '.webp']:
            image_files.extend(self.source_dir.glob(f'*{ext}'))
            image_files.extend(self.source_dir.glob(f'*{ext.upper()}'))
        
        if not image_files:
            raise ValueError(f"åœ¨ {self.source_dir} ä¸­æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡æ–‡ä»¶")
        
        print(f"ğŸ“ æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡")
        
        # åˆ†æ‰¹
        batches = []
        for i in range(0, len(image_files), self.batch_size):
            batch_files = image_files[i:i + self.batch_size]
            batches.append((i // self.batch_size + 1, [str(f) for f in batch_files]))
        
        print(f"ğŸ“Š å°‡åˆ†æˆ {len(batches)} æ‰¹ï¼Œæ¯æ‰¹ç´„ {self.batch_size} å¼µ")
        return batches
    
    def setup_batch_training_dir(self, batch_num: int, batch_files: List[str]) -> str:
        """è¨­ç½®æ‰¹æ¬¡è¨“ç·´ç›®éŒ„"""
        batch_dir = self.work_dir / f"batch_{batch_num}"
        train_dir = batch_dir / "lora_train_set" / "10_test"
        
        # æ¸…ç†ä¸¦å‰µå»ºç›®éŒ„
        if batch_dir.exists():
            shutil.rmtree(batch_dir)
        train_dir.mkdir(parents=True, exist_ok=True)
        
        # è¤‡è£½åœ–ç‰‡åˆ°æ‰¹æ¬¡ç›®éŒ„
        for img_path in batch_files:
            img_file = Path(img_path)
            if img_file.exists():
                shutil.copy2(img_file, train_dir / img_file.name)
        
        print(f"ğŸ“ æ‰¹æ¬¡ {batch_num} æº–å‚™å®Œæˆ: {len(batch_files)} å¼µåœ–ç‰‡")
        return str(batch_dir)
    
    def run_batch_training(self, batch_num: int, batch_dir: str, continue_training: bool = False) -> dict:
        """åŸ·è¡Œæ‰¹æ¬¡è¨“ç·´"""
        print(f"\nğŸš€ é–‹å§‹æ‰¹æ¬¡ {batch_num} è¨“ç·´...")
        
        # åˆ‡æ›åˆ°æ‰¹æ¬¡ç›®éŒ„
        original_cwd = os.getcwd()
        os.chdir(batch_dir)
        
        try:
            # æº–å‚™è¨“ç·´å‘½ä»¤
            script_path = Path(original_cwd) / "train_lora_monitored.py"
            
            cmd = [
                "python", str(script_path),
                "--new" if not continue_training else "--continue"
            ]
            
            print(f"ğŸ“ åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # åŸ·è¡Œè¨“ç·´
            start_time = time.time()
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',
                errors='replace',
                timeout=14400  # 4å°æ™‚è¶…æ™‚
            )
            
            end_time = time.time()
            training_time = end_time - start_time
            
            # æª¢æŸ¥çµæœ
            success = result.returncode == 0
            
            # ç§»å‹•çµæœåˆ°çµæœç›®éŒ„
            lora_output = Path("lora_output")
            batch_result_dir = self.results_dir / f"batch_{batch_num}"
            
            if lora_output.exists():
                if batch_result_dir.exists():
                    shutil.rmtree(batch_result_dir)
                shutil.move(str(lora_output), str(batch_result_dir))
            
            # è¿”å›çµæœ
            return {
                "batch_num": batch_num,
                "success": success,
                "training_time": training_time,
                "returncode": result.returncode,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "result_dir": str(batch_result_dir) if batch_result_dir.exists() else None
            }
            
        except subprocess.TimeoutExpired:
            return {
                "batch_num": batch_num,
                "success": False,
                "training_time": 14400,
                "error": "è¨“ç·´è¶…æ™‚ï¼ˆ4å°æ™‚ï¼‰",
                "returncode": -1
            }
        except Exception as e:
            return {
                "batch_num": batch_num,
                "success": False,
                "training_time": 0,
                "error": str(e),
                "returncode": -1
            }
        finally:
            os.chdir(original_cwd)
    
    def merge_models(self, batch_results: List[dict]) -> str:
        """åˆä½µå¤šå€‹æ‰¹æ¬¡çš„æ¨¡å‹ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰"""
        print("\nğŸ”„ åˆä½µæ‰¹æ¬¡è¨“ç·´çµæœ...")
        
        # æ‰¾åˆ°æ‰€æœ‰æˆåŠŸçš„æ‰¹æ¬¡
        successful_batches = [r for r in batch_results if r["success"] and r.get("result_dir")]
        
        if not successful_batches:
            print("âŒ æ²’æœ‰æˆåŠŸçš„æ‰¹æ¬¡å¯ä»¥åˆä½µ")
            return ""
        
        # å‰µå»ºåˆä½µçµæœç›®éŒ„
        merged_dir = self.results_dir / "merged_result"
        merged_dir.mkdir(exist_ok=True)
        
        # ç°¡å–®åˆä½µï¼šè¤‡è£½ç¬¬ä¸€å€‹æˆåŠŸçš„æ¨¡å‹ä½œç‚ºåŸºç¤
        # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡éœ€è¦æ›´å¾©é›œçš„æ¨¡å‹åˆä½µé‚è¼¯
        first_batch = successful_batches[0]
        first_batch_dir = Path(first_batch["result_dir"])
        
        for item in first_batch_dir.iterdir():
            if item.is_file():
                shutil.copy2(item, merged_dir / item.name)
        
        print(f"ğŸ“ åˆä½µçµæœä¿å­˜åˆ°: {merged_dir}")
        print("âš ï¸  æ³¨æ„ï¼šç•¶å‰ä½¿ç”¨ç°¡å–®åˆä½µç­–ç•¥ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­éœ€è¦æ›´å¾©é›œçš„æ¨¡å‹èåˆ")
        
        return str(merged_dir)
    
    def run_full_batch_training(self, continue_training: bool = False) -> dict:
        """åŸ·è¡Œå®Œæ•´çš„æ‰¹æ¬¡è¨“ç·´æµç¨‹"""
        print("ğŸš€ é–‹å§‹æ‰¹æ¬¡è¨“ç·´æµç¨‹...")
        
        # æº–å‚™æ‰¹æ¬¡
        batches = self.prepare_batches()
        
        # åŸ·è¡Œæ‰€æœ‰æ‰¹æ¬¡
        batch_results = []
        total_start_time = time.time()
        
        for batch_num, batch_files in batches:
            print(f"\n{'='*60}")
            print(f"ğŸ“¦ è™•ç†æ‰¹æ¬¡ {batch_num}/{len(batches)}")
            print(f"{'='*60}")
            
            # è¨­ç½®æ‰¹æ¬¡ç›®éŒ„
            batch_dir = self.setup_batch_training_dir(batch_num, batch_files)
            
            # åŸ·è¡Œè¨“ç·´
            result = self.run_batch_training(batch_num, batch_dir, continue_training)
            batch_results.append(result)
            
            # é¡¯ç¤ºçµæœ
            if result["success"]:
                print(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ ({result['training_time']:.1f}ç§’)")
            else:
                print(f"âŒ æ‰¹æ¬¡ {batch_num} å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
        
        total_end_time = time.time()
        total_time = total_end_time - total_start_time
        
        # çµ±è¨ˆçµæœ
        successful_batches = [r for r in batch_results if r["success"]]
        success_rate = len(successful_batches) / len(batch_results) * 100
        
        print(f"\n{'='*60}")
        print("ğŸ“Š æ‰¹æ¬¡è¨“ç·´å®Œæˆçµ±è¨ˆ")
        print(f"{'='*60}")
        print(f"ç¸½æ‰¹æ¬¡æ•¸: {len(batch_results)}")
        print(f"æˆåŠŸæ‰¹æ¬¡: {len(successful_batches)}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"ç¸½æ™‚é–“: {total_time/3600:.1f} å°æ™‚")
        
        # åˆä½µçµæœ
        merged_result = ""
        if successful_batches:
            merged_result = self.merge_models(batch_results)
        
        # ä¿å­˜å®Œæ•´å ±å‘Š
        report = {
            "total_batches": len(batch_results),
            "successful_batches": len(successful_batches),
            "success_rate": success_rate,
            "total_time": total_time,
            "batch_results": batch_results,
            "merged_result": merged_result,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        report_file = self.results_dir / "batch_training_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ å®Œæ•´å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return report

def main():
    parser = argparse.ArgumentParser(description="LoRA æ‰¹æ¬¡è¨“ç·´åŠ©æ‰‹")
    parser.add_argument("source_dir", help="æºåœ–ç‰‡ç›®éŒ„")
    parser.add_argument("--batch-size", type=int, default=50, help="æ¯æ‰¹æ¬¡åœ–ç‰‡æ•¸é‡ (é»˜èª: 50)")
    parser.add_argument("--continue", action="store_true", help="å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æºç›®éŒ„
    if not os.path.exists(args.source_dir):
        print(f"âŒ æºç›®éŒ„ä¸å­˜åœ¨: {args.source_dir}")
        return 1
    
    # å‰µå»ºæ‰¹æ¬¡è¨“ç·´åŠ©æ‰‹
    helper = BatchTrainingHelper(args.source_dir, args.batch_size)
    
    try:
        # åŸ·è¡Œæ‰¹æ¬¡è¨“ç·´
        report = helper.run_full_batch_training(getattr(args, 'continue'))
        
        if report["success_rate"] > 0:
            print(f"\nğŸ‰ æ‰¹æ¬¡è¨“ç·´å®Œæˆï¼æˆåŠŸç‡: {report['success_rate']:.1f}%")
            return 0
        else:
            print(f"\nâŒ æ‰¹æ¬¡è¨“ç·´å¤±æ•—ï¼")
            return 1
            
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
