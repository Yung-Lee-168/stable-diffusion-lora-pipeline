#!/usr/bin/env python3
"""
LoRA è¨“ç·´é€²åº¦ç›£æ§å™¨
åœ¨ train_network.py è¨“ç·´éç¨‹ä¸­å³æ™‚ç›£æ§å’Œè¨˜éŒ„æŠ€è¡“æŒ‡æ¨™

åŠŸèƒ½ï¼š
1. ç›£æ§è¨“ç·´æå¤±å’Œå­¸ç¿’ç‡
2. è¨˜éŒ„ä¸­é–“æª¢æŸ¥é»çš„æ€§èƒ½æŒ‡æ¨™
3. æä¾›æ—©åœæ©Ÿåˆ¶å»ºè­°
4. ç”Ÿæˆè¨“ç·´é€²åº¦å ±å‘Š
5. æ±ºå®šæ˜¯å¦ç¹¼çºŒåˆ°æ¨ç†éšæ®µ
"""

import os
import json
import time
import datetime
import subprocess
import sys
from typing import Dict, List, Any, Optional
import numpy as np
import matplotlib.pyplot as plt
from dataclasses import dataclass
import threading
import queue
import logging

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class TrainingProgress:
    """è¨“ç·´é€²åº¦æ•¸æ“šé¡"""
    step: int
    epoch: int
    loss: float
    lr: float
    timestamp: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'step': self.step,
            'epoch': self.epoch,
            'loss': self.loss,
            'lr': self.lr,
            'timestamp': self.timestamp
        }

class LoRATrainingMonitor:
    """LoRA è¨“ç·´ç›£æ§å™¨"""
    
    def __init__(self, 
                 training_dir: str = ".",
                 log_dir: str = "training_logs",
                 checkpoint_interval: int = 100,
                 early_stop_patience: int = 500):
        self.training_dir = training_dir
        self.log_dir = log_dir
        self.checkpoint_interval = checkpoint_interval
        self.early_stop_patience = early_stop_patience
        
        # å»ºç«‹æ—¥èªŒç›®éŒ„
        os.makedirs(log_dir, exist_ok=True)
        
        # è¨“ç·´æ•¸æ“šè¨˜éŒ„
        self.training_history: List[TrainingProgress] = []
        self.loss_history: List[float] = []
        self.lr_history: List[float] = []
        self.best_loss = float('inf')
        self.steps_without_improvement = 0
        
        # ç›£æ§é–¾å€¼
        self.thresholds = {
            'loss_spike_threshold': 2.0,  # æå¤±çªç„¶å¢åŠ çš„é–¾å€¼
            'loss_plateau_threshold': 0.001,  # æå¤±å¹³ç©©çš„é–¾å€¼
            'min_loss_improvement': 0.01,  # æœ€å°æå¤±æ”¹å–„
            'target_loss': 0.1,  # ç›®æ¨™æå¤±å€¼
            'max_lr_decay': 0.1,  # æœ€å¤§å­¸ç¿’ç‡è¡°æ¸›
        }
        
        # è¨­å®šæ—¥èªŒ
        self.setup_logging()
        
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_file = os.path.join(self.log_dir, f"training_monitor_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def parse_training_log(self, log_line: str) -> Optional[TrainingProgress]:
        """è§£æè¨“ç·´æ—¥èªŒè¡Œ"""
        try:
            # è§£æé¡ä¼¼é€™æ¨£çš„æ—¥èªŒè¡Œï¼š
            # "epoch 1, step 100, loss: 0.1234, lr: 0.0001"
            if "loss:" in log_line and "lr:" in log_line:
                parts = log_line.split()
                
                # æå– epoch
                epoch = 0
                if "epoch" in log_line:
                    epoch_idx = parts.index("epoch") + 1
                    if epoch_idx < len(parts):
                        epoch = int(parts[epoch_idx].rstrip(','))
                
                # æå– step
                step = 0
                if "step" in log_line:
                    step_idx = parts.index("step") + 1
                    if step_idx < len(parts):
                        step = int(parts[step_idx].rstrip(','))
                
                # æå– loss
                loss = 0.0
                if "loss:" in log_line:
                    loss_idx = parts.index("loss:") + 1
                    if loss_idx < len(parts):
                        loss = float(parts[loss_idx].rstrip(','))
                
                # æå– lr
                lr = 0.0
                if "lr:" in log_line:
                    lr_idx = parts.index("lr:") + 1
                    if lr_idx < len(parts):
                        lr = float(parts[lr_idx].rstrip(','))
                
                return TrainingProgress(
                    step=step,
                    epoch=epoch,
                    loss=loss,
                    lr=lr,
                    timestamp=datetime.datetime.now().isoformat()
                )
                
        except Exception as e:
            self.logger.warning(f"è§£ææ—¥èªŒè¡Œå¤±æ•—: {log_line}, éŒ¯èª¤: {e}")
            
        return None
        
    def update_training_progress(self, progress: TrainingProgress):
        """æ›´æ–°è¨“ç·´é€²åº¦"""
        self.training_history.append(progress)
        self.loss_history.append(progress.loss)
        self.lr_history.append(progress.lr)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        if progress.loss < self.best_loss:
            improvement = self.best_loss - progress.loss
            if improvement >= self.thresholds['min_loss_improvement']:
                self.best_loss = progress.loss
                self.steps_without_improvement = 0
                self.logger.info(f"ğŸ¯ æå¤±æ”¹å–„: {improvement:.4f}, æ–°æœ€ä½³: {self.best_loss:.4f}")
            else:
                self.steps_without_improvement += 1
        else:
            self.steps_without_improvement += 1
            
        # è¨˜éŒ„é€²åº¦
        self.logger.info(f"ğŸ“Š Step {progress.step}: Loss={progress.loss:.4f}, LR={progress.lr:.6f}")
        
        # æª¢æŸ¥é è­¦æ¢ä»¶
        self.check_training_alerts(progress)
        
        # å®šæœŸä¿å­˜é€²åº¦
        if progress.step % self.checkpoint_interval == 0:
            self.save_progress_checkpoint(progress.step)
            
    def check_training_alerts(self, progress: TrainingProgress):
        """æª¢æŸ¥è¨“ç·´é è­¦"""
        alerts = []
        
        # æª¢æŸ¥æå¤±çªç„¶å¢åŠ 
        if len(self.loss_history) >= 2:
            current_loss = progress.loss
            prev_loss = self.loss_history[-2]
            
            if current_loss > prev_loss * self.thresholds['loss_spike_threshold']:
                alerts.append(f"âš ï¸ æå¤±çªç„¶å¢åŠ : {prev_loss:.4f} â†’ {current_loss:.4f}")
                
        # æª¢æŸ¥å­¸ç¿’ç‡ç•°å¸¸
        if progress.lr < self.thresholds['max_lr_decay']:
            alerts.append(f"âš ï¸ å­¸ç¿’ç‡éä½: {progress.lr:.6f}")
            
        # æª¢æŸ¥æ—©åœæ¢ä»¶
        if self.steps_without_improvement >= self.early_stop_patience:
            alerts.append(f"ğŸ›‘ å»ºè­°æ—©åœ: {self.steps_without_improvement} æ­¥ç„¡æ”¹å–„")
            
        # æª¢æŸ¥ç›®æ¨™é”æˆ
        if progress.loss <= self.thresholds['target_loss']:
            alerts.append(f"ğŸ¯ é”åˆ°ç›®æ¨™æå¤±: {progress.loss:.4f}")
            
        # è¼¸å‡ºé è­¦
        for alert in alerts:
            self.logger.warning(alert)
            
    def save_progress_checkpoint(self, step: int):
        """ä¿å­˜é€²åº¦æª¢æŸ¥é»"""
        checkpoint_data = {
            'step': step,
            'timestamp': datetime.datetime.now().isoformat(),
            'training_history': [p.to_dict() for p in self.training_history],
            'best_loss': self.best_loss,
            'steps_without_improvement': self.steps_without_improvement,
            'thresholds': self.thresholds
        }
        
        checkpoint_file = os.path.join(self.log_dir, f"training_checkpoint_{step}.json")
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"ğŸ’¾ ä¿å­˜æª¢æŸ¥é»: {checkpoint_file}")
        
    def generate_training_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè¨“ç·´å ±å‘Š"""
        if not self.training_history:
            return {}
            
        report = {
            'training_summary': {
                'total_steps': len(self.training_history),
                'start_time': self.training_history[0].timestamp,
                'end_time': self.training_history[-1].timestamp,
                'best_loss': self.best_loss,
                'final_loss': self.training_history[-1].loss,
                'final_lr': self.training_history[-1].lr
            },
            'training_metrics': {
                'loss_improvement': self.training_history[0].loss - self.best_loss,
                'loss_reduction_rate': (self.training_history[0].loss - self.best_loss) / self.training_history[0].loss * 100,
                'average_loss': sum(self.loss_history) / len(self.loss_history),
                'loss_variance': np.var(self.loss_history),
                'steps_without_improvement': self.steps_without_improvement
            },
            'training_evaluation': self.evaluate_training_performance(),
            'recommendations': self.generate_training_recommendations()
        }
        
        return report
        
    def evaluate_training_performance(self) -> Dict[str, Any]:
        """è©•ä¼°è¨“ç·´è¡¨ç¾"""
        if not self.training_history:
            return {}
            
        # è¨ˆç®—è¨“ç·´æ•ˆç‡
        loss_improvement = self.training_history[0].loss - self.best_loss
        steps_taken = len(self.training_history)
        efficiency = loss_improvement / steps_taken if steps_taken > 0 else 0
        
        # è¨ˆç®—æ”¶æ–‚æ€§
        recent_losses = self.loss_history[-10:] if len(self.loss_history) >= 10 else self.loss_history
        loss_stability = 1.0 - (np.std(recent_losses) / np.mean(recent_losses)) if recent_losses else 0
        
        # è©•ä¼°ç­‰ç´š
        performance_grade = "poor"
        if self.best_loss <= self.thresholds['target_loss']:
            performance_grade = "excellent"
        elif loss_improvement >= 0.1:
            performance_grade = "good"
        elif loss_improvement >= 0.05:
            performance_grade = "average"
            
        return {
            'efficiency': efficiency,
            'loss_stability': loss_stability,
            'performance_grade': performance_grade,
            'convergence_rate': self.calculate_convergence_rate(),
            'training_effectiveness': min(efficiency * 100, 100)
        }
        
    def calculate_convergence_rate(self) -> float:
        """è¨ˆç®—æ”¶æ–‚ç‡"""
        if len(self.loss_history) < 10:
            return 0.0
            
        # è¨ˆç®—æœ€è¿‘ 10 æ­¥çš„å¹³å‡æ”¹å–„ç‡
        recent_losses = self.loss_history[-10:]
        improvements = []
        
        for i in range(1, len(recent_losses)):
            if recent_losses[i-1] > 0:
                improvement = (recent_losses[i-1] - recent_losses[i]) / recent_losses[i-1]
                improvements.append(improvement)
                
        return np.mean(improvements) if improvements else 0.0
        
    def generate_training_recommendations(self) -> List[str]:
        """ç”Ÿæˆè¨“ç·´å»ºè­°"""
        recommendations = []
        
        if not self.training_history:
            return recommendations
            
        # åŸºæ–¼æå¤±æ”¹å–„çš„å»ºè­°
        loss_improvement = self.training_history[0].loss - self.best_loss
        if loss_improvement < 0.01:
            recommendations.append("ğŸ”§ æå¤±æ”¹å–„ä¸è¶³ï¼Œå»ºè­°èª¿æ•´å­¸ç¿’ç‡æˆ–å¢åŠ è¨“ç·´æ­¥æ•¸")
            
        # åŸºæ–¼æ”¶æ–‚æ€§çš„å»ºè­°
        convergence_rate = self.calculate_convergence_rate()
        if convergence_rate < 0.001:
            recommendations.append("ğŸ“ˆ æ”¶æ–‚ç·©æ…¢ï¼Œå»ºè­°æª¢æŸ¥æ•¸æ“šè³ªé‡æˆ–èª¿æ•´å„ªåŒ–å™¨")
            
        # åŸºæ–¼æ—©åœçš„å»ºè­°
        if self.steps_without_improvement >= self.early_stop_patience // 2:
            recommendations.append("â¹ï¸ å¯èƒ½éœ€è¦æ—©åœï¼Œæˆ–å˜—è©¦é™ä½å­¸ç¿’ç‡")
            
        # åŸºæ–¼ç›®æ¨™é”æˆçš„å»ºè­°
        if self.best_loss <= self.thresholds['target_loss']:
            recommendations.append("âœ… å·²é”åˆ°ç›®æ¨™æå¤±ï¼Œå¯ä»¥é€²è¡Œæ¨ç†æ¸¬è©¦")
        elif self.best_loss <= self.thresholds['target_loss'] * 2:
            recommendations.append("ğŸ¯ æ¥è¿‘ç›®æ¨™æå¤±ï¼Œå»ºè­°ç¹¼çºŒå¾®èª¿")
            
        return recommendations
        
    def should_continue_to_inference(self) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ç¹¼çºŒåˆ°æ¨ç†éšæ®µ"""
        if not self.training_history:
            return False
            
        # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™æå¤±
        if self.best_loss <= self.thresholds['target_loss']:
            return True
            
        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„æ”¹å–„
        if len(self.training_history) >= 2:
            loss_improvement = self.training_history[0].loss - self.best_loss
            if loss_improvement >= 0.05:  # è‡³å°‘ 5% çš„æ”¹å–„
                return True
                
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“ç©©å®š
        if self.steps_without_improvement >= self.early_stop_patience:
            return True
            
        return False
        
    def generate_training_charts(self, output_path: str = "training_progress.png"):
        """ç”Ÿæˆè¨“ç·´åœ–è¡¨"""
        if not self.training_history:
            return
            
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('LoRA è¨“ç·´é€²åº¦ç›£æ§', fontsize=16, fontweight='bold')
        
        steps = [p.step for p in self.training_history]
        
        # 1. æå¤±æ›²ç·š
        ax1.plot(steps, self.loss_history, 'b-', linewidth=2, label='Training Loss')
        ax1.axhline(y=self.best_loss, color='r', linestyle='--', alpha=0.7, label=f'Best Loss: {self.best_loss:.4f}')
        ax1.axhline(y=self.thresholds['target_loss'], color='g', linestyle='--', alpha=0.7, label=f'Target: {self.thresholds["target_loss"]:.4f}')
        ax1.set_title('è¨“ç·´æå¤±æ›²ç·š')
        ax1.set_xlabel('æ­¥æ•¸')
        ax1.set_ylabel('æå¤±å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å­¸ç¿’ç‡æ›²ç·š
        ax2.plot(steps, self.lr_history, 'g-', linewidth=2, label='Learning Rate')
        ax2.set_title('å­¸ç¿’ç‡è®ŠåŒ–')
        ax2.set_xlabel('æ­¥æ•¸')
        ax2.set_ylabel('å­¸ç¿’ç‡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æå¤±æ”¹å–„åˆ†å¸ƒ
        if len(self.loss_history) >= 10:
            loss_improvements = []
            for i in range(1, len(self.loss_history)):
                improvement = self.loss_history[i-1] - self.loss_history[i]
                loss_improvements.append(improvement)
                
            ax3.hist(loss_improvements, bins=20, alpha=0.7, color='orange', edgecolor='black')
            ax3.set_title('æå¤±æ”¹å–„åˆ†å¸ƒ')
            ax3.set_xlabel('æå¤±æ”¹å–„å€¼')
            ax3.set_ylabel('é »ç‡')
            ax3.grid(True, alpha=0.3)
        
        # 4. è¨“ç·´æ•ˆç‡
        if len(self.training_history) >= 10:
            window_size = 10
            efficiency_values = []
            window_steps = []
            
            for i in range(window_size, len(self.training_history)):
                window_losses = self.loss_history[i-window_size:i]
                improvement = window_losses[0] - window_losses[-1]
                efficiency = improvement / window_size
                efficiency_values.append(efficiency)
                window_steps.append(steps[i])
                
            ax4.plot(window_steps, efficiency_values, 'purple', linewidth=2, label='è¨“ç·´æ•ˆç‡')
            ax4.set_title('è¨“ç·´æ•ˆç‡è¶¨å‹¢')
            ax4.set_xlabel('æ­¥æ•¸')
            ax4.set_ylabel('æ•ˆç‡å€¼')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        self.logger.info(f"ğŸ“Š è¨“ç·´åœ–è¡¨å·²ä¿å­˜: {output_path}")
        
    def monitor_training_process(self, training_command: str, max_wait_time: int = 3600):
        """ç›£æ§è¨“ç·´éç¨‹"""
        self.logger.info(f"ğŸš€ é–‹å§‹ç›£æ§è¨“ç·´éç¨‹: {training_command}")
        
        try:
            # å•Ÿå‹•è¨“ç·´éç¨‹
            process = subprocess.Popen(
                training_command,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            start_time = time.time()
            
            # ç›£æ§è¼¸å‡º
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                    
                if output:
                    # è§£æè¨“ç·´æ—¥èªŒ
                    progress = self.parse_training_log(output.strip())
                    if progress:
                        self.update_training_progress(progress)
                    
                    # è¼¸å‡ºåŸå§‹æ—¥èªŒ
                    print(output.strip())
                    
                # æª¢æŸ¥è¶…æ™‚
                if time.time() - start_time > max_wait_time:
                    self.logger.warning(f"â° è¨“ç·´è¶…æ™‚ ({max_wait_time} ç§’)")
                    process.terminate()
                    break
                    
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait()
            
            # ç”Ÿæˆæœ€çµ‚å ±å‘Š
            report = self.generate_training_report()
            report_file = os.path.join(self.log_dir, f"training_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
                
            # ç”Ÿæˆåœ–è¡¨
            chart_file = os.path.join(self.log_dir, f"training_chart_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            self.generate_training_charts(chart_file)
            
            self.logger.info(f"âœ… è¨“ç·´ç›£æ§å®Œæˆ, è¿”å›ç¢¼: {return_code}")
            self.logger.info(f"ğŸ“‹ å ±å‘Šå·²ä¿å­˜: {report_file}")
            
            return return_code == 0, report
            
        except Exception as e:
            self.logger.error(f"âŒ ç›£æ§è¨“ç·´å¤±æ•—: {e}")
            return False, {}

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´é€²åº¦ç›£æ§å™¨")
    parser.add_argument("--training-command", required=True, help="è¨“ç·´å‘½ä»¤")
    parser.add_argument("--log-dir", default="training_logs", help="æ—¥èªŒç›®éŒ„")
    parser.add_argument("--checkpoint-interval", type=int, default=100, help="æª¢æŸ¥é»é–“éš”")
    parser.add_argument("--early-stop-patience", type=int, default=500, help="æ—©åœè€å¿ƒå€¼")
    parser.add_argument("--max-wait-time", type=int, default=3600, help="æœ€å¤§ç­‰å¾…æ™‚é–“")
    
    args = parser.parse_args()
    
    # å»ºç«‹ç›£æ§å™¨
    monitor = LoRATrainingMonitor(
        log_dir=args.log_dir,
        checkpoint_interval=args.checkpoint_interval,
        early_stop_patience=args.early_stop_patience
    )
    
    # é–‹å§‹ç›£æ§
    success, report = monitor.monitor_training_process(args.training_command, args.max_wait_time)
    
    if success:
        print("âœ… è¨“ç·´å®Œæˆ")
        
        # æª¢æŸ¥æ˜¯å¦æ‡‰è©²ç¹¼çºŒæ¨ç†
        if monitor.should_continue_to_inference():
            print("ğŸ¯ å»ºè­°ç¹¼çºŒé€²è¡Œæ¨ç†æ¸¬è©¦")
            return 0
        else:
            print("âš ï¸ å»ºè­°èª¿æ•´åƒæ•¸å¾Œé‡æ–°è¨“ç·´")
            return 1
    else:
        print("âŒ è¨“ç·´å¤±æ•—")
        return 2

if __name__ == "__main__":
    sys.exit(main())
