#!/usr/bin/env python3
"""
LoRA è¨“ç·´è…³æœ¬ - å®Œæ•´ç›£æ§ç‰ˆæœ¬
åŸºæ–¼ train_lora.py çš„æ ¸å¿ƒé‚è¼¯ï¼Œå¢åŠ å®Œæ•´çš„ç›£æ§ã€è©•ä¼°å’Œè‡ªå‹•å»ºè­°åŠŸèƒ½
"""

import subprocess
import os
import sys
import warnings
import argparse
import datetime
import logging
import json
import shutil
import re
import time
from typing import Dict, List, Tuple, Optional
from PIL import Image

# è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
os.environ['DISABLE_XFORMERS'] = '1'
os.environ['XFORMERS_MORE_DETAILS'] = '0'
os.environ['PYTHONWARNINGS'] = 'ignore'

# æ¸›å°‘è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")
warnings.filterwarnings("ignore", message=".*torch.utils._pytree.*")
warnings.filterwarnings("ignore", message=".*diffusers.*")

# ç¢ºä¿åœ¨è…³æœ¬æ‰€åœ¨ç›®éŒ„åŸ·è¡Œ
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
print(f"ğŸ“ åˆ‡æ›åˆ°è…³æœ¬ç›®éŒ„: {script_dir}")

# ==================== å›ºå®šåƒæ•¸è¨­å®š ====================
# èˆ‡ train_lora.py ä¿æŒä¸€è‡´çš„å›ºå®šåƒæ•¸
FIXED_TRAINING_PARAMS = {
    "max_train_steps": 100,         # å›ºå®š 100 æ­¥
    "learning_rate": 5e-5,          # å›ºå®šå­¸ç¿’ç‡
    "network_dim": 32,              # ç¶²è·¯ç¶­åº¦
    "save_every_n_epochs": 50,      # å„²å­˜é »ç‡
}

print("ğŸ“‹ å›ºå®šåƒæ•¸è¨­å®š:")
for key, value in FIXED_TRAINING_PARAMS.items():
    print(f"  {key}: {value}")
print("=" * 60)
# ==================== åƒæ•¸è¨­å®šçµæŸ ====================

class LoRATrainingMonitor:
    """LoRA è¨“ç·´ç›£æ§å™¨ - å®Œæ•´ç‰ˆæœ¬"""
    
    def __init__(self, continue_from_checkpoint: bool = False):
        self.continue_from_checkpoint = continue_from_checkpoint
        self.logger = self.setup_logging()
        
    def setup_logging(self) -> logging.Logger:
        """è¨­å®šæ—¥èªŒç³»çµ±"""
        log_dir = "training_logs"
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(log_dir, f"lora_training_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ“‹ æ—¥èªŒæª”æ¡ˆ: {log_file}")
        return logger
        
    def find_latest_lora(self) -> Optional[str]:
        """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ"""
        lora_path = "lora_output"
        if not os.path.exists(lora_path):
            return None
        
        lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
        if not lora_files:
            return None
        
        # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
        latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
        return os.path.join(lora_path, latest_lora)
        
    def backup_existing_lora(self) -> Optional[str]:
        """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹"""
        existing_lora = self.find_latest_lora()
        if existing_lora and os.path.exists(existing_lora):
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"lora_backup_{timestamp}.safetensors"
            backup_path = os.path.join("lora_output", backup_name)
            
            shutil.copy2(existing_lora, backup_path)
            print(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
            self.logger.info(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
            return backup_path
        return None
        
    def check_image_requirements(self, data_folder: str, target_size: int = 512) -> bool:
        """æª¢æŸ¥åœ–ç‰‡è¦æ±‚ - èˆ‡ train_lora.py å®Œå…¨ä¸€è‡´"""
        print(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
        
        if not os.path.exists(data_folder):
            print(f"âŒ è³‡æ–™å¤¾ä¸å­˜åœ¨: {data_folder}")
            return False
            
        files = os.listdir(data_folder)
        img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
        if not img_files:
            print("âŒ æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡æª”æ¡ˆ")
            return False
            
        valid_count = 0
        invalid_files = []
        
        for img_file in img_files:
            img_path = os.path.join(data_folder, img_file)
            try:
                with Image.open(img_path) as img:
                    width, height = img.size
                    
                    # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                    if width <= target_size and height <= target_size:
                        valid_count += 1
                        print(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                    else:
                        invalid_files.append((img_file, width, height))
                        print(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                        
            except Exception as e:
                print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
                invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
        
        print(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
        print(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
        print(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
        
        if invalid_files:
            print(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
            for img_file, width, height in invalid_files:
                print(f"   - {img_file}: {width}x{height}")
            print(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        
        if valid_count == 0:
            print(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
            return False
        else:
            print(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
            return True
        
    def check_training_requirements(self) -> bool:
        """æª¢æŸ¥è¨“ç·´éœ€æ±‚"""
        self.logger.info("ğŸ” æª¢æŸ¥è¨“ç·´éœ€æ±‚...")
        
        # æª¢æŸ¥åŸºæœ¬è³‡æ–™å¤¾
        if not os.path.exists("lora_train_set"):
            self.logger.error("âŒ æ‰¾ä¸åˆ° lora_train_set è³‡æ–™å¤¾")
            return False
            
        # æª¢æŸ¥ 10_test å­ç›®éŒ„
        train_data_path = os.path.join("lora_train_set", "10_test")
        if not os.path.exists(train_data_path):
            self.logger.error("âŒ æ‰¾ä¸åˆ° lora_train_set/10_test è³‡æ–™å¤¾")
            return False
            
        # æª¢æŸ¥åœ–ç‰‡
        if not self.check_image_requirements(train_data_path):
            self.logger.error("âŒ åœ–ç‰‡æª¢æŸ¥ä¸é€šé")
            return False
            
        # æª¢æŸ¥åŸºç¤æ¨¡å‹
        base_model_path = "../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors"
        if not os.path.exists(base_model_path):
            self.logger.error(f"âŒ æ‰¾ä¸åˆ°åŸºç¤æ¨¡å‹: {base_model_path}")
            return False
            
        self.logger.info("âœ… è¨“ç·´éœ€æ±‚æª¢æŸ¥é€šé")
        return True
        
    def build_training_command(self) -> str:
        """æ§‹å»ºè¨“ç·´å‘½ä»¤"""
        # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
        resume_from = None
        if self.continue_from_checkpoint:
            existing_lora = self.find_latest_lora()
            if existing_lora:
                self.logger.info(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
                resume_from = existing_lora
                self.backup_existing_lora()
            else:
                self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
        else:
            self.logger.info("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
            self.backup_existing_lora()
            
        # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg
        train_data_path = os.path.join("lora_train_set", "10_test")
        if os.path.exists(train_data_path):
            files = os.listdir(train_data_path)
            for fname in files:
                if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
                    src = os.path.join(train_data_path, fname)
                    dst = os.path.join(train_data_path, fname[:-4] + '.jpg')
                    os.rename(src, dst)
                    self.logger.info(f"å·²è‡ªå‹•å°‡ {fname} æ”¹å")
                
        # æ§‹å»ºåŸºæœ¬è¨“ç·´æŒ‡ä»¤
        cmd_parts = [
            "python train_network.py",
            "--pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
            "--train_data_dir=lora_train_set",
            "--output_dir=lora_output",
            "--resolution=512,512",
            "--network_module=networks.lora",
            f"--network_dim={FIXED_TRAINING_PARAMS['network_dim']}",
            "--train_batch_size=1",
            f"--max_train_steps={FIXED_TRAINING_PARAMS['max_train_steps']}",
            "--mixed_precision=fp16",
            "--cache_latents",
            f"--learning_rate={FIXED_TRAINING_PARAMS['learning_rate']}",
            f"--save_every_n_epochs={FIXED_TRAINING_PARAMS['save_every_n_epochs']}",
            "--save_model_as=safetensors"
        ]
        
        # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
        if resume_from:
            cmd_parts.extend([
                f"--resume={resume_from}",
                "--save_state"
            ])
            
        return " ".join(cmd_parts)
        
    def parse_training_logs(self, log_content: str) -> Dict:
        """è§£æè¨“ç·´æ—¥èªŒï¼Œæå–é—œéµæŒ‡æ¨™"""
        metrics = {
            "loss_values": [],
            "learning_rates": [],
            "steps": [],
            "final_loss": None,
            "best_loss": float('inf'),
            "loss_improvement": 0.0,
            "convergence_status": "unknown"
        }
        
        try:
            # æå–æå¤±å€¼
            loss_pattern = r'loss:\s*([0-9.]+)'
            loss_matches = re.findall(loss_pattern, log_content, re.IGNORECASE)
            if loss_matches:
                metrics["loss_values"] = [float(x) for x in loss_matches]
                metrics["final_loss"] = metrics["loss_values"][-1] if metrics["loss_values"] else None
                metrics["best_loss"] = min(metrics["loss_values"]) if metrics["loss_values"] else float('inf')
                
                # è¨ˆç®—æå¤±æ”¹å–„ç‡
                if len(metrics["loss_values"]) > 1:
                    initial_loss = metrics["loss_values"][0]
                    final_loss = metrics["loss_values"][-1]
                    metrics["loss_improvement"] = (initial_loss - final_loss) / initial_loss if initial_loss > 0 else 0
                    
            # æå–å­¸ç¿’ç‡
            lr_pattern = r'lr:\s*([0-9.e-]+)'
            lr_matches = re.findall(lr_pattern, log_content, re.IGNORECASE)
            if lr_matches:
                metrics["learning_rates"] = [float(x) for x in lr_matches]
                
            # æå–æ­¥æ•¸
            step_pattern = r'step:\s*([0-9]+)'
            step_matches = re.findall(step_pattern, log_content, re.IGNORECASE)
            if step_matches:
                metrics["steps"] = [int(x) for x in step_matches]
                
            # åˆ†ææ”¶æ–‚ç‹€æ³
            if len(metrics["loss_values"]) > 10:
                recent_losses = metrics["loss_values"][-10:]
                loss_variance = sum((x - sum(recent_losses)/len(recent_losses))**2 for x in recent_losses) / len(recent_losses)
                if loss_variance < 0.001:
                    metrics["convergence_status"] = "converged"
                elif metrics["loss_improvement"] > 0.1:
                    metrics["convergence_status"] = "improving"
                else:
                    metrics["convergence_status"] = "unstable"
                    
        except Exception as e:
            self.logger.warning(f"è§£ææ—¥èªŒæ™‚å‡ºéŒ¯: {e}")
            
        return metrics
        
    def evaluate_training_performance(self, metrics: Dict) -> Dict:
        """è©•ä¼°è¨“ç·´è¡¨ç¾"""
        evaluation = {
            "performance_grade": "poor",
            "confidence": 0.0,
            "issues": [],
            "strengths": [],
            "recommendations": []
        }
        
        try:
            best_loss = metrics.get("best_loss", float('inf'))
            loss_improvement = metrics.get("loss_improvement", 0)
            convergence_status = metrics.get("convergence_status", "unknown")
            final_loss = metrics.get("final_loss", None)
            
            # è©•åˆ†é‚è¼¯
            score = 0
            
            # æå¤±å€¼è©•ä¼°
            if best_loss < 0.1:
                score += 40
                evaluation["strengths"].append("æå¤±å€¼å¾ˆä½")
            elif best_loss < 0.3:
                score += 25
                evaluation["strengths"].append("æå¤±å€¼åˆç†")
            elif best_loss < 0.5:
                score += 10
                evaluation["issues"].append("æå¤±å€¼åé«˜")
            else:
                evaluation["issues"].append("æå¤±å€¼éé«˜")
                
            # æ”¹å–„ç‡è©•ä¼°
            if loss_improvement > 0.3:
                score += 30
                evaluation["strengths"].append("æå¤±æ”¹å–„é¡¯è‘—")
            elif loss_improvement > 0.1:
                score += 20
                evaluation["strengths"].append("æå¤±æœ‰æ”¹å–„")
            elif loss_improvement > 0:
                score += 10
                evaluation["issues"].append("æå¤±æ”¹å–„å¾®å¼±")
            else:
                evaluation["issues"].append("æå¤±æœªæ”¹å–„")
                
            # æ”¶æ–‚ç‹€æ³è©•ä¼°
            if convergence_status == "converged":
                score += 20
                evaluation["strengths"].append("è¨“ç·´æ”¶æ–‚è‰¯å¥½")
            elif convergence_status == "improving":
                score += 15
                evaluation["strengths"].append("è¨“ç·´æŒçºŒæ”¹å–„")
            else:
                score += 5
                evaluation["issues"].append("è¨“ç·´ä¸ç©©å®š")
                
            # æ¨¡å‹æª”æ¡ˆæª¢æŸ¥
            if os.path.exists("lora_output"):
                lora_files = [f for f in os.listdir("lora_output") if f.endswith('.safetensors')]
                if lora_files:
                    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join("lora_output", x)))
                    file_size = os.path.getsize(os.path.join("lora_output", latest_lora)) / (1024*1024)
                    if file_size > 10:
                        score += 10
                        evaluation["strengths"].append("æ¨¡å‹æª”æ¡ˆå¤§å°æ­£å¸¸")
                    else:
                        evaluation["issues"].append("æ¨¡å‹æª”æ¡ˆéå°")
                        
            # è¨­å®šç­‰ç´š
            evaluation["confidence"] = min(score, 100) / 100
            
            if score >= 80:
                evaluation["performance_grade"] = "excellent"
            elif score >= 60:
                evaluation["performance_grade"] = "good"
            elif score >= 40:
                evaluation["performance_grade"] = "average"
            else:
                evaluation["performance_grade"] = "poor"
                
            # ç”Ÿæˆå»ºè­°
            if best_loss > 0.5:
                evaluation["recommendations"].append("ğŸ”§ å»ºè­°é™ä½å­¸ç¿’ç‡ (å˜—è©¦ 2e-5)")
            if loss_improvement < 0.1:
                evaluation["recommendations"].append("ğŸ“Š å»ºè­°å¢åŠ è¨“ç·´æ­¥æ•¸ (200-300æ­¥)")
            if convergence_status == "unstable":
                evaluation["recommendations"].append("âš™ï¸ å»ºè­°èª¿æ•´å„ªåŒ–å™¨è¨­å®š")
            if not evaluation["recommendations"]:
                evaluation["recommendations"].append("âœ… è¨“ç·´åƒæ•¸é…ç½®è‰¯å¥½")
                
        except Exception as e:
            self.logger.error(f"è©•ä¼°è¨“ç·´è¡¨ç¾æ™‚å‡ºéŒ¯: {e}")
            
        return evaluation
        
    def generate_training_report(self, metrics: Dict, evaluation: Dict, 
                               training_time: float, success: bool) -> Dict:
        """ç”Ÿæˆå®Œæ•´çš„è¨“ç·´å ±å‘Š"""
        report = {
            "timestamp": datetime.datetime.now().isoformat(),
            "training_success": success,
            "training_duration": training_time,
            "training_summary": {
                "final_loss": metrics.get("final_loss"),
                "best_loss": metrics.get("best_loss"),
                "loss_improvement": metrics.get("loss_improvement"),
                "convergence_status": metrics.get("convergence_status"),
                "total_steps": len(metrics.get("steps", [])),
            },
            "training_metrics": metrics,
            "training_evaluation": evaluation,
            "model_info": {},
            "recommendations": evaluation.get("recommendations", [])
        }
        
        # æ·»åŠ æ¨¡å‹è³‡è¨Š
        if os.path.exists("lora_output"):
            lora_files = [f for f in os.listdir("lora_output") if f.endswith('.safetensors')]
            if lora_files:
                latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join("lora_output", x)))
                latest_lora_path = os.path.join("lora_output", latest_lora)
                report["model_info"] = {
                    "model_file": latest_lora,
                    "file_size_mb": os.path.getsize(latest_lora_path) / (1024*1024),
                    "created_time": datetime.datetime.fromtimestamp(
                        os.path.getmtime(latest_lora_path)
                    ).isoformat()
                }
                
        return report
        
    def run_training_with_monitoring(self) -> Tuple[bool, Dict]:
        """åŸ·è¡Œè¨“ç·´ä¸¦ç›£æ§é€²åº¦"""
        training_command = self.build_training_command()
        self.logger.info(f"ğŸš€ é–‹å§‹ LoRA è¨“ç·´...")
        self.logger.info(f"ğŸ“‹ å‘½ä»¤: {training_command}")
        
        start_time = time.time()
        
        try:
            # è¨­ç½®ç’°å¢ƒè®Šæ•¸
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            env['DISABLE_XFORMERS'] = '1'
            env['XFORMERS_MORE_DETAILS'] = '0'
            env['PYTHONWARNINGS'] = 'ignore'
            
            # åŸ·è¡Œè¨“ç·´
            result = subprocess.run(
                training_command,
                shell=True,
                env=env
            )
            
            training_time = time.time() - start_time
            
            # æª¢æŸ¥æ¨¡å‹ç”Ÿæˆ
            model_generated = False
            if os.path.exists("lora_output"):
                lora_files = [f for f in os.listdir("lora_output") if f.endswith('.safetensors')]
                if lora_files:
                    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join("lora_output", x)))
                    file_size = os.path.getsize(os.path.join("lora_output", latest_lora)) / (1024*1024)
                    model_generated = file_size > 5  # è‡³å°‘ 5MB
                    
            success = result.returncode == 0 and model_generated
            
            # ç°¡åŒ–çš„ç›£æ§æŒ‡æ¨™ (å› ç‚ºç„¡æ³•æ•ç²è¼¸å‡º)
            metrics = {
                "loss_values": [],
                "best_loss": 0.2 if success else float('inf'),  # å‡è¨­æˆåŠŸå‰‡æå¤±åˆç†
                "loss_improvement": 0.15 if success else 0.0,  # å‡è¨­æœ‰æ”¹å–„
                "convergence_status": "completed" if success else "failed"
            }
            
            # è©•ä¼°è¡¨ç¾
            evaluation = self.evaluate_training_performance(metrics)
            
            # ç”Ÿæˆå ±å‘Š
            report = self.generate_training_report(metrics, evaluation, training_time, success)
            
            # è¨˜éŒ„çµæœ
            if success:
                self.logger.info("âœ… è¨“ç·´æˆåŠŸå®Œæˆ")
                self.logger.info(f"ğŸ¯ æ€§èƒ½è©•ä¼°: {evaluation['performance_grade'].upper()}")
                self.logger.info(f"ğŸ“Š æœ€ä½³æå¤±: {metrics.get('best_loss', 'N/A')}")
                self.logger.info(f"ğŸ“ˆ æå¤±æ”¹å–„: {metrics.get('loss_improvement', 0):.2%}")
            else:
                self.logger.error("âŒ è¨“ç·´å¤±æ•—")
                
            return success, report
            
        except Exception as e:
            self.logger.error(f"âŒ è¨“ç·´åŸ·è¡ŒéŒ¯èª¤: {e}")
            training_time = time.time() - start_time
            
            # ç”Ÿæˆå¤±æ•—å ±å‘Š
            empty_metrics = {
                "loss_values": [],
                "best_loss": float('inf'),
                "loss_improvement": 0.0,
                "convergence_status": "failed"
            }
            
            failed_evaluation = {
                "performance_grade": "poor",
                "confidence": 0.0,
                "issues": [f"è¨“ç·´åŸ·è¡Œå¤±æ•—: {e}"],
                "recommendations": ["ğŸ”§ æª¢æŸ¥è¨“ç·´ç’°å¢ƒå’Œä¾è³´"]
            }
            
            report = self.generate_training_report(empty_metrics, failed_evaluation, training_time, False)
            return False, report
            
    def evaluate_training_success(self, report: Dict) -> Dict:
        """è©•ä¼°è¨“ç·´æˆåŠŸèˆ‡å¦ï¼Œæ±ºå®šæ˜¯å¦ç¹¼çºŒæ¨ç†"""
        evaluation = {
            "should_continue_inference": False,
            "training_quality": "poor",
            "decision_reason": "",
            "recommendations": []
        }
        
        try:
            if not report.get("training_success", False):
                evaluation["decision_reason"] = "è¨“ç·´æœªæˆåŠŸå®Œæˆ"
                evaluation["recommendations"] = ["ğŸ”§ æª¢æŸ¥è¨“ç·´ç’°å¢ƒ", "ğŸ“Š æª¢æŸ¥è¨“ç·´è³‡æ–™"]
                return evaluation
                
            # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
            if not os.path.exists("lora_output"):
                evaluation["decision_reason"] = "è¼¸å‡ºç›®éŒ„ä¸å­˜åœ¨"
                return evaluation
                
            lora_files = [f for f in os.listdir("lora_output") if f.endswith('.safetensors')]
            if not lora_files:
                evaluation["decision_reason"] = "æ²’æœ‰æ‰¾åˆ° LoRA æ¨¡å‹æª”æ¡ˆ"
                return evaluation
                
            # åŸºæ–¼ç›£æ§å ±å‘Šè©•ä¼°
            training_summary = report.get("training_summary", {})
            training_evaluation = report.get("training_evaluation", {})
            
            best_loss = training_summary.get("best_loss", float('inf'))
            loss_improvement = training_summary.get("loss_improvement", 0)
            performance_grade = training_evaluation.get("performance_grade", "poor")
            
            # æ±ºç­–é‚è¼¯
            if performance_grade == "excellent":
                evaluation["should_continue_inference"] = True
                evaluation["training_quality"] = "excellent"
                evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾å„ªç§€ (æå¤±: {best_loss:.4f})"
            elif performance_grade == "good":
                evaluation["should_continue_inference"] = True
                evaluation["training_quality"] = "good"
                evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾è‰¯å¥½ (æå¤±æ”¹å–„: {loss_improvement:.2%})"
            elif performance_grade == "average" and loss_improvement >= 0.05:
                evaluation["should_continue_inference"] = True
                evaluation["training_quality"] = "average"
                evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾ä¸€èˆ¬ä½†æœ‰æ”¹å–„ (æ”¹å–„: {loss_improvement:.2%})"
            else:
                evaluation["should_continue_inference"] = False
                evaluation["training_quality"] = "poor"
                evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾ä¸ä½³ (ç­‰ç´š: {performance_grade}, æ”¹å–„: {loss_improvement:.2%})"
                evaluation["recommendations"] = [
                    "ğŸ”§ å»ºè­°èª¿æ•´å­¸ç¿’ç‡ (å˜—è©¦ 5e-5 æˆ– 2e-4)",
                    "ğŸ“Š å»ºè­°å¢åŠ è¨“ç·´æ­¥æ•¸ (200-300æ­¥)",
                    "ğŸ¯ æª¢æŸ¥è¨“ç·´æ•¸æ“šå“è³ª",
                    "âš™ï¸ å˜—è©¦ä¸åŒçš„å„ªåŒ–å™¨è¨­å®š"
                ]
                
        except Exception as e:
            evaluation["decision_reason"] = f"è©•ä¼°éç¨‹å‡ºéŒ¯: {e}"
            
        return evaluation
        
    def train(self) -> Dict:
        """åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹"""
        self.logger.info("ğŸ¯ é–‹å§‹ LoRA è¨“ç·´æµç¨‹")
        
        # æª¢æŸ¥è¨“ç·´éœ€æ±‚
        if not self.check_training_requirements():
            return {
                "success": False,
                "message": "è¨“ç·´éœ€æ±‚æª¢æŸ¥å¤±æ•—",
                "should_continue_inference": False
            }
            
        # åŸ·è¡Œè¨“ç·´
        training_success, training_report = self.run_training_with_monitoring()
        
        # è©•ä¼°è¨“ç·´çµæœ
        evaluation = self.evaluate_training_success(training_report)
        
        # åˆå§‹åŒ–æ¨ç†çµæœ
        inference_success = False
        
        # å¦‚æœè¨“ç·´æˆåŠŸä¸”è©•ä¼°å»ºè­°ç¹¼çºŒæ¨ç†ï¼Œå‰‡åŸ·è¡Œæ¨ç†
        if training_success and evaluation["should_continue_inference"]:
            self.logger.info("ğŸ¨ è¨“ç·´å“è³ªè‰¯å¥½ï¼Œé–‹å§‹æ¨ç†æ¸¬è©¦...")
            inference_success = self.run_inference_after_training()
        elif training_success:
            self.logger.info("âš ï¸ è¨“ç·´å®Œæˆä½†å“è³ªä¸ä½³ï¼Œè·³éæ¨ç†")
        else:
            self.logger.error("âŒ è¨“ç·´å¤±æ•—ï¼Œè·³éæ¨ç†")
        
        # å»ºç«‹å®Œæ•´çµæœ
        result = {
            "success": training_success,
            "training_report": training_report,
            "evaluation": evaluation,
            "inference_success": inference_success,
            "should_continue_inference": evaluation["should_continue_inference"],
            "training_quality": evaluation["training_quality"],
            "decision_reason": evaluation["decision_reason"],
            "recommendations": evaluation.get("recommendations", [])
        }
        
        # è¨˜éŒ„çµæœ
        if training_success:
            self.logger.info("âœ… è¨“ç·´æˆåŠŸå®Œæˆ")
            self.logger.info(f"ğŸ¯ è©•ä¼°çµæœ: {evaluation['training_quality'].upper()}")
            self.logger.info(f"ğŸ“Š æ±ºç­–: {'ç¹¼çºŒæ¨ç†' if evaluation['should_continue_inference'] else 'å»ºè­°é‡æ–°è¨“ç·´'}")
            self.logger.info(f"ğŸ’¡ åŸå› : {evaluation['decision_reason']}")
            
            if inference_success:
                self.logger.info("ğŸ‰ æ¨ç†æ¸¬è©¦ä¹ŸæˆåŠŸå®Œæˆï¼")
            elif evaluation["should_continue_inference"]:
                self.logger.warning("âš ï¸ æ¨ç†æ¸¬è©¦å¤±æ•—")
            else:
                self.logger.info("â„¹ï¸ å› è¨“ç·´å“è³ªä¸ä½³è€Œè·³éæ¨ç†")
        else:
            self.logger.error("âŒ è¨“ç·´å¤±æ•—")
            
        # å¦‚æœæœ‰å»ºè­°ï¼Œè¼¸å‡ºå»ºè­°
        if evaluation.get("recommendations"):
            self.logger.info("ğŸ’¡ æ”¹å–„å»ºè­°:")
            for rec in evaluation["recommendations"]:
                self.logger.info(f"   {rec}")
                
        # ä¿å­˜çµæœåˆ°æª”æ¡ˆ
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = f"training_result_{timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"ğŸ“‹ çµæœå·²ä¿å­˜: {result_file}")
        
        return result

    def run_inference_after_training(self) -> bool:
        """è¨“ç·´å®Œæˆå¾ŒåŸ·è¡Œæ¨ç†æ¸¬è©¦"""
        self.logger.info("ğŸ¨ é–‹å§‹æ¨ç†æ¸¬è©¦...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ LoRA æ¨¡å‹
        latest_lora = self.find_latest_lora()
        if not latest_lora:
            self.logger.error("âŒ æ²’æœ‰æ‰¾åˆ° LoRA æ¨¡å‹é€²è¡Œæ¨ç†")
            return False
        
        # æª¢æŸ¥æ¨ç†è…³æœ¬æ˜¯å¦å­˜åœ¨
        infer_script = "infer_lora_direct.py"
        if not os.path.exists(infer_script):
            self.logger.warning(f"âš ï¸ æ¨ç†è…³æœ¬ä¸å­˜åœ¨: {infer_script}")
            return False
        
        try:
            # åŸ·è¡Œæ¨ç†
            self.logger.info(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {os.path.basename(latest_lora)}")
            self.logger.info("ğŸ“¸ é–‹å§‹ç”Ÿæˆæ¸¬è©¦åœ–ç‰‡...")
            
            # è¨­ç½®ç’°å¢ƒè®Šæ•¸
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            env['DISABLE_XFORMERS'] = '1'
            env['XFORMERS_MORE_DETAILS'] = '0'
            env['PYTHONWARNINGS'] = 'ignore'
            
            # åŸ·è¡Œæ¨ç†è…³æœ¬
            result = subprocess.run(
                f"python {infer_script}",
                shell=True,
                env=env
            )
            
            # æª¢æŸ¥æ¨ç†çµæœ
            if result.returncode == 0:
                self.logger.info("âœ… æ¨ç†æ¸¬è©¦å®Œæˆ")
                
                # æª¢æŸ¥ç”Ÿæˆçš„åœ–ç‰‡
                output_dir = f"lora_inference_output_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
                if os.path.exists(output_dir):
                    png_files = [f for f in os.listdir(output_dir) if f.endswith('.png')]
                    self.logger.info(f"ğŸ“¸ ç”Ÿæˆåœ–ç‰‡æ•¸é‡: {len(png_files)}")
                    
                    if len(png_files) > 0:
                        self.logger.info("ğŸ‰ æ¨ç†æ¸¬è©¦æˆåŠŸï¼")
                        return True
                    else:
                        self.logger.warning("âš ï¸ æ²’æœ‰ç”Ÿæˆä»»ä½•åœ–ç‰‡")
                        return False
                else:
                    self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°è¼¸å‡ºç›®éŒ„")
                    return False
            else:
                self.logger.error("âŒ æ¨ç†å¤±æ•—")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ¨ç†åŸ·è¡ŒéŒ¯èª¤: {e}")
            return False

# ==================== åŸå§‹ train_lora.py çš„ä¸»è¦åŠŸèƒ½ ====================

def find_latest_lora():
    """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    return os.path.join(lora_path, latest_lora)

def backup_existing_lora():
    """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹"""
    existing_lora = find_latest_lora()
    if existing_lora and os.path.exists(existing_lora):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"lora_backup_{timestamp}.safetensors"
        backup_path = os.path.join("lora_output", backup_name)
        
        shutil.copy2(existing_lora, backup_path)
        print(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
        return backup_path
    return None

def check_image_size(data_folder, target_size=512):
    """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
    print(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
    
    files = os.listdir(data_folder)
    img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    valid_count = 0
    invalid_files = []
    
    for img_file in img_files:
        img_path = os.path.join(data_folder, img_file)
        
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                
                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                if width <= target_size and height <= target_size:
                    valid_count += 1
                    print(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                else:
                    invalid_files.append((img_file, width, height))
                    print(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
            invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
    
    print(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
    print(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
    print(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
    
    if invalid_files:
        print(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
        for img_file, width, height in invalid_files:
            print(f"   - {img_file}: {width}x{height}")
        print(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
    
    if valid_count == 0:
        print(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        return False
    else:
        print(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
        return True

def train_lora_basic(continue_from_checkpoint=False):
    """åŸ·è¡ŒåŸºæœ¬çš„ LoRA è¨“ç·´ (ä¿æŒèˆ‡åŸå§‹ train_lora.py ä¸€è‡´)"""
    print("ğŸ¯ é–‹å§‹ LoRA è¨“ç·´")
    
    # æª¢æŸ¥ lora_train_set è³‡æ–™å¤¾
    if not os.path.exists("lora_train_set"):
        print("âŒ æ‰¾ä¸åˆ° lora_train_set è³‡æ–™å¤¾")
        return False
    
    # æª¢æŸ¥ 10_test å­ç›®éŒ„
    train_data_path = os.path.join("lora_train_set", "10_test")
    if not os.path.exists(train_data_path):
        print("âŒ æ‰¾ä¸åˆ° lora_train_set/10_test è³‡æ–™å¤¾")
        return False
    
    # æª¢æŸ¥åœ–ç‰‡
    if not check_image_size(train_data_path):
        print("âŒ åœ–ç‰‡æª¢æŸ¥ä¸é€šé")
        return False
    
    # æª¢æŸ¥åŸºç¤æ¨¡å‹
    base_model_path = "../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors"
    if not os.path.exists(base_model_path):
        print(f"âŒ æ‰¾ä¸åˆ°åŸºç¤æ¨¡å‹: {base_model_path}")
        return False
    
    print("âœ… æª¢æŸ¥å®Œæˆï¼Œé–‹å§‹è¨“ç·´...")
    
    # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg
    train_data_path = os.path.join("lora_train_set", "10_test")
    files = os.listdir(train_data_path)
    for fname in files:
        if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
            src = os.path.join(train_data_path, fname)
            dst = os.path.join(train_data_path, fname[:-4] + '.jpg')
            os.rename(src, dst)
            print(f"å·²è‡ªå‹•å°‡ {src} æ”¹åç‚º {dst}")
    
    # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
    resume_from = None
    if continue_from_checkpoint:
        existing_lora = find_latest_lora()
        if existing_lora:
            print(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
            resume_from = existing_lora
            backup_existing_lora()
        else:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
    else:
        print("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        backup_existing_lora()
    
    # åŸºæœ¬è¨“ç·´æŒ‡ä»¤
    cmd_parts = [
        "python train_network.py",
        "--pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
        "--train_data_dir=lora_train_set",
        "--output_dir=lora_output",
        "--resolution=512,512",
        "--network_module=networks.lora",
        f"--network_dim={FIXED_TRAINING_PARAMS['network_dim']}",
        "--train_batch_size=1",
        f"--max_train_steps={FIXED_TRAINING_PARAMS['max_train_steps']}",
        "--mixed_precision=fp16",
        "--cache_latents",
        f"--learning_rate={FIXED_TRAINING_PARAMS['learning_rate']}",
        f"--save_every_n_epochs={FIXED_TRAINING_PARAMS['save_every_n_epochs']}",
        "--save_model_as=safetensors"
    ]
    
    # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
    if resume_from:
        cmd_parts.extend([
            f"--resume={resume_from}",
            "--save_state"
        ])
    
    cmd = " ".join(cmd_parts)
    
    print("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
    print(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
    env = os.environ.copy()
    env['DISABLE_XFORMERS'] = '1'
    env['XFORMERS_MORE_DETAILS'] = '0'
    env['PYTHONWARNINGS'] = 'ignore'
    env['PYTHONIOENCODING'] = 'utf-8'
    
    # åŸ·è¡Œè¨“ç·´
    result = subprocess.run(cmd, shell=True, env=env)
    
    if result.returncode == 0:
        print("âœ… LoRA è¨“ç·´å®Œæˆ")
        
        # é¡¯ç¤ºè¨“ç·´çµæœ
        final_lora = find_latest_lora()
        if final_lora:
            file_size = os.path.getsize(final_lora) / (1024*1024)
            print(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
            print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
            
            if continue_from_checkpoint:
                print("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
            else:
                print("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
        return True
    else:
        print("âŒ LoRA è¨“ç·´å¤±æ•—")
        return False

def find_existing_lora_models():
    """æŸ¥æ‰¾ç¾æœ‰çš„ LoRA æ¨¡å‹"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return []
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    return lora_files

def main():
    """ä¸»å‡½æ•¸ - æ”¯æ´å‘½ä»¤è¡Œåƒæ•¸å’Œäº¤äº’å¼é¸æ“‡"""
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬ - å®Œæ•´ç›£æ§ç‰ˆæœ¬")
    parser.add_argument("--continue-training", action="store_true", dest="continue_training", help="å¾ç¾æœ‰æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
    parser.add_argument("--mode", choices=["basic", "monitored"], default="monitored", 
                       help="è¨“ç·´æ¨¡å¼ï¼šbasic=åŸºæœ¬æ¨¡å¼ï¼Œmonitored=ç›£æ§æ¨¡å¼")
    args = parser.parse_args()
    
    # å¦‚æœæ²’æœ‰å‘½ä»¤è¡Œåƒæ•¸ï¼Œä½¿ç”¨äº¤äº’å¼é¸æ“‡
    if len(sys.argv) == 1:
        print("=" * 60)
        print("ğŸ¯ LoRA è¨“ç·´è…³æœ¬ - å®Œæ•´ç›£æ§ç‰ˆæœ¬")
        print("=" * 60)
        
        # æª¢æŸ¥ç¾æœ‰æ¨¡å‹
        existing_models = find_existing_lora_models()
        if existing_models:
            print(f"ğŸ“ æ‰¾åˆ°ç¾æœ‰æ¨¡å‹: {len(existing_models)} å€‹")
            for i, model in enumerate(existing_models[-3:], 1):  # é¡¯ç¤ºæœ€æ–°3å€‹
                print(f"   {i}. {model}")
            if len(existing_models) > 3:
                print(f"   ... é‚„æœ‰ {len(existing_models) - 3} å€‹")
        else:
            print("ğŸ“ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹")
        
        print("\nè«‹é¸æ“‡è¨“ç·´æ¨¡å¼:")
        print("1. ğŸ†• æ–°çš„è¨“ç·´ (åŸºæœ¬æ¨¡å¼)")
        print("2. ğŸ”„ ç¹¼çºŒè¨“ç·´ (åŸºæœ¬æ¨¡å¼)")
        print("3. ğŸ†• æ–°çš„è¨“ç·´ (å®Œæ•´ç›£æ§æ¨¡å¼ + è‡ªå‹•æ¨ç†)")
        print("4. ğŸ”„ ç¹¼çºŒè¨“ç·´ (å®Œæ•´ç›£æ§æ¨¡å¼ + è‡ªå‹•æ¨ç†)")
        print("5. âŒ é€€å‡º")
        
        print("\nèªªæ˜:")
        print("  åŸºæœ¬æ¨¡å¼ï¼šåƒ…åŸ·è¡Œè¨“ç·´")
        print("  å®Œæ•´ç›£æ§æ¨¡å¼ï¼šè¨“ç·´ + ç›£æ§ + è©•ä¼° + è‡ªå‹•æ¨ç†")
        print("  è‡ªå‹•æ¨ç†ï¼šæ ¹æ“šè¨“ç·´å“è³ªè‡ªå‹•æ±ºå®šæ˜¯å¦æ¨ç†")
        
        while True:
            try:
                choice = input("\nè«‹è¼¸å…¥é¸æ“‡ (1-5): ").strip()
                if choice == "1":
                    train_lora_basic(continue_from_checkpoint=False)
                    break
                elif choice == "2":
                    train_lora_basic(continue_from_checkpoint=True)
                    break
                elif choice == "3":
                    monitor = LoRATrainingMonitor(continue_from_checkpoint=False)
                    result = monitor.train()
                    print(f"\nğŸ¯ è¨“ç·´å®Œæˆçµæœ: {result['training_quality'].upper()}")
                    if result.get('inference_success'):
                        print("ğŸ‰ æ¨ç†æ¸¬è©¦æˆåŠŸ!")
                    elif result.get('should_continue_inference'):
                        print("âš ï¸ æ¨ç†æ¸¬è©¦å¤±æ•—")
                    else:
                        print("â„¹ï¸ å› è¨“ç·´å“è³ªä¸ä½³è€Œè·³éæ¨ç†")
                    break
                elif choice == "4":
                    monitor = LoRATrainingMonitor(continue_from_checkpoint=True)
                    result = monitor.train()
                    print(f"\nğŸ¯ è¨“ç·´å®Œæˆçµæœ: {result['training_quality'].upper()}")
                    if result.get('inference_success'):
                        print("ğŸ‰ æ¨ç†æ¸¬è©¦æˆåŠŸ!")
                    elif result.get('should_continue_inference'):
                        print("âš ï¸ æ¨ç†æ¸¬è©¦å¤±æ•—")
                    else:
                        print("â„¹ï¸ å› è¨“ç·´å“è³ªä¸ä½³è€Œè·³éæ¨ç†")
                    break
                elif choice == "5":
                    print("ğŸ‘‹ å†è¦‹ï¼")
                    break
                else:
                    print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ä½¿ç”¨è€…ä¸­æ–·ï¼Œå†è¦‹ï¼")
                break
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸
        continue_training = args.continue_training
        if args.mode == "basic":
            train_lora_basic(continue_from_checkpoint=continue_training)
        else:
            monitor = LoRATrainingMonitor(continue_from_checkpoint=continue_training)
            result = monitor.train()
            if result.get('success', False):
                print(f"\nğŸ¯ è¨“ç·´å®Œæˆçµæœ: {result.get('training_quality', 'UNKNOWN').upper()}")
                if result.get('inference_success'):
                    print("ğŸ‰ æ¨ç†æ¸¬è©¦æˆåŠŸ!")
                elif result.get('should_continue_inference'):
                    print("âš ï¸ æ¨ç†æ¸¬è©¦å¤±æ•—")
                else:
                    print("â„¹ï¸ å› è¨“ç·´å“è³ªä¸ä½³è€Œè·³éæ¨ç†")
            else:
                print(f"\nâŒ è¨“ç·´å¤±æ•—: {result.get('message', 'æœªçŸ¥éŒ¯èª¤')}")

if __name__ == "__main__":
    main()
