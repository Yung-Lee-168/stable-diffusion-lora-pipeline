#!/usr/bin/env python3
"""
LoRA è¨“ç·´åƒæ•¸å„ªåŒ–å™¨ - æ ¹æ“šåœ–ç‰‡æ•¸é‡è‡ªå‹•èª¿æ•´è¨“ç·´åƒæ•¸
"""

import os
import argparse
import json
from pathlib import Path
from typing import Dict, Any

class LoRATrainingOptimizer:
    def __init__(self):
        self.base_params = {
            "pretrained_model_name_or_path": "runwayml/stable-diffusion-v1-5",
            "output_dir": "lora_output",
            "train_data_dir": "lora_train_set",
            "resolution": 512,
            "train_batch_size": 1,
            "gradient_accumulation_steps": 1,
            "dataloader_num_workers": 0,
            "num_train_epochs": 1,
            "max_train_steps": 200,
            "learning_rate": 1e-4,
            "scale_lr": False,
            "lr_scheduler": "cosine",
            "lr_warmup_steps": 0,
            "snr_gamma": 5.0,
            "use_8bit_adam": True,
            "mixed_precision": "fp16",
            "save_precision": "fp16",
            "enable_xformers_memory_efficient_attention": True,
            "cache_latents": True,
            "save_model_as": "safetensors",
            "network_module": "networks.lora",
            "network_dim": 32,
            "network_alpha": 32,
            "network_train_unet_only": True,
            "network_train_text_encoder_only": False,
            "save_every_n_epochs": 1
        }
    
    def calculate_optimal_params(self, image_count: int, target_time_minutes: int = 120) -> Dict[str, Any]:
        """æ ¹æ“šåœ–ç‰‡æ•¸é‡å’Œç›®æ¨™æ™‚é–“è¨ˆç®—æœ€å„ªåƒæ•¸"""
        params = self.base_params.copy()
        
        # æ ¹æ“šåœ–ç‰‡æ•¸é‡èª¿æ•´åƒæ•¸
        if image_count <= 20:
            # å°æ•¸æ“šé›†ï¼šå¢åŠ è¨“ç·´æ­¥æ•¸
            params["max_train_steps"] = 300
            params["learning_rate"] = 1e-4
            params["network_dim"] = 32
            recommendation = "å°æ•¸æ“šé›†ï¼Œå¢åŠ è¨“ç·´æ­¥æ•¸ä»¥å……åˆ†å­¸ç¿’"
            
        elif image_count <= 50:
            # ä¸­ç­‰æ•¸æ“šé›†ï¼šæ¨™æº–åƒæ•¸
            params["max_train_steps"] = 200
            params["learning_rate"] = 1e-4
            params["network_dim"] = 32
            recommendation = "ä¸­ç­‰æ•¸æ“šé›†ï¼Œä½¿ç”¨æ¨™æº–åƒæ•¸"
            
        elif image_count <= 100:
            # å¤§æ•¸æ“šé›†ï¼šæ¸›å°‘è¨“ç·´æ­¥æ•¸
            params["max_train_steps"] = 150
            params["learning_rate"] = 5e-5
            params["network_dim"] = 32
            recommendation = "å¤§æ•¸æ“šé›†ï¼Œæ¸›å°‘è¨“ç·´æ­¥æ•¸é¿å…éæ“¬åˆ"
            
        elif image_count <= 200:
            # è¶…å¤§æ•¸æ“šé›†ï¼šé€²ä¸€æ­¥æ¸›å°‘è¨“ç·´æ­¥æ•¸
            params["max_train_steps"] = 100
            params["learning_rate"] = 5e-5
            params["network_dim"] = 32
            recommendation = "è¶…å¤§æ•¸æ“šé›†ï¼Œå¤§å¹…æ¸›å°‘è¨“ç·´æ­¥æ•¸"
            
        else:
            # æµ·é‡æ•¸æ“šé›†ï¼šå»ºè­°åˆ†æ‰¹è¨“ç·´
            params["max_train_steps"] = 50
            params["learning_rate"] = 2e-5
            params["network_dim"] = 32
            recommendation = "æµ·é‡æ•¸æ“šé›†ï¼Œå¼·çƒˆå»ºè­°åˆ†æ‰¹è¨“ç·´"
        
        # æ ¹æ“šç›®æ¨™æ™‚é–“é€²ä¸€æ­¥èª¿æ•´
        if target_time_minutes <= 60:
            # å¿«é€Ÿè¨“ç·´æ¨¡å¼
            params["max_train_steps"] = min(params["max_train_steps"], 100)
            params["network_dim"] = 16  # æ¸›å°‘åƒæ•¸é‡
            recommendation += " + å¿«é€Ÿè¨“ç·´æ¨¡å¼"
            
        elif target_time_minutes <= 120:
            # æ¨™æº–è¨“ç·´æ¨¡å¼
            pass  # ä¿æŒç•¶å‰åƒæ•¸
            
        else:
            # æ·±åº¦è¨“ç·´æ¨¡å¼
            params["max_train_steps"] = min(params["max_train_steps"] + 50, 400)
            params["network_dim"] = 64  # å¢åŠ åƒæ•¸é‡
            recommendation += " + æ·±åº¦è¨“ç·´æ¨¡å¼"
        
        # è¨ˆç®—é ä¼°è¨“ç·´æ™‚é–“
        estimated_time = self.estimate_training_time(image_count, params["max_train_steps"])
        
        return {
            "params": params,
            "image_count": image_count,
            "estimated_time_minutes": estimated_time,
            "recommendation": recommendation,
            "fits_in_timeout": estimated_time <= 240  # 4å°æ™‚é™åˆ¶
        }
    
    def estimate_training_time(self, image_count: int, max_train_steps: int) -> int:
        """ä¼°ç®—è¨“ç·´æ™‚é–“ï¼ˆåˆ†é˜ï¼‰"""
        # åŸºç¤æ™‚é–“ï¼šæ¯å€‹stepç´„éœ€è¦ 0.5-2 ç§’ï¼Œå–æ±ºæ–¼åœ–ç‰‡æ•¸é‡
        if image_count <= 20:
            seconds_per_step = 0.5
        elif image_count <= 50:
            seconds_per_step = 1.0
        elif image_count <= 100:
            seconds_per_step = 1.5
        else:
            seconds_per_step = 2.0
        
        # è¨ˆç®—ç¸½æ™‚é–“
        total_seconds = max_train_steps * seconds_per_step
        
        # åŠ ä¸Šåˆå§‹åŒ–å’Œä¿å­˜æ™‚é–“
        overhead_seconds = 60 + (image_count * 2)  # åˆå§‹åŒ– + æ¯å¼µåœ–ç‰‡2ç§’è™•ç†æ™‚é–“
        
        total_time_minutes = (total_seconds + overhead_seconds) / 60
        
        # åŠ ä¸Š50%ç·©è¡æ™‚é–“
        return int(total_time_minutes * 1.5)
    
    def generate_training_config(self, image_count: int, target_time_minutes: int = 120) -> str:
        """ç”Ÿæˆè¨“ç·´é…ç½®æ–‡ä»¶"""
        config = self.calculate_optimal_params(image_count, target_time_minutes)
        
        # ç”Ÿæˆé…ç½®æ–‡ä»¶å…§å®¹
        config_content = f"""# LoRA è¨“ç·´å„ªåŒ–é…ç½®
# åœ–ç‰‡æ•¸é‡: {config['image_count']}
# é ä¼°æ™‚é–“: {config['estimated_time_minutes']} åˆ†é˜
# æ¨è–¦ç­–ç•¥: {config['recommendation']}
# é©åˆè¶…æ™‚é™åˆ¶: {'âœ… æ˜¯' if config['fits_in_timeout'] else 'âŒ å¦ï¼Œå»ºè­°åˆ†æ‰¹è¨“ç·´'}

import json

TRAINING_CONFIG = {json.dumps(config['params'], indent=4)}

def get_training_args():
    return TRAINING_CONFIG
"""
        
        return config_content
    
    def save_config(self, image_count: int, target_time_minutes: int = 120, 
                   output_file: str = "lora_training_config.py") -> str:
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_content = self.generate_training_config(image_count, target_time_minutes)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        return output_file

def main():
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´åƒæ•¸å„ªåŒ–å™¨")
    parser.add_argument("image_count", type=int, help="è¨“ç·´åœ–ç‰‡æ•¸é‡")
    parser.add_argument("--target-time", type=int, default=120, help="ç›®æ¨™è¨“ç·´æ™‚é–“ï¼ˆåˆ†é˜ï¼‰")
    parser.add_argument("--output", default="lora_training_config.py", help="è¼¸å‡ºé…ç½®æ–‡ä»¶å")
    parser.add_argument("--analyze-only", action="store_true", help="åªåˆ†æä¸ç”Ÿæˆé…ç½®æ–‡ä»¶")
    
    args = parser.parse_args()
    
    optimizer = LoRATrainingOptimizer()
    config = optimizer.calculate_optimal_params(args.image_count, args.target_time)
    
    print("ğŸ”§ LoRA è¨“ç·´åƒæ•¸å„ªåŒ–åˆ†æ")
    print("=" * 50)
    print(f"ğŸ“Š åœ–ç‰‡æ•¸é‡: {config['image_count']}")
    print(f"â±ï¸  ç›®æ¨™æ™‚é–“: {args.target_time} åˆ†é˜")
    print(f"ğŸ“ˆ é ä¼°æ™‚é–“: {config['estimated_time_minutes']} åˆ†é˜")
    print(f"ğŸ’¡ æ¨è–¦ç­–ç•¥: {config['recommendation']}")
    print(f"âœ… é©åˆè¶…æ™‚: {'æ˜¯' if config['fits_in_timeout'] else 'å¦'}")
    
    print("\nğŸ¯ å„ªåŒ–å¾Œçš„é—œéµåƒæ•¸:")
    key_params = ['max_train_steps', 'learning_rate', 'network_dim', 'network_alpha']
    for param in key_params:
        if param in config['params']:
            print(f"  {param}: {config['params'][param]}")
    
    if not config['fits_in_timeout']:
        print("\nâš ï¸  è­¦å‘Šï¼šé ä¼°æ™‚é–“è¶…é4å°æ™‚é™åˆ¶")
        print("ğŸ“‹ å»ºè­°æ–¹æ¡ˆï¼š")
        print("  1. æ¸›å°‘åœ–ç‰‡æ•¸é‡")
        print("  2. ä½¿ç”¨åˆ†æ‰¹è¨“ç·´")
        print("  3. é€²ä¸€æ­¥æ¸›å°‘ max_train_steps")
    
    if not args.analyze_only:
        config_file = optimizer.save_config(args.image_count, args.target_time, args.output)
        print(f"\nğŸ“„ é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_file}")
    
    return 0

if __name__ == "__main__":
    exit(main())
