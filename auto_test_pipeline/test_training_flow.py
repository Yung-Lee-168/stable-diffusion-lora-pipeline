#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¬è©¦è¨“ç·´æµç¨‹ - é©—è­‰ä¸åŒæƒ…æ³ä¸‹çš„ç¹¼çºŒè¨“ç·´è¡Œç‚º
"""
import os
import sys
import shutil
import subprocess
import time
from pathlib import Path

# æ·»åŠ è·¯å¾‘
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

def test_training_scenarios():
    """æ¸¬è©¦ä¸åŒçš„è¨“ç·´æƒ…æ³"""
    
    print("ðŸ§ª é–‹å§‹æ¸¬è©¦è¨“ç·´æµç¨‹...")
    
    # æ¸¬è©¦ç›®éŒ„
    lora_output = current_dir / "lora_output"
    test_data = current_dir / "lora_train_set" / "10_test"
    
    # æª¢æŸ¥æ¸¬è©¦è³‡æ–™æ˜¯å¦å­˜åœ¨
    if not test_data.exists():
        print(f"âŒ æ¸¬è©¦è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {test_data}")
        return False
    
    # æ¸…ç†èˆŠçš„è¼¸å‡º
    if lora_output.exists():
        print(f"ðŸ§¹ æ¸…ç†èˆŠçš„è¼¸å‡ºç›®éŒ„: {lora_output}")
        shutil.rmtree(lora_output)
    
    # æ¸¬è©¦ 1: å…¨æ–°è¨“ç·´ (1 epoch)
    print("\nðŸ“ æ¸¬è©¦ 1: å…¨æ–°è¨“ç·´")
    cmd = [
        sys.executable, "train_lora.py",
        "--epochs", "1",
        "--no-continue"
    ]
    
    print(f"åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=current_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ å…¨æ–°è¨“ç·´å¤±æ•—:")
        print(f"stdout: {result.stdout}")
        print(f"stderr: {result.stderr}")
        return False
    
    print("âœ… å…¨æ–°è¨“ç·´å®Œæˆ")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ LoRA æª”æ¡ˆ
    lora_files = list(lora_output.glob("*.safetensors"))
    if not lora_files:
        print("âŒ æ²’æœ‰æ‰¾åˆ° LoRA æª”æ¡ˆ")
        return False
    
    print(f"ðŸ“ æ‰¾åˆ° LoRA æª”æ¡ˆ: {[f.name for f in lora_files]}")
    
    # æ¸¬è©¦ 2: å¾žç¾æœ‰ LoRA ç¹¼çºŒè¨“ç·´ (1 epoch)
    print("\nðŸ“ æ¸¬è©¦ 2: å¾žç¾æœ‰ LoRA ç¹¼çºŒè¨“ç·´")
    cmd = [
        sys.executable, "train_lora.py",
        "--epochs", "1",
        "--continue"
    ]
    
    print(f"åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=current_dir, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ ç¹¼çºŒè¨“ç·´å¤±æ•—:")
        print(f"stdout: {result.stdout}")
        print(f"stderr: {result.stderr}")
        return False
    
    print("âœ… ç¹¼çºŒè¨“ç·´å®Œæˆ")
    
    # æª¢æŸ¥å‚™ä»½æ˜¯å¦å­˜åœ¨
    backup_files = list(lora_output.glob("backup_*.safetensors"))
    if backup_files:
        print(f"ðŸ“ æ‰¾åˆ°å‚™ä»½æª”æ¡ˆ: {[f.name for f in backup_files]}")
    
    print("\nðŸŽ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆ!")
    return True

def test_simple_training():
    """ç°¡å–®çš„è¨“ç·´æ¸¬è©¦"""
    print("ðŸ§ª é–‹å§‹ç°¡å–®è¨“ç·´æ¸¬è©¦...")
    
    # æ¸¬è©¦ç›®éŒ„
    current_dir = Path(__file__).parent
    test_data = current_dir / "lora_train_set" / "10_test"
    
    # æª¢æŸ¥æ¸¬è©¦è³‡æ–™æ˜¯å¦å­˜åœ¨
    if not test_data.exists():
        print(f"âŒ æ¸¬è©¦è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {test_data}")
        return False
    
    # åŸ·è¡Œ 1 epoch è¨“ç·´
    cmd = [
        sys.executable, "train_lora.py",
        "--epochs", "1",
        "--no-continue"
    ]
    
    print(f"åŸ·è¡Œå‘½ä»¤: {' '.join(cmd)}")
    print("è¨“ç·´é–‹å§‹...")
    
    start_time = time.time()
    result = subprocess.run(cmd, cwd=current_dir)
    end_time = time.time()
    
    print(f"è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {end_time - start_time:.2f} ç§’")
    
    if result.returncode == 0:
        print("âœ… è¨“ç·´æˆåŠŸ!")
        return True
    else:
        print("âŒ è¨“ç·´å¤±æ•—!")
        return False

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "simple":
        test_simple_training()
    else:
        test_training_scenarios()
