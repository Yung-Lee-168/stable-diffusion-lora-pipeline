import os
import sys
import subprocess
import json
import warnings
from datetime import datetime

# æ¸›å°‘è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")

# ç¢ºä¿åœ¨è…³æœ¬æ‰€åœ¨ç›®éŒ„åŸ·è¡Œ
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
print(f"ğŸ“ åˆ‡æ›åˆ°è…³æœ¬ç›®éŒ„: {script_dir}")

def find_latest_lora():
    """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        print("Cannot find LoRA output folder")
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        print("No LoRA model files found")
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    lora_full_path = os.path.join(lora_path, latest_lora)
    
    print(f"Found LoRA model: {latest_lora}")
    print(f"File size: {os.path.getsize(lora_full_path) / (1024*1024):.2f} MB")
    
    return lora_full_path

def generate_test_images():
    """ä½¿ç”¨ LoRA ç”¢ç”Ÿæ¸¬è©¦åœ–ç‰‡"""
    
    print("Starting test image generation...")
    
    # æª¢æŸ¥ LoRA æ¨¡å‹
    lora_path = find_latest_lora()
    if not lora_path:
        return False
    
    # è®€å–åŸå§‹è¨“ç·´è³‡æ–™çš„æç¤ºè©
    train_data_dir = "lora_train_set/10_test"
    
    if not os.path.exists(train_data_dir):
        print(f"Cannot find training data directory: {train_data_dir}")
        return False
    
    # ç²å–æ‰€æœ‰æ–‡å­—æª”æ¡ˆ
    txt_files = [f for f in os.listdir(train_data_dir) if f.endswith('.txt')]
    txt_files.sort()  # ç¢ºä¿é †åºä¸€è‡´
    
    test_prompts = []
    for txt_file in txt_files:
        txt_path = os.path.join(train_data_dir, txt_file)
        with open(txt_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            # åŠ ä¸Š LoRA è§¸ç™¼è©
            prompt = f"test, {content}"
            test_prompts.append(prompt)
    
    print(f"Found {len(test_prompts)} training prompts to test")
    
    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    output_dir = "test_images"
    os.makedirs(output_dir, exist_ok=True)
    
    # å»ºç«‹æ¸¬è©¦è…³æœ¬ - æ­£ç¢ºè¼‰å…¥ LoRA æ¬Šé‡
    test_script = """# -*- coding: utf-8 -*-
import torch
from diffusers import StableDiffusionPipeline
from diffusers.loaders import LoraLoaderMixin
import os
import sys
import warnings
warnings.filterwarnings("ignore")

# è¨­å®šç·¨ç¢¼ç’°å¢ƒ
os.environ['PYTHONIOENCODING'] = 'utf-8'
os.environ['PYTHONUTF8'] = '1'

# ç¦ç”¨ xFormers ç›¸é—œè­¦å‘Š
os.environ['DISABLE_XFORMERS'] = '1'
os.environ['XFORMERS_MORE_DETAILS'] = '0'

def generate_with_lora(prompt, lora_path, output_path):
    try:
        print("Loading model...")
        
        # è¨­å®šè¨­å‚™
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Using device: {device}")
        
        # ä½¿ç”¨ç·šä¸Šæ¨¡å‹
        model_path = "runwayml/stable-diffusion-v1-5"
        print(f"Loading base model: {model_path}")
        
        # è¼‰å…¥åŸºç¤ç®¡ç·š
        pipe = StableDiffusionPipeline.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False,
            use_safetensors=True,
            variant="fp16" if device == "cuda" else None
        )
        
        pipe = pipe.to(device)
        
        # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
        try:
            pipe.enable_attention_slicing()
            if device == "cuda":
                pipe.enable_model_cpu_offload()
        except Exception as e:
            print(f"Memory optimization warning: {e}")
            pass
        
        # è¼‰å…¥ LoRA æ¬Šé‡
        print(f"Loading LoRA weights: {os.path.basename(lora_path)}")
        try:
            # ä½¿ç”¨ diffusers çš„ LoRA è¼‰å…¥åŠŸèƒ½
            pipe.load_lora_weights(".", weight_name=os.path.basename(lora_path))
            print("LoRA weights loaded successfully")
        except Exception as e:
            print(f"Failed to load LoRA weights: {e}")
            print("Continuing with base model only...")
        
        print("Generating image...")
        
        # ç”Ÿæˆåœ–ç‰‡ - ä½¿ç”¨ LoRA è§¸ç™¼è©
        with torch.no_grad():
            image = pipe(
                prompt=prompt,
                num_inference_steps=30,  # ä½¿ç”¨æ›´å¤šæ­¥æ•¸å±•ç¤º LoRA æ•ˆæœ
                guidance_scale=7.5,
                width=512,
                height=512,
                generator=torch.Generator(device=device).manual_seed(42)
            ).images[0]
        
        # å„²å­˜åœ–ç‰‡
        image.save(output_path)
        print(f"Image saved to: {output_path}")
        return True
        
    except Exception as e:
        print(f"Generation failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    if len(sys.argv) != 4:
        print("Usage: python script.py <prompt> <lora_path> <output_path>")
        sys.exit(1)
        
    prompt = sys.argv[1]
    lora_path = sys.argv[2]
    output_path = sys.argv[3]
    
    success = generate_with_lora(prompt, lora_path, output_path)
    sys.exit(0 if success else 1)
"""
    
    # å„²å­˜æ¸¬è©¦è…³æœ¬
    script_path = "temp_generate.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(test_script)
    
    # ç”¢ç”Ÿæ¸¬è©¦åœ–ç‰‡
    successful_images = 0
    test_info = {
        "lora_model": os.path.basename(lora_path),
        "test_time": datetime.now().isoformat(),
        "prompts": [],
        "success_count": 0,
        "total_count": len(test_prompts)
    }
    
    for i, prompt in enumerate(test_prompts):
        output_path = os.path.join(output_dir, f"test_{i+1:02d}.png")
        print(f"  ç”Ÿæˆåœ–ç‰‡ {i+1}/{len(test_prompts)}: {prompt[:50]}...")
        
        # å˜—è©¦ç”Ÿæˆåœ–ç‰‡
        try:
            # è¨­å®šç’°å¢ƒè®Šæ•¸é¿å…ç·¨ç¢¼å•é¡Œå’Œ xFormers è­¦å‘Š
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            env['DISABLE_XFORMERS'] = '1'
            env['XFORMERS_MORE_DETAILS'] = '0'
            
            result = subprocess.run([
                sys.executable, script_path, prompt, lora_path, output_path
            ], capture_output=True, text=True, timeout=180, env=env, encoding='utf-8', errors='ignore')
            
            if result.returncode == 0 and os.path.exists(output_path):
                successful_images += 1
                status = "SUCCESS"
                print(f"    SUCCESS")
            else:
                status = "FAILED"
                print(f"    FAILED")
                if result.stderr:
                    print(f"    Error: {result.stderr.strip()}")
                if result.stdout:
                    print(f"    Output: {result.stdout.strip()}")
                
        except subprocess.TimeoutExpired:
            status = "TIMEOUT"
            print(f"    TIMEOUT")
        except Exception as e:
            status = f"ERROR: {str(e)}"
            print(f"    ERROR: {str(e)}")
        
        test_info["prompts"].append({
            "prompt": prompt,
            "output_file": f"test_{i+1:02d}.png",
            "status": status
        })
        
        print(f"    {status}")
    
    test_info["success_count"] = successful_images
    
    # å„²å­˜æ¸¬è©¦è³‡è¨Š
    with open(os.path.join(output_dir, "test_info.json"), 'w', encoding='utf-8') as f:
        json.dump(test_info, f, indent=2, ensure_ascii=False)
    
    # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
    if os.path.exists(script_path):
        os.remove(script_path)
    
    print(f"Test image generation completed: {successful_images}/{len(test_prompts)} successful")
    return successful_images > 0

if __name__ == "__main__":
    generate_test_images()