import subprocess
import os
import sys
import warnings
import argparse
import datetime
import logging
import json
import time
from PIL import Image

# è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
os.environ['DISABLE_XFORMERS'] = '1'
os.environ['XFORMERS_MORE_DETAILS'] = '0'
os.environ['PYTHONWARNINGS'] = 'ignore'

# æ¸›å°‘è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")
warnings.filterwarnings("ignore", message=".*torch.utils._pytree.*")
warnings.filterwarnings("ignore", message=".*diffusers.*")

# ç¢ºä¿åœ¨è…³æœ¬æ‰€åœ¨ç›®éŒ„åŸ·è¡Œ
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
print(f"ğŸ“ åˆ‡æ›åˆ°è…³æœ¬ç›®éŒ„: {script_dir}")

# å…¨å±€è®Šé‡ç”¨æ–¼ç›£æ§
MONITOR_ENABLED = False
LOGGER = None

def setup_monitor():
    """è¨­å®šç›£æ§ - å¯é¸åŠŸèƒ½"""
    global MONITOR_ENABLED, LOGGER
    
    try:
        log_dir = "training_logs"
        os.makedirs(log_dir, exist_ok=True)
        
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        log_file = os.path.join(log_dir, f"training_{timestamp}.log")
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        LOGGER = logging.getLogger(__name__)
        MONITOR_ENABLED = True
        print(f"ğŸ“‹ ç›£æ§å·²å•Ÿç”¨ï¼Œæ—¥èªŒ: {log_file}")
        
    except Exception as e:
        print(f"âš ï¸ ç›£æ§è¨­å®šå¤±æ•—ï¼Œå°‡ä½¿ç”¨åŸºæœ¬æ¨¡å¼: {e}")
        MONITOR_ENABLED = False

def log_info(message):
    """è¨˜éŒ„è¨Šæ¯"""
    if MONITOR_ENABLED and LOGGER:
        LOGGER.info(message)
    print(message)

def find_latest_lora():
    """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    return os.path.join(lora_path, latest_lora)

def backup_existing_lora():
    """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹"""
    existing_lora = find_latest_lora()
    if existing_lora and os.path.exists(existing_lora):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"lora_backup_{timestamp}.safetensors"
        backup_path = os.path.join("lora_output", backup_name)
        
        import shutil
        shutil.copy2(existing_lora, backup_path)
        log_info(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
        return backup_path
    return None

def check_image_size(data_folder, target_size=512):
    """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
    log_info(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
    
    files = os.listdir(data_folder)
    img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    valid_count = 0
    invalid_files = []
    
    for img_file in img_files:
        img_path = os.path.join(data_folder, img_file)
        
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                
                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                if width <= target_size and height <= target_size:
                    valid_count += 1
                    log_info(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                else:
                    invalid_files.append((img_file, width, height))
                    log_info(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                
        except Exception as e:
            log_info(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
            invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
    
    log_info(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
    log_info(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
    log_info(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
    
    if invalid_files:
        log_info(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
        for img_file, width, height in invalid_files:
            log_info(f"   - {img_file}: {width}x{height}")
        log_info(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
    
    if valid_count == 0:
        log_info(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        return False
    else:
        log_info(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
        return True

def train_lora(continue_from_checkpoint=False):
    """åŸ·è¡Œ LoRA è¨“ç·´ - èˆ‡åŸç‰ˆå®Œå…¨ç›¸åŒï¼Œåªæ˜¯æ·»åŠ äº†ç›£æ§"""
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    
    # ç›´æ¥æŒ‡å®šæ­£ç¢ºçš„è³‡æ–™å¤¾åç¨±æ ¼å¼
    train_dir = "lora_train_set"
    sub_folder = "10_test"
    data_folder = os.path.join(train_dir, sub_folder)

    # è‡ªå‹•è™•ç†è³‡æ–™å¤¾æ”¹å
    old_folder = os.path.join(train_dir, "test_10")
    if os.path.exists(old_folder) and not os.path.exists(data_folder):
        os.rename(old_folder, data_folder)
        log_info(f"å·²è‡ªå‹•å°‡ {old_folder} æ”¹åç‚º {data_folder}")

    # è©³ç´°æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹
    log_info(f"ğŸ” æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹...")
    if not os.path.isdir(train_dir):
        log_info(f"âŒ æ‰¾ä¸åˆ°çˆ¶è³‡æ–™å¤¾ï¼š{train_dir}")
        sys.exit(1)

    if not os.path.isdir(data_folder):
        log_info(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™å¤¾ï¼š{data_folder}")
        log_info(f"ğŸ“ {train_dir} å…§å®¹ï¼š")
        for item in os.listdir(train_dir):
            log_info(f"  {item}")
        sys.exit(1)

    # æª¢æŸ¥åœ–ç‰‡å’Œæ–‡å­—æª”æ¡ˆ
    files = os.listdir(data_folder)
    jpg_files = [f for f in files if f.lower().endswith('.jpg')]
    txt_files = [f for f in files if f.lower().endswith('.txt')]

    log_info(f"ğŸ“‚ {data_folder} å…§å®¹ï¼š")
    log_info(f"  åœ–ç‰‡æª”æ¡ˆæ•¸é‡ï¼š{len(jpg_files)}")
    log_info(f"  æ–‡å­—æª”æ¡ˆæ•¸é‡ï¼š{len(txt_files)}")

    if len(jpg_files) == 0:
        log_info("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .jpg æª”æ¡ˆï¼")
        sys.exit(1)

    # ğŸ¯ é—œéµæ­¥é©Ÿï¼šæª¢æŸ¥åœ–ç‰‡å¤§å°
    if not check_image_size(data_folder, target_size=512):
        log_info("âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        log_info("ğŸ’¡ è«‹ä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        sys.exit(1)

    # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg
    for fname in files:
        if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
            src = os.path.join(data_folder, fname)
            dst = os.path.join(data_folder, fname[:-4] + '.jpg')
            os.rename(src, dst)
            log_info(f"å·²è‡ªå‹•å°‡ {src} æ”¹åç‚º {dst}")

    # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
    resume_from = None
    if continue_from_checkpoint:
        existing_lora = find_latest_lora()
        if existing_lora:
            log_info(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
            resume_from = existing_lora
            # å‚™ä»½ç¾æœ‰æ¨¡å‹
            backup_existing_lora()
        else:
            log_info("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
    else:
        log_info("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        # å¦‚æœå­˜åœ¨èˆŠæ¨¡å‹ï¼Œå‚™ä»½å®ƒ
        backup_existing_lora()

    # åŸºæœ¬è¨“ç·´æŒ‡ä»¤
    cmd_parts = [
        "python train_network.py",
        "--pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
        "--train_data_dir=lora_train_set",
        "--output_dir=lora_output",
        "--resolution=512,512",
        "--network_module=networks.lora",
        "--network_dim=32",        # æ›´æ–°ç‚º32ç¶­
        "--train_batch_size=1",
        "--max_train_steps=100",   # é»˜èª100æ­¥ï¼Œé©åˆ100å¼µåœ–ç‰‡
        "--mixed_precision=fp16",
        "--cache_latents",
        "--learning_rate=5e-5",    # èª¿æ•´ç‚ºé©åˆå¤§æ•¸æ“šé›†çš„å­¸ç¿’ç‡
        "--save_every_n_epochs=50",
        "--save_model_as=safetensors"
    ]
    
    # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
    if resume_from:
        cmd_parts.extend([
            f"--resume={resume_from}",
            "--save_state"  # ä¿å­˜è¨“ç·´ç‹€æ…‹
        ])
    
    cmd = " ".join(cmd_parts)
    
    log_info("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
    log_info(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
    env = os.environ.copy()
    env['DISABLE_XFORMERS'] = '1'
    env['XFORMERS_MORE_DETAILS'] = '0'
    env['PYTHONWARNINGS'] = 'ignore'
    env['PYTHONIOENCODING'] = 'utf-8'
    
    # åŸ·è¡Œè¨“ç·´ - èˆ‡åŸç‰ˆå®Œå…¨ç›¸åŒ
    result = subprocess.run(cmd, shell=True, env=env)
    
    # è¨˜éŒ„çµæŸæ™‚é–“
    end_time = time.time()
    training_time = end_time - start_time
    
    if result.returncode == 0:
        log_info("âœ… LoRA è¨“ç·´å®Œæˆ")
        
        # é¡¯ç¤ºè¨“ç·´çµæœ
        final_lora = find_latest_lora()
        if final_lora:
            file_size = os.path.getsize(final_lora) / (1024*1024)
            log_info(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
            log_info(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
            log_info(f"â±ï¸ è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’")
            
            # ç°¡å–®çš„å“è³ªè©•ä¼°
            if file_size > 20:
                quality = "å„ªç§€"
            elif file_size > 15:
                quality = "è‰¯å¥½"
            elif file_size > 10:
                quality = "æ™®é€š"
            else:
                quality = "éœ€è¦æ”¹é€²"
                
            log_info(f"ğŸ¯ å“è³ªè©•ä¼°: {quality}")
            
            if continue_from_checkpoint:
                log_info("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
            else:
                log_info("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
                
            # ä¿å­˜ç›£æ§å ±å‘Š
            if MONITOR_ENABLED:
                try:
                    report = {
                        "timestamp": datetime.datetime.now().isoformat(),
                        "training_time_seconds": training_time,
                        "model_file": os.path.basename(final_lora),
                        "file_size_mb": file_size,
                        "quality": quality,
                        "continue_from_checkpoint": continue_from_checkpoint,
                        "success": True
                    }
                    
                    report_file = f"training_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    with open(report_file, 'w', encoding='utf-8') as f:
                        json.dump(report, f, indent=2, ensure_ascii=False)
                    
                    log_info(f"ğŸ“‹ ç›£æ§å ±å‘Š: {report_file}")
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜å ±å‘Šå¤±æ•—: {e}")
        
        return True
    else:
        log_info("âŒ LoRA è¨“ç·´å¤±æ•—")
        return False

def main():
    """ä¸»å‡½æ•¸ - è™•ç†å‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬ - ç›£æ§ç‰ˆ")
    parser.add_argument("--continue", "-c", action="store_true", 
                       dest="continue_training",
                       help="å¾ç¾æœ‰çš„ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´")
    parser.add_argument("--new", "-n", action="store_true",
                       dest="new_training", 
                       help="é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
    parser.add_argument("--monitor", "-m", action="store_true",
                       help="å•Ÿç”¨ç›£æ§åŠŸèƒ½")
    
    args = parser.parse_args()
    
    # è¨­å®šç›£æ§
    if args.monitor:
        setup_monitor()
    
    # æ±ºå®šè¨“ç·´æ¨¡å¼
    if args.continue_training and args.new_training:
        log_info("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        sys.exit(1)
    elif args.continue_training:
        log_info("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        continue_from_checkpoint = True
    elif args.new_training:
        log_info("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        continue_from_checkpoint = False
    else:
        # å¦‚æœæ²’æœ‰æŒ‡å®šåƒæ•¸ï¼Œè©¢å•ç”¨æˆ¶
        existing_lora = find_latest_lora()
        if existing_lora:
            log_info(f"ğŸ” ç™¼ç¾ç¾æœ‰çš„ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
            log_info("è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
            log_info("1. å¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´ (ç´¯ç©èª¿æ•™)")
            log_info("2. é–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´ (é‡æ–°é–‹å§‹)")
            
            while True:
                choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
                if choice == "1":
                    continue_from_checkpoint = True
                    break
                elif choice == "2":
                    continue_from_checkpoint = False
                    break
                else:
                    log_info("è«‹è¼¸å…¥ 1 æˆ– 2")
        else:
            log_info("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            continue_from_checkpoint = False
    
    # åŸ·è¡Œè¨“ç·´
    success = train_lora(continue_from_checkpoint)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
