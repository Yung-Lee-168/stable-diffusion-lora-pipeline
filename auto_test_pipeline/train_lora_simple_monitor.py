#!/usr/bin/env python3
"""
LoRA è¨“ç·´è…³æœ¬ - ç°¡åŒ–ç›£æ§ç‰ˆæœ¬
åŸºæ–¼ train_lora.py çš„æ ¸å¿ƒé‚è¼¯ï¼ŒåŠ å…¥ç°¡åŒ–çš„ç›£æ§åŠŸèƒ½
"""

import subprocess
import os
import sys
import warnings
import argparse
import datetime
import logging
import json
import time
from PIL import Image

# è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
os.environ['DISABLE_XFORMERS'] = '1'
os.environ['XFORMERS_MORE_DETAILS'] = '0'
os.environ['PYTHONWARNINGS'] = 'ignore'

# æ¸›å°‘è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")
warnings.filterwarnings("ignore", message=".*torch.utils._pytree.*")
warnings.filterwarnings("ignore", message=".*diffusers.*")

# ç¢ºä¿åœ¨è…³æœ¬æ‰€åœ¨ç›®éŒ„åŸ·è¡Œ
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
print(f"ğŸ“ åˆ‡æ›åˆ°è…³æœ¬ç›®éŒ„: {script_dir}")

def setup_logging():
    """è¨­å®šæ—¥èªŒç³»çµ±"""
    log_dir = "training_logs"
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f"lora_training_{timestamp}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“‹ æ—¥èªŒæª”æ¡ˆ: {log_file}")
    return logger

def find_latest_lora():
    """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    return os.path.join(lora_path, latest_lora)

def backup_existing_lora():
    """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹"""
    existing_lora = find_latest_lora()
    if existing_lora and os.path.exists(existing_lora):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"lora_backup_{timestamp}.safetensors"
        backup_path = os.path.join("lora_output", backup_name)
        
        import shutil
        shutil.copy2(existing_lora, backup_path)
        print(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
        return backup_path
    return None

def check_image_size(data_folder, target_size=512):
    """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
    print(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
    
    files = os.listdir(data_folder)
    img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    valid_count = 0
    invalid_files = []
    
    for img_file in img_files:
        img_path = os.path.join(data_folder, img_file)
        
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                
                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                if width <= target_size and height <= target_size:
                    valid_count += 1
                    print(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                else:
                    invalid_files.append((img_file, width, height))
                    print(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
            invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
    
    print(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
    print(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
    print(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
    
    if invalid_files:
        print(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
        for img_file, width, height in invalid_files:
            print(f"   - {img_file}: {width}x{height}")
        print(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
    
    if valid_count == 0:
        print(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        return False
    else:
        print(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
        return True

def train_lora_with_monitoring(continue_from_checkpoint=False):
    """åŸ·è¡Œ LoRA è¨“ç·´ - å¸¶ç°¡åŒ–ç›£æ§"""
    logger = setup_logging()
    logger.info("ğŸ¯ é–‹å§‹ LoRA è¨“ç·´æµç¨‹")
    
    start_time = time.time()
    
    # ç›´æ¥æŒ‡å®šæ­£ç¢ºçš„è³‡æ–™å¤¾åç¨±æ ¼å¼
    train_dir = "lora_train_set"
    sub_folder = "10_test"
    data_folder = os.path.join(train_dir, sub_folder)

    # è‡ªå‹•è™•ç†è³‡æ–™å¤¾æ”¹å
    old_folder = os.path.join(train_dir, "test_10")
    if os.path.exists(old_folder) and not os.path.exists(data_folder):
        os.rename(old_folder, data_folder)
        logger.info(f"å·²è‡ªå‹•å°‡ {old_folder} æ”¹åç‚º {data_folder}")

    # è©³ç´°æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹
    logger.info("ğŸ” æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹...")
    if not os.path.isdir(train_dir):
        logger.error(f"âŒ æ‰¾ä¸åˆ°çˆ¶è³‡æ–™å¤¾ï¼š{train_dir}")
        return False

    if not os.path.isdir(data_folder):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™å¤¾ï¼š{data_folder}")
        logger.info(f"ğŸ“ {train_dir} å…§å®¹ï¼š")
        for item in os.listdir(train_dir):
            logger.info(f"  {item}")
        return False

    # æª¢æŸ¥åœ–ç‰‡å’Œæ–‡å­—æª”æ¡ˆ
    files = os.listdir(data_folder)
    jpg_files = [f for f in files if f.lower().endswith('.jpg')]
    txt_files = [f for f in files if f.lower().endswith('.txt')]

    logger.info(f"ğŸ“‚ {data_folder} å…§å®¹ï¼š")
    logger.info(f"  åœ–ç‰‡æª”æ¡ˆæ•¸é‡ï¼š{len(jpg_files)}")
    logger.info(f"  æ–‡å­—æª”æ¡ˆæ•¸é‡ï¼š{len(txt_files)}")

    if len(jpg_files) == 0:
        logger.error("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .jpg æª”æ¡ˆï¼")
        return False

    # ğŸ¯ é—œéµæ­¥é©Ÿï¼šæª¢æŸ¥åœ–ç‰‡å¤§å°
    if not check_image_size(data_folder, target_size=512):
        logger.error("âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        logger.info("ğŸ’¡ è«‹ä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        return False

    # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg
    for fname in files:
        if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
            src = os.path.join(data_folder, fname)
            dst = os.path.join(data_folder, fname[:-4] + '.jpg')
            os.rename(src, dst)
            logger.info(f"å·²è‡ªå‹•å°‡ {src} æ”¹åç‚º {dst}")

    # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
    resume_from = None
    if continue_from_checkpoint:
        existing_lora = find_latest_lora()
        if existing_lora:
            logger.info(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
            resume_from = existing_lora
            # å‚™ä»½ç¾æœ‰æ¨¡å‹
            backup_existing_lora()
        else:
            logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
    else:
        logger.info("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        # å¦‚æœå­˜åœ¨èˆŠæ¨¡å‹ï¼Œå‚™ä»½å®ƒ
        backup_existing_lora()

    # åŸºæœ¬è¨“ç·´æŒ‡ä»¤
    cmd_parts = [
        "python train_network.py",
        "--pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
        "--train_data_dir=lora_train_set",
        "--output_dir=lora_output",
        "--resolution=512,512",
        "--network_module=networks.lora",
        "--network_dim=32",
        "--train_batch_size=1",
        "--max_train_steps=100",
        "--mixed_precision=fp16",
        "--cache_latents",
        "--learning_rate=5e-5",
        "--save_every_n_epochs=50",
        "--save_model_as=safetensors"
    ]
    
    # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
    if resume_from:
        cmd_parts.extend([
            f"--resume={resume_from}",
            "--save_state"
        ])
    
    cmd = " ".join(cmd_parts)
    
    logger.info("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
    logger.info(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
    env = os.environ.copy()
    env['DISABLE_XFORMERS'] = '1'
    env['XFORMERS_MORE_DETAILS'] = '0'
    env['PYTHONWARNINGS'] = 'ignore'
    env['PYTHONIOENCODING'] = 'utf-8'
    
    # åŸ·è¡Œè¨“ç·´ - ä½¿ç”¨èˆ‡ train_lora.py ç›¸åŒçš„æ–¹å¼
    result = subprocess.run(cmd, shell=True, env=env)
    
    training_time = time.time() - start_time
    
    # æª¢æŸ¥çµæœ
    success = result.returncode == 0
    
    if success:
        logger.info("âœ… LoRA è¨“ç·´å®Œæˆ")
        
        # é¡¯ç¤ºè¨“ç·´çµæœ
        final_lora = find_latest_lora()
        if final_lora:
            file_size = os.path.getsize(final_lora) / (1024*1024)
            logger.info(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
            logger.info(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
            logger.info(f"â±ï¸ è¨“ç·´æ™‚é–“: {training_time:.2f} ç§’")
            
            # ç°¡åŒ–çš„æ€§èƒ½è©•ä¼°
            if file_size > 20:
                grade = "excellent"
            elif file_size > 15:
                grade = "good"
            elif file_size > 10:
                grade = "average"
            else:
                grade = "poor"
            
            logger.info(f"ğŸ¯ è¨“ç·´å“è³ªè©•ä¼°: {grade.upper()}")
            
            if continue_from_checkpoint:
                logger.info("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
            else:
                logger.info("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
            
            # ç”Ÿæˆç°¡åŒ–å ±å‘Š
            report = {
                "timestamp": datetime.datetime.now().isoformat(),
                "success": True,
                "training_time": training_time,
                "model_file": os.path.basename(final_lora),
                "file_size_mb": file_size,
                "quality_grade": grade,
                "continue_from_checkpoint": continue_from_checkpoint
            }
            
            # ä¿å­˜å ±å‘Š
            report_file = f"training_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“‹ å ±å‘Šå·²ä¿å­˜: {report_file}")
            
            # æ ¹æ“šå“è³ªæ±ºå®šæ˜¯å¦æ¨ç†
            if grade in ["excellent", "good"]:
                logger.info("ğŸ¨ è¨“ç·´å“è³ªè‰¯å¥½ï¼Œå»ºè­°é€²è¡Œæ¨ç†æ¸¬è©¦")
                return True
            else:
                logger.info("âš ï¸ è¨“ç·´å“è³ªä¸€èˆ¬ï¼Œå»ºè­°æª¢æŸ¥åƒæ•¸")
                return True
        else:
            logger.error("âŒ æ²’æœ‰æ‰¾åˆ°è¼¸å‡ºæ¨¡å‹")
            return False
    else:
        logger.error("âŒ LoRA è¨“ç·´å¤±æ•—")
        return False

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬ - ç°¡åŒ–ç›£æ§ç‰ˆæœ¬")
    parser.add_argument("--continue", "-c", action="store_true", 
                       dest="continue_training",
                       help="å¾ç¾æœ‰çš„ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´")
    parser.add_argument("--new", "-n", action="store_true",
                       dest="new_training", 
                       help="é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
    
    args = parser.parse_args()
    
    # æ±ºå®šè¨“ç·´æ¨¡å¼
    if args.continue_training and args.new_training:
        print("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        sys.exit(1)
    elif args.continue_training:
        print("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        continue_from_checkpoint = True
    elif args.new_training:
        print("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        continue_from_checkpoint = False
    else:
        # å¦‚æœæ²’æœ‰æŒ‡å®šåƒæ•¸ï¼Œè©¢å•ç”¨æˆ¶
        existing_lora = find_latest_lora()
        if existing_lora:
            print(f"ğŸ” ç™¼ç¾ç¾æœ‰çš„ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
            print("è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
            print("1. å¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´ (ç´¯ç©èª¿æ•™)")
            print("2. é–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´ (é‡æ–°é–‹å§‹)")
            
            while True:
                choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
                if choice == "1":
                    continue_from_checkpoint = True
                    break
                elif choice == "2":
                    continue_from_checkpoint = False
                    break
                else:
                    print("è«‹è¼¸å…¥ 1 æˆ– 2")
        else:
            print("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            continue_from_checkpoint = False
    
    # åŸ·è¡Œè¨“ç·´
    success = train_lora_with_monitoring(continue_from_checkpoint)
    
    if success:
        print("ğŸ‰ è¨“ç·´æµç¨‹å®Œæˆï¼")
    else:
        print("âŒ è¨“ç·´æµç¨‹å¤±æ•—")
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
