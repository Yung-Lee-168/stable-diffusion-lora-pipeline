import subprocess
import os
import sys
import warnings
import argparse
import datetime
from PIL import Image

# è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š
os.environ['DISABLE_XFORMERS'] = '1'
os.environ['XFORMERS_MORE_DETAILS'] = '0'
os.environ['PYTHONWARNINGS'] = 'ignore'

# æ¸›å°‘è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")
warnings.filterwarnings("ignore", message=".*torch.utils._pytree.*")
warnings.filterwarnings("ignore", message=".*diffusers.*")

# ç¢ºä¿åœ¨è…³æœ¬æ‰€åœ¨ç›®éŒ„åŸ·è¡Œ
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
print(f"ğŸ“ åˆ‡æ›åˆ°è…³æœ¬ç›®éŒ„: {script_dir}")

def find_latest_lora():
    """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    return os.path.join(lora_path, latest_lora)

def backup_existing_lora():
    """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹"""
    existing_lora = find_latest_lora()
    if existing_lora and os.path.exists(existing_lora):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"lora_backup_{timestamp}.safetensors"
        backup_path = os.path.join("lora_output", backup_name)
        
        import shutil
        shutil.copy2(existing_lora, backup_path)
        print(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
        return backup_path
    return None

def check_image_size(data_folder, target_size=512):
    """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
    print(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
    
    files = os.listdir(data_folder)
    img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    valid_count = 0
    invalid_files = []
    
    for img_file in img_files:
        img_path = os.path.join(data_folder, img_file)
        
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                
                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                if width <= target_size and height <= target_size:
                    valid_count += 1
                    print(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                else:
                    invalid_files.append((img_file, width, height))
                    print(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
            invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
    
    print(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
    print(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
    print(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
    
    if invalid_files:
        print(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
        for img_file, width, height in invalid_files:
            print(f"   - {img_file}: {width}x{height}")
        print(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
    
    if valid_count == 0:
        print(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        return False
    else:
        print(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
        return True

def find_latest_state_dir():
    """æ‰¾åˆ°æœ€æ–°çš„è¨“ç·´ç‹€æ…‹ç›®éŒ„"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return None
    
    print(f"ğŸ” æª¢æŸ¥ {lora_path} ç›®éŒ„å…§å®¹...")
    
    # å°‹æ‰¾æ‰€æœ‰ç‹€æ…‹ç›®éŒ„ï¼ˆä¸æ˜¯ .safetensors æª”æ¡ˆçš„ç›®éŒ„ï¼‰
    state_dirs = []
    all_items = []
    
    for item in os.listdir(lora_path):
        item_path = os.path.join(lora_path, item)
        all_items.append(f"  {'[DIR]' if os.path.isdir(item_path) else '[FILE]'} {item}")
        
        if os.path.isdir(item_path) and not item.endswith('.safetensors'):
            state_dirs.append(item_path)
            print(f"  ğŸ“ ç™¼ç¾ç‹€æ…‹ç›®éŒ„: {item}")
    
    # é¡¯ç¤ºæ‰€æœ‰é …ç›®
    print("ğŸ“‚ lora_output ç›®éŒ„å…§å®¹:")
    for item in all_items:
        print(item)
    
    if not state_dirs:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‹€æ…‹ç›®éŒ„")
        return None
    
    # æ‰¾æœ€æ–°çš„ç›®éŒ„
    latest_state_dir = max(state_dirs, key=os.path.getmtime)
    print(f"âœ… æœ€æ–°ç‹€æ…‹ç›®éŒ„: {os.path.basename(latest_state_dir)}")
    return latest_state_dir

def cleanup_old_states(keep_recent=2):
    """æ¸…ç†èˆŠçš„ç‹€æ…‹ç›®éŒ„ï¼Œåªä¿ç•™æœ€è¿‘çš„å¹¾å€‹"""
    lora_path = "lora_output"
    if not os.path.exists(lora_path):
        return
    
    # å°‹æ‰¾æ‰€æœ‰ç‹€æ…‹ç›®éŒ„
    state_dirs = []
    for item in os.listdir(lora_path):
        item_path = os.path.join(lora_path, item)
        if os.path.isdir(item_path) and not item.endswith('.safetensors'):
            state_dirs.append(item_path)
    
    if len(state_dirs) <= keep_recent:
        return
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œåˆªé™¤èˆŠçš„
    state_dirs.sort(key=os.path.getmtime, reverse=True)
    old_dirs = state_dirs[keep_recent:]
    
    for old_dir in old_dirs:
        try:
            import shutil
            shutil.rmtree(old_dir)
            print(f"ğŸ—‘ï¸ æ¸…ç†èˆŠç‹€æ…‹ç›®éŒ„: {os.path.basename(old_dir)}")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æ¸…ç† {old_dir}: {e}")

def train_lora(continue_from_checkpoint=False):
    """åŸ·è¡Œ LoRA è¨“ç·´"""
    
    # ç›´æ¥æŒ‡å®šæ­£ç¢ºçš„è³‡æ–™å¤¾åç¨±æ ¼å¼
    train_dir = "lora_train_set"
    sub_folder = "10_test"
    data_folder = os.path.join(train_dir, sub_folder)

    # è‡ªå‹•è™•ç†è³‡æ–™å¤¾æ”¹å
    old_folder = os.path.join(train_dir, "test_10")
    if os.path.exists(old_folder) and not os.path.exists(data_folder):
        os.rename(old_folder, data_folder)
        print(f"å·²è‡ªå‹•å°‡ {old_folder} æ”¹åç‚º {data_folder}")

    # è©³ç´°æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹
    print(f"ğŸ” æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹...")
    if not os.path.isdir(train_dir):
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¶è³‡æ–™å¤¾ï¼š{train_dir}")
        sys.exit(1)

    if not os.path.isdir(data_folder):
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™å¤¾ï¼š{data_folder}")
        print(f"ğŸ“ {train_dir} å…§å®¹ï¼š")
        for item in os.listdir(train_dir):
            print(f"  {item}")
        sys.exit(1)

    # æª¢æŸ¥åœ–ç‰‡å’Œæ–‡å­—æª”æ¡ˆ
    files = os.listdir(data_folder)
    jpg_files = [f for f in files if f.lower().endswith('.jpg')]
    txt_files = [f for f in files if f.lower().endswith('.txt')]

    print(f"ğŸ“‚ {data_folder} å…§å®¹ï¼š")
    print(f"  åœ–ç‰‡æª”æ¡ˆæ•¸é‡ï¼š{len(jpg_files)}")
    print(f"  æ–‡å­—æª”æ¡ˆæ•¸é‡ï¼š{len(txt_files)}")

    if len(jpg_files) == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .jpg æª”æ¡ˆï¼")
        sys.exit(1)

    # ğŸ¯ é—œéµæ­¥é©Ÿï¼šæª¢æŸ¥åœ–ç‰‡å¤§å°
    if not check_image_size(data_folder, target_size=512):
        print("âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        print("ğŸ’¡ è«‹ä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        sys.exit(1)

    # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg
    for fname in files:
        if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
            src = os.path.join(data_folder, fname)
            dst = os.path.join(data_folder, fname[:-4] + '.jpg')
            os.rename(src, dst)
            print(f"å·²è‡ªå‹•å°‡ {src} æ”¹åç‚º {dst}")

    # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
    resume_from = None
    if continue_from_checkpoint:
        # å…ˆæŸ¥æ‰¾ç‹€æ…‹ç›®éŒ„
        state_dir = find_latest_state_dir()
        existing_lora = find_latest_lora()
        
        if state_dir:
            print(f"ğŸ”„ æ‰¾åˆ°è¨“ç·´ç‹€æ…‹ç›®éŒ„: {os.path.basename(state_dir)}")
            resume_from = state_dir
            # å‚™ä»½ç¾æœ‰æ¨¡å‹
            backup_existing_lora()
        elif existing_lora:
            print(f"âš ï¸ æ‰¾åˆ° LoRA æª”æ¡ˆä½†ç„¡ç‹€æ…‹ç›®éŒ„: {os.path.basename(existing_lora)}")
            print("ğŸ’¡ å°‡ä½¿ç”¨ç¾æœ‰ LoRA ä½œç‚ºåŸºç¤ç¹¼çºŒè¨“ç·´")
            # ä½¿ç”¨ç¾æœ‰ LoRA æª”æ¡ˆä½œç‚ºèµ·é»
            resume_from = existing_lora
            backup_existing_lora()
        else:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆæˆ–ç‹€æ…‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
    else:
        print("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        # å¦‚æœå­˜åœ¨èˆŠæ¨¡å‹ï¼Œå‚™ä»½å®ƒ
        backup_existing_lora()
        # æ¸…ç†èˆŠçš„ç‹€æ…‹ç›®éŒ„
        cleanup_old_states()

    # åŸºæœ¬è¨“ç·´æŒ‡ä»¤
    cmd_parts = [
        "python train_network.py",
        "--pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
        "--train_data_dir=lora_train_set",
        "--output_dir=lora_output",
        "--resolution=512,512",
        "--network_module=networks.lora",
        "--network_dim=32",        # æ›´æ–°ç‚º32ç¶­
        "--train_batch_size=1",
        "--max_train_steps=100",   # é»˜èª100æ­¥ï¼Œé©åˆ100å¼µåœ–ç‰‡
        "--mixed_precision=fp16",
        "--cache_latents",
        "--learning_rate=5e-5",    # èª¿æ•´ç‚ºé©åˆå¤§æ•¸æ“šé›†çš„å­¸ç¿’ç‡
        "--save_every_n_epochs=50",
        "--save_model_as=safetensors",
        "--save_state"             # ç¸½æ˜¯ä¿å­˜ç‹€æ…‹ä»¥ä¾¿å°‡ä¾†ç¹¼çºŒè¨“ç·´
    ]
    
    # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
    if resume_from:
        if resume_from.endswith('.safetensors'):
            # å¾ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´ï¼Œä½¿ç”¨ network_weights åƒæ•¸
            cmd_parts.append(f"--network_weights={resume_from}")
            print(f"ğŸ”„ å°‡å¾ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´: {os.path.basename(resume_from)}")
        else:
            # å¾ç‹€æ…‹ç›®éŒ„ç¹¼çºŒè¨“ç·´ï¼Œä½¿ç”¨ resume åƒæ•¸
            cmd_parts.append(f"--resume={resume_from}")
            print(f"ğŸ”„ å°‡å¾ç‹€æ…‹ç›®éŒ„ç¹¼çºŒè¨“ç·´: {os.path.basename(resume_from)}")
    else:
        print("ğŸ†• é–‹å§‹å…¨æ–°è¨“ç·´")
    
    cmd = " ".join(cmd_parts)
    
    print("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
    print(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š - æ›´å®Œæ•´çš„è¨­å®š
    env = os.environ.copy()
    env['DISABLE_XFORMERS'] = '1'
    env['XFORMERS_MORE_DETAILS'] = '0'
    env['PYTHONWARNINGS'] = 'ignore'
    env['PYTHONIOENCODING'] = 'utf-8'
    env['CUDA_LAUNCH_BLOCKING'] = '0'
    env['TRANSFORMERS_VERBOSITY'] = 'error'
    env['DIFFUSERS_VERBOSITY'] = 'error'
    env['TRITON_DISABLE'] = '1'
    env['NO_TRITON'] = '1'
    
    # ç›´æ¥åŸ·è¡Œå‘½ä»¤ï¼Œä¸ä½¿ç”¨éæ¿¾å™¨é¿å…ç·¨ç¢¼å•é¡Œ
    print("ğŸš€ æ­£åœ¨åŸ·è¡Œè¨“ç·´...")
    result = subprocess.run(cmd, shell=True, env=env)
    if result.returncode == 0:
        print("âœ… LoRA è¨“ç·´å®Œæˆ")
        
        # é¡¯ç¤ºè¨“ç·´çµæœ
        final_lora = find_latest_lora()
        if final_lora:
            file_size = os.path.getsize(final_lora) / (1024*1024)
            print(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
            print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
            
            if continue_from_checkpoint:
                print("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
            else:
                print("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        return True
    else:
        print("âŒ LoRA è¨“ç·´å¤±æ•—")
        return False

def main():
    """ä¸»å‡½æ•¸ - è™•ç†å‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬")
    parser.add_argument("--continue", "-c", action="store_true", 
                       dest="continue_training",
                       help="å¾ç¾æœ‰çš„ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´")
    parser.add_argument("--new", "-n", action="store_true",
                       dest="new_training", 
                       help="é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
    
    args = parser.parse_args()
    
    # æ±ºå®šè¨“ç·´æ¨¡å¼
    if args.continue_training and args.new_training:
        print("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        sys.exit(1)
    elif args.continue_training:
        print("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        continue_from_checkpoint = True
    elif args.new_training:
        print("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        continue_from_checkpoint = False
    else:
        # å¦‚æœæ²’æœ‰æŒ‡å®šåƒæ•¸ï¼Œè©¢å•ç”¨æˆ¶
        existing_lora = find_latest_lora()
        if existing_lora:
            print(f"ğŸ” ç™¼ç¾ç¾æœ‰çš„ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
            print("è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
            print("1. å¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´ (ç´¯ç©èª¿æ•™)")
            print("2. é–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´ (é‡æ–°é–‹å§‹)")
            
            while True:
                choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
                if choice == "1":
                    continue_from_checkpoint = True
                    break
                elif choice == "2":
                    continue_from_checkpoint = False
                    break
                else:
                    print("è«‹è¼¸å…¥ 1 æˆ– 2")
        else:
            print("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            continue_from_checkpoint = False
    
    # åŸ·è¡Œè¨“ç·´
    success = train_lora(continue_from_checkpoint)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()