import subprocess
import os
import sys
import warnings
import argparse
import datetime
import json
from PIL import Image

# Set environment variables to suppress warnings
os.environ['DISABLE_XFORMERS'] = '1'
os.environ['XFORMERS_MORE_DETAILS'] = '0'
os.environ['PYTHONWARNINGS'] = 'ignore'

# Reduce warning messages
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")
warnings.filterwarnings("ignore", message=".*torch.utils._pytree.*")
warnings.filterwarnings("ignore", message=".*diffusers.*")

# Ensure execution in script directory
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
print(f"ğŸ“ Switched to script directory: {script_dir}")

def check_dependencies():
    """Check and report required and optional dependencies"""
    required_deps = ['torch', 'PIL', 'numpy']
    optional_deps = ['tensorboard', 'matplotlib']
    
    missing_required = []
    missing_optional = []
    
    for dep in required_deps:
        try:
            __import__(dep)
            print(f"âœ… Required dependency installed: {dep}")
        except ImportError:
            missing_required.append(dep)
            print(f"âŒ Missing required dependency: {dep}")
    
    for dep in optional_deps:
        try:
            __import__(dep)
            print(f"âœ… Optional dependency installed: {dep}")
        except ImportError:
            missing_optional.append(dep)
            print(f"âš ï¸ Missing optional dependency: {dep} (will use alternative features)")
    
    if missing_required:
        print(f"\nâŒ Cannot continue because of missing required dependencies: {missing_required}")
        print(f"ğŸ’¡ You can install them with: pip install {' '.join(missing_required)}")
        return False
    
    if missing_optional:
        print(f"\nğŸ’¡ Optional features explanation:")
        if 'tensorboard' in missing_optional:
            print(f"   - tensorboard: will use built-in loss tracking instead of TensorBoard")
        if 'matplotlib' in missing_optional:
            print(f"   - matplotlib: will skip PNG chart generation, only generate JSON reports")
        print(f"   You can install optional dependencies with: pip install {' '.join(missing_optional)}")
    
    return True

def find_latest_lora():
    """Find the latest LoRA model file"""
    # ğŸ”§ FIX: Search in lora_output directory
    lora_path = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\lora_output"
    if not os.path.exists(lora_path):
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    return os.path.join(lora_path, latest_lora)

def backup_existing_lora():
    """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹"""
    existing_lora = find_latest_lora()
    if existing_lora and os.path.exists(existing_lora):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"lora_backup_{timestamp}.safetensors"
        # ğŸ”§ FIX: å‚™ä»½ä¹Ÿæ”¾åœ¨ lora_output ç›®éŒ„
        backup_path = os.path.join(r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\lora_output", backup_name)
        
        import shutil
        shutil.copy2(existing_lora, backup_path)
        print(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
        return backup_path
    return None

def check_image_size(data_folder, target_size=512):
    """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
    print(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
    
    files = os.listdir(data_folder)
    img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    valid_count = 0
    invalid_files = []
    
    for img_file in img_files:
        img_path = os.path.join(data_folder, img_file)
        
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                
                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                if width <= target_size and height <= target_size:
                    valid_count += 1
                    print(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                else:
                    invalid_files.append((img_file, width, height))
                    print(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
            invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
    
    print(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
    print(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
    print(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
    
    if invalid_files:
        print(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
        for img_file, width, height in invalid_files:
            print(f"   - {img_file}: {width}x{height}")
        print(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
    
    if valid_count == 0:
        print(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        return False
    else:
        print(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
        return True

def find_latest_state_dir():
    """æ‰¾åˆ°æœ€æ–°çš„è¨“ç·´ç‹€æ…‹ç›®éŒ„"""
    # ğŸ”§ FIX: æ”¹ç‚ºå¾ lora_output ç›®éŒ„æŸ¥æ‰¾ç‹€æ…‹
    lora_path = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\lora_output"
    if not os.path.exists(lora_path):
        return None
    
    print(f"ğŸ” æª¢æŸ¥ {lora_path} ç›®éŒ„å…§å®¹...")
    
    # å°‹æ‰¾æ‰€æœ‰ç‹€æ…‹ç›®éŒ„ï¼ˆä¸æ˜¯ .safetensors æª”æ¡ˆçš„ç›®éŒ„ï¼‰
    state_dirs = []
    all_items = []
    
    for item in os.listdir(lora_path):
        item_path = os.path.join(lora_path, item)
        all_items.append(f"  {'[DIR]' if os.path.isdir(item_path) else '[FILE]'} {item}")
        
        if os.path.isdir(item_path) and not item.endswith('.safetensors'):
            state_dirs.append(item_path)
            print(f"  ğŸ“ ç™¼ç¾ç‹€æ…‹ç›®éŒ„: {item}")
    
    # é¡¯ç¤ºæ‰€æœ‰é …ç›®
    print("ğŸ“‚ lora_output ç›®éŒ„å…§å®¹:")
    for item in all_items:
        print(item)
    
    if not state_dirs:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç‹€æ…‹ç›®éŒ„")
        return None
    
    # æ‰¾æœ€æ–°çš„ç›®éŒ„
    latest_state_dir = max(state_dirs, key=os.path.getmtime)
    print(f"âœ… æœ€æ–°ç‹€æ…‹ç›®éŒ„: {os.path.basename(latest_state_dir)}")
    return latest_state_dir

def cleanup_old_states(keep_recent=2):
    """æ¸…ç†èˆŠçš„ç‹€æ…‹ç›®éŒ„ï¼Œåªä¿ç•™æœ€è¿‘çš„å¹¾å€‹"""
    # ğŸ”§ FIX: æ”¹ç‚ºå¾ lora_output ç›®éŒ„æ¸…ç†
    lora_path = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\lora_output"
    if not os.path.exists(lora_path):
        return
    
    # å°‹æ‰¾æ‰€æœ‰ç‹€æ…‹ç›®éŒ„
    state_dirs = []
    for item in os.listdir(lora_path):
        item_path = os.path.join(lora_path, item)
        if os.path.isdir(item_path) and not item.endswith('.safetensors'):
            state_dirs.append(item_path)
    
    if len(state_dirs) <= keep_recent:
        return
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼Œåˆªé™¤èˆŠçš„
    state_dirs.sort(key=os.path.getmtime, reverse=True)
    old_dirs = state_dirs[keep_recent:]
    
    for old_dir in old_dirs:
        try:
            import shutil
            shutil.rmtree(old_dir)
            print(f"ğŸ—‘ï¸ æ¸…ç†èˆŠç‹€æ…‹ç›®éŒ„: {os.path.basename(old_dir)}")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æ¸…ç† {old_dir}: {e}")

def train_lora(continue_from_checkpoint=False, custom_steps=None):
    """åŸ·è¡Œ LoRA è¨“ç·´"""
    
    # ç›´æ¥æŒ‡å®šæ­£ç¢ºçš„è³‡æ–™å¤¾åç¨±æ ¼å¼
    train_dir = "lora_train_set"
    sub_folder = "10_test"
    data_folder = os.path.join(train_dir, sub_folder)

    # è‡ªå‹•è™•ç†è³‡æ–™å¤¾æ”¹å
    old_folder = os.path.join(train_dir, "test_10")
    if os.path.exists(old_folder) and not os.path.exists(data_folder):
        os.rename(old_folder, data_folder)
        print(f"å·²è‡ªå‹•å°‡ {old_folder} æ”¹åç‚º {data_folder}")

    # è©³ç´°æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹
    print(f"ğŸ” æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹...")
    if not os.path.isdir(train_dir):
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¶è³‡æ–™å¤¾ï¼š{train_dir}")
        sys.exit(1)

    if not os.path.isdir(data_folder):
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™å¤¾ï¼š{data_folder}")
        print(f"ğŸ“ {train_dir} å…§å®¹ï¼š")
        for item in os.listdir(train_dir):
            print(f"  {item}")
        sys.exit(1)

    # æª¢æŸ¥åœ–ç‰‡å’Œæ–‡å­—æª”æ¡ˆ
    files = os.listdir(data_folder)
    jpg_files = [f for f in files if f.lower().endswith('.jpg')]
    txt_files = [f for f in files if f.lower().endswith('.txt')]

    print(f"ğŸ“‚ {data_folder} å…§å®¹ï¼š")
    print(f"  åœ–ç‰‡æª”æ¡ˆæ•¸é‡ï¼š{len(jpg_files)}")
    print(f"  æ–‡å­—æª”æ¡ˆæ•¸é‡ï¼š{len(txt_files)}")

    if len(jpg_files) == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .jpg æª”æ¡ˆï¼")
        sys.exit(1)

    # ğŸ¯ é—œéµæ­¥é©Ÿï¼šæª¢æŸ¥åœ–ç‰‡å¤§å°
    if not check_image_size(data_folder, target_size=512):
        print("âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        print("ğŸ’¡ è«‹ä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        sys.exit(1)

    # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg
    for fname in files:
        if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
            src = os.path.join(data_folder, fname)
            dst = os.path.join(data_folder, fname[:-4] + '.jpg')
            os.rename(src, dst)
            print(f"å·²è‡ªå‹•å°‡ {src} æ”¹åç‚º {dst}")

    # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
    resume_from = None
    current_step = 0
    
    if continue_from_checkpoint:
        # å…ˆæŸ¥æ‰¾ç‹€æ…‹ç›®éŒ„
        state_dir = find_latest_state_dir()
        existing_lora = find_latest_lora()
        
        if state_dir:
            print(f"ğŸ”„ æ‰¾åˆ°è¨“ç·´ç‹€æ…‹ç›®éŒ„: {os.path.basename(state_dir)}")
            current_step = get_current_training_step(state_dir)
            resume_from = state_dir
            # å‚™ä»½ç¾æœ‰æ¨¡å‹
            backup_existing_lora()
        elif existing_lora:
            print(f"âš ï¸ æ‰¾åˆ° LoRA æª”æ¡ˆä½†ç„¡ç‹€æ…‹ç›®éŒ„: {os.path.basename(existing_lora)}")
            print("ğŸ’¡ å°‡ä½¿ç”¨ç¾æœ‰ LoRA ä½œç‚ºåŸºç¤ç¹¼çºŒè¨“ç·´")
            # ä½¿ç”¨ç¾æœ‰ LoRA æª”æ¡ˆä½œç‚ºèµ·é»
            resume_from = existing_lora
            backup_existing_lora()
        else:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆæˆ–ç‹€æ…‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
    else:
        print("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        # å¦‚æœå­˜åœ¨èˆŠæ¨¡å‹ï¼Œå‚™ä»½å®ƒ
        backup_existing_lora()
        # æ¸…ç†èˆŠçš„ç‹€æ…‹ç›®éŒ„
        cleanup_old_states()
    
    # ğŸ¯ æ™ºèƒ½è¨ˆç®—æœ€å¤§è¨“ç·´æ­¥æ•¸
    if custom_steps is not None:
        # ä½¿ç”¨ç”¨æˆ¶æŒ‡å®šçš„æ­¥æ•¸
        additional_steps = custom_steps
        print(f"ğŸ“Š ä½¿ç”¨ç”¨æˆ¶æŒ‡å®šæ­¥æ•¸: {custom_steps}")
    else:
        # äº¤äº’å¼è©¢å•æ­¥æ•¸
        if continue_from_checkpoint:
            print(f"\nğŸ”¢ è«‹è¨­å®šè¦ç¹¼çºŒè¨“ç·´çš„æ­¥æ•¸:")
            print(f"   ç•¶å‰å·²å®Œæˆ: {current_step} æ­¥")
            default_steps = 100
        else:
            print(f"\nğŸ”¢ è«‹è¨­å®šæ–°è¨“ç·´çš„ç¸½æ­¥æ•¸:")
            default_steps = 100
        
        while True:
            try:
                user_input = input(f"è«‹è¼¸å…¥æ­¥æ•¸ (é»˜èª {default_steps}): ").strip()
                if user_input == "":
                    additional_steps = default_steps
                    break
                else:
                    additional_steps = int(user_input)
                    if additional_steps > 0:
                        break
                    else:
                        print("âŒ æ­¥æ•¸å¿…é ˆå¤§æ–¼0ï¼Œè«‹é‡æ–°è¼¸å…¥")
            except ValueError:
                print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
    
    max_train_steps = calculate_smart_max_steps(current_step, additional_steps=additional_steps)

    # åŸºæœ¬è¨“ç·´æŒ‡ä»¤ - ä½¿ç”¨ç•¶å‰Pythonè§£é‡‹å™¨
    python_executable = sys.executable  # ç²å–ç•¶å‰Pythonè§£é‡‹å™¨è·¯å¾‘
    print(f"ğŸ ä½¿ç”¨Pythonè§£é‡‹å™¨: {python_executable}")
    
    # ğŸ”§ FIX: ç¢ºä¿æ­£ç¢ºçš„è¼¸å‡ºç›®éŒ„å­˜åœ¨
    # LoRAæ¨¡å‹å’Œç‹€æ…‹è¼¸å‡ºåˆ° lora_output
    # å ±å‘Šå’Œæ—¥èªŒè¼¸å‡ºåˆ° training_logs
    lora_output_dir = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\lora_output"
    training_logs_dir = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs"
    logs_dir = os.path.join(training_logs_dir, "logs")
    
    os.makedirs(lora_output_dir, exist_ok=True)
    os.makedirs(training_logs_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    
    print(f"ğŸ“ LoRAæ¨¡å‹è¼¸å‡ºç›®éŒ„: {lora_output_dir}")
    print(f"ğŸ“ å ±å‘Šå’Œæ—¥èªŒç›®éŒ„: {training_logs_dir}")
    
    cmd_parts = [
        f'"{python_executable}" train_network.py',  # ä½¿ç”¨ç•¶å‰Pythonç’°å¢ƒ
        "--pretrained_model_name_or_path=../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
        "--train_data_dir=lora_train_set",
        f"--output_dir={lora_output_dir}",  # ğŸ”§ FIX: LoRAæ¨¡å‹è¼¸å‡ºåˆ° lora_output
        f"--logging_dir={logs_dir}",       # ğŸ”§ FIX: æ—¥èªŒè¼¸å‡ºåˆ° training_logs/logs
        "--resolution=512,512",
        "--network_module=networks.lora",
        "--network_dim=32",        # æ›´æ–°ç‚º32ç¶­
        "--train_batch_size=1",
        f"--max_train_steps={max_train_steps}",   # æ™ºèƒ½èª¿æ•´çš„æœ€å¤§è¨“ç·´æ­¥æ•¸
        "--mixed_precision=fp16",
        "--cache_latents",
        "--learning_rate=5e-5",    # èª¿æ•´ç‚ºé©åˆå¤§æ•¸æ“šé›†çš„å­¸ç¿’ç‡
        "--save_every_n_epochs=50",
        "--save_model_as=safetensors",
        "--save_state",            # ç¸½æ˜¯ä¿å­˜ç‹€æ…‹ä»¥ä¾¿å°‡ä¾†ç¹¼çºŒè¨“ç·´
        "--log_with=tensorboard",  # ğŸ“Š ä½¿ç”¨TensorBoardè¨˜éŒ„è¨“ç·´éç¨‹ï¼ˆå¯é¸ï¼‰
        "--gradient_accumulation_steps=1",  # ğŸ¯ æ˜ç¢ºè¨­å®šç´¯ç©æ­¥æ•¸ï¼Œç¢ºä¿æ­¥æ•¸æ§åˆ¶ç²¾ç¢º
        "--save_precision=fp16",   # ğŸ”§ FIX: ç¢ºä¿ä¿å­˜ç²¾åº¦ä¸€è‡´
        "--log_tracker_name=lora_training",  # ğŸ”§ FIX: è¨­å®šè¿½è¹¤å™¨åç¨±
    ]
    
    # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
    if resume_from:
        if resume_from.endswith('.safetensors'):
            # å¾ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´ï¼Œä½¿ç”¨ network_weights åƒæ•¸
            cmd_parts.append(f"--network_weights={resume_from}")
            print(f"ğŸ”„ å°‡å¾ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´: {os.path.basename(resume_from)}")
        else:
            # å¾ç‹€æ…‹ç›®éŒ„ç¹¼çºŒè¨“ç·´ï¼Œä½¿ç”¨ resume åƒæ•¸
            cmd_parts.append(f"--resume={resume_from}")
            print(f"ğŸ”„ å°‡å¾ç‹€æ…‹ç›®éŒ„ç¹¼çºŒè¨“ç·´: {os.path.basename(resume_from)}")
    else:
        print("ğŸ†• é–‹å§‹å…¨æ–°è¨“ç·´")
    
    cmd = " ".join(cmd_parts)
    
    print("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
    print(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†æŠ‘åˆ¶è­¦å‘Š - æ›´å®Œæ•´çš„è¨­å®š
    env = os.environ.copy()
    env['DISABLE_XFORMERS'] = '1'
    env['XFORMERS_MORE_DETAILS'] = '0'
    env['PYTHONWARNINGS'] = 'ignore'
    env['PYTHONIOENCODING'] = 'utf-8'
    env['CUDA_LAUNCH_BLOCKING'] = '0'
    env['TRANSFORMERS_VERBOSITY'] = 'error'
    env['DIFFUSERS_VERBOSITY'] = 'error'
    env['TRITON_DISABLE'] = '1'
    env['NO_TRITON'] = '1'
    env['PYTORCH_DISABLE_XFORMERS'] = '1'  # ğŸ”§ FIX: é¡å¤–çš„ xformers æŠ‘åˆ¶
    env['FORCE_XFORMERS'] = '0'           # ğŸ”§ FIX: å¼·åˆ¶ç¦ç”¨ xformers
    env['XFORMERS_DISABLED'] = '1'        # ğŸ”§ FIX: æ˜ç¢ºç¦ç”¨ xformers
    
    # ç›´æ¥åŸ·è¡Œå‘½ä»¤ï¼Œä½¿ç”¨å…§å»ºç›£æ§
    print("ğŸš€ æ­£åœ¨åŸ·è¡Œè¨“ç·´...")
    
    # ğŸ”§ FIX: ä½¿ç”¨å…§å»ºlossç›£æ§æ›¿ä»£ç´”TensorBoardä¾è³´
    success = monitor_training_process(cmd, env, training_logs_dir)
    
    if success:
        print("âœ… LoRA è¨“ç·´å®Œæˆ")
        
        # ğŸ¯ è©³ç´°é¡¯ç¤ºæ‰€æœ‰è¨“ç·´è¼¸å‡ºæ–‡ä»¶
        print("\n" + "="*60)
        print("ğŸ“ LoRA è¨“ç·´å®Œæˆå¾Œçš„è¼¸å‡ºæ–‡ä»¶è©³ç´°èªªæ˜")
        print("="*60)
        
        # 1. ä¸»è¦LoRAæ¨¡å‹æ–‡ä»¶
        final_lora = find_latest_lora()
        if final_lora:
            file_size = os.path.getsize(final_lora) / (1024*1024)
            print(f"\nğŸ¯ ä¸»è¦LoRAæ¨¡å‹æ–‡ä»¶:")
            print(f"   ğŸ“„ æ–‡ä»¶å: {os.path.basename(final_lora)}")
            print(f"   ğŸ“‚ ä½ç½®: {os.path.abspath(final_lora)}")
            print(f"   ğŸ“Š å¤§å°: {file_size:.2f} MB")
            print(f"   ğŸ’¡ èªªæ˜: é€™æ˜¯è¨“ç·´å®Œæˆçš„LoRAæ¬Šé‡æ–‡ä»¶ï¼Œå¯ç›´æ¥åœ¨WebUIä¸­ä½¿ç”¨")
        
        # 2. è¨“ç·´ç‹€æ…‹ç›®éŒ„
        state_dir = find_latest_state_dir()
        if state_dir:
            print(f"\nğŸ”„ è¨“ç·´ç‹€æ…‹ç›®éŒ„:")
            print(f"   ğŸ“‚ ä½ç½®: {os.path.abspath(state_dir)}")
            print(f"   ğŸ’¡ èªªæ˜: åŒ…å«å®Œæ•´çš„è¨“ç·´ç‹€æ…‹ï¼Œå¯ç”¨æ–¼ç¹¼çºŒè¨“ç·´")
            
            # åˆ—å‡ºç‹€æ…‹ç›®éŒ„å…§å®¹
            if os.path.exists(state_dir):
                state_files = os.listdir(state_dir)
                print(f"   ğŸ“‹ åŒ…å«æ–‡ä»¶: {', '.join(state_files[:5])}")
                if len(state_files) > 5:
                    print(f"   ã€€ã€€ã€€ã€€ã€€ã€€ï¼ˆé‚„æœ‰ {len(state_files)-5} å€‹å…¶ä»–æ–‡ä»¶...ï¼‰")
        
        # 3. è¨“ç·´æ—¥èªŒ
        log_dir = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs\logs"
        if os.path.exists(log_dir):
            print(f"\nğŸ“Š è¨“ç·´æ—¥èªŒ:")
            print(f"   ğŸ“‚ ä½ç½®: {os.path.abspath(log_dir)}")
            print(f"   ğŸ’¡ èªªæ˜: åŒ…å«æ¯æ­¥çš„lossè¨˜éŒ„å’ŒTensorBoardæ—¥èªŒ")
            
            # æª¢æŸ¥TensorBoardäº‹ä»¶æ–‡ä»¶
            tb_files = [f for f in os.listdir(log_dir) if f.startswith('events.out.tfevents')]
            if tb_files:
                print(f"   ğŸ“ˆ TensorBoardæ–‡ä»¶: {len(tb_files)} å€‹")
                print(f"   ğŸ¯ æŸ¥çœ‹æ–¹æ³•: åœ¨training_logs/logsç›®éŒ„åŸ·è¡Œ 'tensorboard --logdir .'")
        
        # 4. å‚™ä»½æ–‡ä»¶
        training_logs_dir = r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs"
        backup_files = [f for f in os.listdir(training_logs_dir) if f.startswith('lora_backup_') and f.endswith('.safetensors')]
        if backup_files:
            print(f"\nğŸ—„ï¸ å‚™ä»½æ–‡ä»¶:")
            print(f"   ğŸ“‚ ä½ç½®: {os.path.abspath(training_logs_dir)}")
            print(f"   ğŸ“„ å‚™ä»½æ–‡ä»¶: {len(backup_files)} å€‹")
            print(f"   ğŸ’¡ èªªæ˜: è¨“ç·´å‰çš„èˆŠæ¨¡å‹å‚™ä»½")
        
        # 5. è¼¸å‡ºæ–‡ä»¶ä½¿ç”¨èªªæ˜
        print(f"\nğŸ¯ å¦‚ä½•ä½¿ç”¨é€™äº›æ–‡ä»¶:")
        print(f"   1ï¸âƒ£ ä¸»æ¨¡å‹æ–‡ä»¶ï¼ˆ{os.path.basename(final_lora) if final_lora else 'last.safetensors'}ï¼‰")
        print(f"      â†’ è¤‡è£½åˆ° WebUI çš„ models/Lora/ ç›®éŒ„")
        print(f"      â†’ åœ¨ WebUI ä¸­å¯ç›´æ¥é¸æ“‡ä½¿ç”¨")
        print(f"   2ï¸âƒ£ ç‹€æ…‹ç›®éŒ„")
        print(f"      â†’ ç”¨æ–¼ç¹¼çºŒè¨“ç·´ï¼špython train_lora.py --continue")
        print(f"   3ï¸âƒ£ TensorBoardæ—¥èªŒ")
        print(f"      â†’ æŸ¥çœ‹è¨“ç·´æ›²ç·šï¼šcd training_logs/logs && tensorboard --logdir .")
        
        print("="*60)
        
        # ğŸ¯ ç”Ÿæˆlossè¨“ç·´å ±å‘Š - å„ªå…ˆä½¿ç”¨å…§å»ºæ—¥èªŒ
        print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆè¨“ç·´å ±å‘Š...")
        
        # é¦–å…ˆå˜—è©¦å¾å…§å»ºæ—¥èªŒç”Ÿæˆå ±å‘Š
        builtin_log_file = os.path.join(training_logs_dir, "training_loss_log.txt")
        if os.path.exists(builtin_log_file):
            print(f"âœ… ä½¿ç”¨å…§å»ºlossæ—¥èªŒç”Ÿæˆå ±å‘Š")
            report_success = generate_loss_report_from_log(builtin_log_file, training_logs_dir)
        else:
            # å¦‚æœå…§å»ºæ—¥èªŒä¸å­˜åœ¨ï¼Œå˜—è©¦ä½¿ç”¨TensorBoardæ—¥èªŒ
            print(f"âš ï¸ å…§å»ºæ—¥èªŒä¸å­˜åœ¨ï¼Œå˜—è©¦ä½¿ç”¨TensorBoardæ—¥èªŒ")
            report_success = generate_loss_report(
                log_dir=r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs\logs", 
                output_dir=r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs"
            )
        
        if report_success:
            print(f"âœ… è¨“ç·´å ±å‘Šç”Ÿæˆå®Œæˆ")
            print(f"   ğŸ“„ JSONå ±å‘Š: lora_training_report_*.json")
            print(f"   ğŸ“ˆ PNGåœ–è¡¨: lora_training_curves_*.png")
        else:
            print(f"âš ï¸ å ±å‘Šç”Ÿæˆå¤±æ•—ï¼Œä½†è¨“ç·´å·²å®Œæˆ")
        
        if continue_from_checkpoint:
            print("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
        else:
            print("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        return True
    else:
        print("âŒ LoRA è¨“ç·´å¤±æ•—")
        return False

def get_current_training_step(state_dir):
    """å¾è¨“ç·´ç‹€æ…‹ä¸­ç²å–ç•¶å‰æ­¥æ•¸"""
    try:
        import json
        state_file = os.path.join(state_dir, "train_state.json")
        if os.path.exists(state_file):
            with open(state_file, 'r', encoding='utf-8') as f:
                state = json.load(f)
                return state.get('current_step', 0)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å–è¨“ç·´ç‹€æ…‹: {e}")
    return 0

def calculate_smart_max_steps(current_step, additional_steps=100):
    """æ™ºèƒ½è¨ˆç®—æœ€å¤§è¨“ç·´æ­¥æ•¸"""
    if current_step == 0:
        # æ–°è¨“ç·´ï¼Œä½¿ç”¨é»˜èªæ­¥æ•¸
        return additional_steps
    else:
        # ç¹¼çºŒè¨“ç·´ï¼Œåœ¨ç•¶å‰æ­¥æ•¸åŸºç¤ä¸Šå¢åŠ 
        new_max_steps = current_step + additional_steps
        print(f"ğŸ“Š ç•¶å‰å·²å®Œæˆæ­¥æ•¸: {current_step}")
        print(f"ğŸ“Š è¨ˆåŠƒå¢åŠ æ­¥æ•¸: {additional_steps}")
        print(f"ğŸ“Š æ–°çš„æœ€å¤§æ­¥æ•¸: {new_max_steps}")
        return new_max_steps

def generate_loss_report(log_dir=r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs\logs", 
                        output_dir=r"E:\Yung_Folder\Project\stable-diffusion-webui\auto_test_pipeline\training_logs"):
    """
    Generate loss report in both JSON and PNG formats with English-only content.
    
    Features:
    - All JSON keys, values, and descriptions are in English
    - PNG chart titles, labels, and annotations are in English  
    - Metrics are organized by type (loss_data, learning_rate_data, other_metrics)
    - Clean JSON keys with normalized naming (no special characters)
    - Comprehensive metadata including descriptions and statistics
    - Professional chart formatting with main title and grid
    
    Args:
        log_dir: Directory containing TensorBoard logs
        output_dir: Directory to save JSON and PNG reports
        
    Returns:
        bool: True if successful, False if failed
    """
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆlossè¨“ç·´å ±å‘Š...")
    
    try:
        from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
        import matplotlib.pyplot as plt
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive backend
        
        # Check log directory
        if not os.path.exists(log_dir):
            print(f"âŒ æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨: {log_dir}")
            return False
        
        # Load TensorBoard data
        ea = EventAccumulator(log_dir)
        ea.Reload()
        
        # Get all available scalar tags
        scalar_tags = ea.Tags()['scalars']
        print(f"ğŸ“‹ æ‰¾åˆ°æ•¸æ“šæ¨™ç±¤: {scalar_tags}")
        
        # Prepare report data with English-only content
        report_data = {
            "training_info": {
                "timestamp": datetime.datetime.now().isoformat(),
                "log_directory": os.path.abspath(log_dir),
                "total_metrics": len(scalar_tags),
                "description": "LoRA Training Report - Generated automatically after training completion"
            },
            "loss_data": {},
            "learning_rate_data": {},
            "other_metrics": {}
        }
        
        # Extract all loss-related data
        loss_tags = [tag for tag in scalar_tags if 'loss' in tag.lower()]
        lr_tags = [tag for tag in scalar_tags if 'lr' in tag.lower()]
        other_tags = [tag for tag in scalar_tags if 'loss' not in tag.lower() and 'lr' not in tag.lower()]
        
        print(f"ğŸ“ˆ LossæŒ‡æ¨™: {len(loss_tags)} å€‹")
        print(f"ğŸ“‰ å­¸ç¿’ç‡æŒ‡æ¨™: {len(lr_tags)} å€‹") 
        print(f"ğŸ“Š å…¶ä»–æŒ‡æ¨™: {len(other_tags)} å€‹")
        
        # Process loss data with English keys
        for tag in loss_tags:
            scalar_events = ea.Scalars(tag)
            steps = [event.step for event in scalar_events]
            values = [event.value for event in scalar_events]
            
            # Clean tag name for JSON key
            clean_tag = tag.replace('/', '_').replace(' ', '_').replace('-', '_').lower()
            
            report_data["loss_data"][clean_tag] = {
                "metric_name": tag,
                "steps": steps,
                "values": values,
                "total_points": len(steps),
                "min_value": min(values) if values else 0,
                "max_value": max(values) if values else 0,
                "final_value": values[-1] if values else 0,
                "step_range": [min(steps), max(steps)] if steps else [0, 0],
                "description": f"Loss metric tracking for {tag}"
            }
        
        # Process learning rate data with English keys
        for tag in lr_tags:
            scalar_events = ea.Scalars(tag)
            steps = [event.step for event in scalar_events]
            values = [event.value for event in scalar_events]
            
            # Clean tag name for JSON key
            clean_tag = tag.replace('/', '_').replace(' ', '_').replace('-', '_').lower()
            
            report_data["learning_rate_data"][clean_tag] = {
                "metric_name": tag,
                "steps": steps,
                "values": values,
                "total_points": len(steps),
                "initial_lr": values[0] if values else 0,
                "final_lr": values[-1] if values else 0,
                "description": f"Learning rate schedule for {tag}"
            }
        
        # Process other metrics with English keys
        for tag in other_tags:
            scalar_events = ea.Scalars(tag)
            steps = [event.step for event in scalar_events]
            values = [event.value for event in scalar_events]
            
            # Clean tag name for JSON key
            clean_tag = tag.replace('/', '_').replace(' ', '_').replace('-', '_').lower()
            
            report_data["other_metrics"][clean_tag] = {
                "metric_name": tag,
                "steps": steps,
                "values": values,
                "total_points": len(steps),
                "description": f"Training metric for {tag}"
            }
        
        # Generate timestamp
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Save JSON report with English content
        json_filename = f"lora_training_report_{timestamp}.json"
        json_path = os.path.join(output_dir, json_filename)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… JSONå ±å‘Šå·²ä¿å­˜: {json_filename}")
        
        # 2. Generate PNG chart with English labels
        png_filename = f"lora_training_curves_{timestamp}.png"
        png_path = os.path.join(output_dir, png_filename)
        
        # Set English font (no Chinese fonts)
        plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
        plt.rcParams['axes.unicode_minus'] = False
        
        # Calculate number of subplots
        total_plots = len(loss_tags) + (1 if lr_tags else 0)
        if total_plots == 0:
            print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°å¯ç¹ªè£½çš„æ•¸æ“š")
            return True
        
        # Create subplots
        if total_plots == 1:
            fig, axes = plt.subplots(1, 1, figsize=(12, 6))
            axes = [axes]
        elif total_plots <= 2:
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        elif total_plots <= 4:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        else:
            rows = (total_plots + 2) // 3
            fig, axes = plt.subplots(rows, 3, figsize=(18, 6 * rows))
            axes = axes.flatten() if rows > 1 else axes
        
        plot_idx = 0
        
        # Plot loss curves with English labels
        for tag in loss_tags:
            if plot_idx >= len(axes):
                break
                
            clean_tag = tag.replace('/', '_').replace(' ', '_').replace('-', '_').lower()
            data = report_data["loss_data"][clean_tag]
            
            axes[plot_idx].plot(data["steps"], data["values"], 'b-', linewidth=2, label=tag)
            axes[plot_idx].set_title(f'Loss Curve: {tag}', fontsize=12, fontweight='bold')
            axes[plot_idx].set_xlabel('Training Steps')
            axes[plot_idx].set_ylabel('Loss Value')
            axes[plot_idx].grid(True, alpha=0.3)
            axes[plot_idx].legend()
            
            # Add statistics in English
            final_loss = data["final_value"]
            min_loss = data["min_value"]
            axes[plot_idx].text(0.02, 0.98, f'Final: {final_loss:.6f}\nMin: {min_loss:.6f}', 
                               transform=axes[plot_idx].transAxes, verticalalignment='top',
                               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            plot_idx += 1
        
        # Plot learning rate curves with English labels (if available)
        if lr_tags and plot_idx < len(axes):
            # Plot all learning rate curves on the same chart
            for tag in lr_tags:
                clean_tag = tag.replace('/', '_').replace(' ', '_').replace('-', '_').lower()
                data = report_data["learning_rate_data"][clean_tag]
                axes[plot_idx].plot(data["steps"], data["values"], linewidth=2, label=tag)
            
            axes[plot_idx].set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')
            axes[plot_idx].set_xlabel('Training Steps')
            axes[plot_idx].set_ylabel('Learning Rate')
            axes[plot_idx].grid(True, alpha=0.3)
            axes[plot_idx].legend()
            axes[plot_idx].set_yscale('log')  # Use logarithmic scale
            plot_idx += 1
        
        # Hide unused subplots
        for i in range(plot_idx, len(axes)):
            axes[i].set_visible(False)
        
        # Add main title
        fig.suptitle('LoRA Training Progress Report', fontsize=16, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(png_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… PNGåœ–è¡¨å·²ä¿å­˜: {png_filename}")
        
        # 3. Generate summary statistics with English content
        print(f"\nğŸ“Š è¨“ç·´çµ±è¨ˆæ‘˜è¦:")
        
        if report_data['loss_data']:
            max_steps = max([max(data['steps']) for data in report_data['loss_data'].values()])
            print(f"   ç¸½è¨“ç·´æ­¥æ•¸: {max_steps}")
            
            for key, data in report_data["loss_data"].items():
                metric_name = data['metric_name']
                print(f"   {metric_name}:")
                print(f"     æœ€çµ‚å€¼: {data['final_value']:.6f}")
                print(f"     æœ€å°å€¼: {data['min_value']:.6f}")
                print(f"     æ•¸æ“šé»: {data['total_points']} å€‹")
        else:
            print(f"   ç¸½è¨“ç·´æ­¥æ•¸: 0")
        
        return True
        
    except ImportError:
        print(f"âš ï¸ ç¼ºå°‘å¿…è¦çš„åº«ï¼Œç„¡æ³•ç”Ÿæˆåœ–è¡¨")
        print(f"ğŸ’¡ è«‹å®‰è£: pip install tensorboard matplotlib")
        return False
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå ±å‘Šæ™‚å‡ºéŒ¯: {e}")
        return False

def create_loss_tracker(output_dir):
    """å‰µå»ºå…§å»ºçš„lossè¿½è¹¤å™¨ï¼Œä¸ä¾è³´TensorBoard"""
    tracker_file = os.path.join(output_dir, "training_loss_log.txt")
    
    # å‰µå»ºlossè¿½è¹¤æ—¥èªŒæ–‡ä»¶
    with open(tracker_file, 'w', encoding='utf-8') as f:
        f.write("# LoRA Training Loss Log\n")
        f.write("# Format: step,epoch,loss,learning_rate,timestamp\n")
        f.write("step,epoch,loss,learning_rate,timestamp\n")
    
    return tracker_file

def monitor_training_process(cmd, env, output_dir):
    """ç›£æ§è¨“ç·´éç¨‹ä¸¦è¨˜éŒ„lossæ•¸æ“š"""
    import subprocess
    import re
    import time
    
    # å‰µå»ºlossè¿½è¹¤å™¨
    loss_tracker_file = create_loss_tracker(output_dir)
    
    print("ğŸš€ æ­£åœ¨åŸ·è¡Œè¨“ç·´ä¸¦ç›£æ§loss...")
    
    # å‰µå»ºé€²ç¨‹ä¾†ç›£æ§è¼¸å‡º
    process = subprocess.Popen(
        cmd,
        shell=True,
        env=env,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        universal_newlines=True,
        encoding='utf-8',  # ğŸ”§ FIX: æ˜ç¢ºæŒ‡å®š UTF-8 ç·¨ç¢¼
        errors='replace',  # ğŸ”§ FIX: é‡åˆ°ç„¡æ³•è§£ç¢¼çš„å­—ç¬¦æ™‚æ›¿æ›è€Œéå ±éŒ¯
        bufsize=1
    )
    
    # ç›£æ§è¼¸å‡ºä¸¦æå–lossæ•¸æ“š - æ”¯æ´å¤šç¨®ä¸»æµæ ¼å¼
    loss_patterns = [
        # æ¨™æº–æ ¼å¼: Step: 100, Loss: 0.5
        re.compile(r'(?:step|Step):\s*(\d+).*?(?:loss|Loss):\s*([\d\.e\-\+]+)', re.IGNORECASE),
        # æ‹¬è™Ÿæ ¼å¼: [100/1000] loss: 0.5
        re.compile(r'\[(\d+)/\d+\].*?(?:loss|Loss):\s*([\d\.e\-\+]+)', re.IGNORECASE),
        # ç°¡çŸ­æ ¼å¼: 100 loss 0.5
        re.compile(r'(\d+)\s+(?:loss|Loss)\s+([\d\.e\-\+]+)', re.IGNORECASE),
        # é€²åº¦æ¢æ ¼å¼: Step 100/1000 Loss: 0.5
        re.compile(r'(?:step|Step)\s+(\d+)/\d+.*?(?:loss|Loss):\s*([\d\.e\-\+]+)', re.IGNORECASE),
        # æ–œç·šæ ¼å¼: 100/1000 loss=0.5
        re.compile(r'(\d+)/\d+.*?(?:loss|Loss)=\s*([\d\.e\-\+]+)', re.IGNORECASE),
        # æ™‚é–“æˆ³æ ¼å¼: [2024-01-01 10:00:00] Step: 100 Loss: 0.5
        re.compile(r'\[.*?\].*?(?:step|Step):\s*(\d+).*?(?:loss|Loss):\s*([\d\.e\-\+]+)', re.IGNORECASE)
    ]
    epoch_pattern = re.compile(r'(?:epoch|Epoch):\s*(\d+)', re.IGNORECASE)
    lr_pattern = re.compile(r'(?:lr|learning.rate):\s*([\d\.e\-\+]+)', re.IGNORECASE)
    
    current_epoch = 0
    current_lr = "unknown"
    
    try:
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            
            if output:
                line = output.strip()
                print(line)  # é¡¯ç¤ºåŸå§‹è¼¸å‡º
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«epochä¿¡æ¯
                epoch_match = epoch_pattern.search(line)
                if epoch_match:
                    current_epoch = epoch_match.group(1)
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«å­¸ç¿’ç‡ä¿¡æ¯
                lr_match = lr_pattern.search(line)
                if lr_match:
                    current_lr = lr_match.group(1)
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«lossä¿¡æ¯ - å˜—è©¦æ‰€æœ‰æ ¼å¼
                loss_match = None
                step = None
                loss = None
                for pattern in loss_patterns:
                    loss_match = pattern.search(line)
                    if loss_match:
                        step = loss_match.group(1)
                        loss = loss_match.group(2)
                        break
                
                if loss_match:
                    timestamp = datetime.datetime.now().isoformat()
                    
                    # è¨˜éŒ„åˆ°lossè¿½è¹¤æ–‡ä»¶
                    with open(loss_tracker_file, 'a', encoding='utf-8') as f:
                        f.write(f"{step},{current_epoch},{loss},{current_lr},{timestamp}\n")
                    
                    print(f"ğŸ“Š è¨˜éŒ„Loss: Step {step}, Loss {loss}")
        
        return_code = process.poll()
        return return_code == 0
        
    except KeyboardInterrupt:
        print("âš ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        process.terminate()
        return False
    except Exception as e:
        print(f"âŒ ç›£æ§è¨“ç·´éç¨‹æ™‚å‡ºéŒ¯: {e}")
        return False

def generate_loss_report_from_log(log_file, output_dir):
    """å¾å…§å»ºæ—¥èªŒæ–‡ä»¶ç”Ÿæˆlosså ±å‘Š"""
    print(f"\nğŸ“Š å¾å…§å»ºæ—¥èªŒç”Ÿæˆè¨“ç·´å ±å‘Š...")
    
    try:
        if not os.path.exists(log_file):
            print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´æ—¥èªŒæ–‡ä»¶: {log_file}")
            return False
        
        # è®€å–lossæ•¸æ“š
        steps = []
        epochs = []
        losses = []
        learning_rates = []
        timestamps = []
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        # è·³éæ¨™é¡Œè¡Œ
        data_lines = [line for line in lines if not line.startswith('#') and line.strip() and 'step,epoch' not in line]
        
        for line in data_lines:
            try:
                parts = line.strip().split(',')
                if len(parts) >= 5:
                    step = int(parts[0])
                    epoch = parts[1]
                    loss = float(parts[2])
                    lr = parts[3]
                    timestamp = parts[4]
                    
                    steps.append(step)
                    epochs.append(epoch)
                    losses.append(loss)
                    learning_rates.append(lr)
                    timestamps.append(timestamp)
            except (ValueError, IndexError) as e:
                print(f"âš ï¸ è·³éç„¡æ•ˆè¡Œ: {line.strip()}")
                continue
        
        if not steps:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„lossæ•¸æ“š")
            return False
        
        print(f"âœ… æˆåŠŸè®€å– {len(steps)} å€‹è¨“ç·´æ­¥é©Ÿçš„æ•¸æ“š")
        
        # ç”Ÿæˆæ™‚é–“æˆ³
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. ç”ŸæˆJSONå ±å‘Š
        report_data = {
            "training_info": {
                "timestamp": datetime.datetime.now().isoformat(),
                "total_steps": len(steps),
                "final_step": max(steps) if steps else 0,
                "final_loss": losses[-1] if losses else 0,
                "best_loss": min(losses) if losses else 0,
                "description": "LoRA Training Report - Generated from built-in loss tracking"
            },
            "loss_data": {
                "training_loss": {
                    "metric_name": "training_loss",
                    "steps": steps,
                    "values": losses,
                    "total_points": len(steps),
                    "min_value": min(losses) if losses else 0,
                    "max_value": max(losses) if losses else 0,
                    "final_value": losses[-1] if losses else 0,
                    "step_range": [min(steps), max(steps)] if steps else [0, 0],
                    "description": "Training loss tracked during LoRA training"
                }
            },
            "raw_data": {
                "steps": steps,
                "epochs": epochs,
                "losses": losses,
                "learning_rates": learning_rates,
                "timestamps": timestamps
            }
        }
        
        json_filename = f"lora_training_report_{timestamp}.json"
        json_path = os.path.join(output_dir, json_filename)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… JSONå ±å‘Šå·²ä¿å­˜: {json_filename}")
        
        # 2. ç”ŸæˆPNGåœ–è¡¨
        try:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            # è¨­å®šå­—é«”
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False
            
            # å‰µå»ºåœ–è¡¨
            fig, ax = plt.subplots(1, 1, figsize=(12, 8))
            
            # ç¹ªè£½lossæ›²ç·š
            ax.plot(steps, losses, 'b-', linewidth=2, label='Training Loss')
            ax.set_title('LoRA Training Loss Curve', fontsize=16, fontweight='bold')
            ax.set_xlabel('Training Steps', fontsize=12)
            ax.set_ylabel('Loss Value', fontsize=12)
            ax.grid(True, alpha=0.3)
            ax.legend()
            
            # æ·»åŠ çµ±è¨ˆä¿¡æ¯
            final_loss = losses[-1] if losses else 0
            min_loss = min(losses) if losses else 0
            max_loss = max(losses) if losses else 0
            
            stats_text = f'Final Loss: {final_loss:.6f}\nMin Loss: {min_loss:.6f}\nMax Loss: {max_loss:.6f}\nTotal Steps: {len(steps)}'
            ax.text(0.02, 0.98, stats_text, 
                   transform=ax.transAxes, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            plt.tight_layout()
            
            png_filename = f"lora_training_curves_{timestamp}.png"
            png_path = os.path.join(output_dir, png_filename)
            plt.savefig(png_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… PNGåœ–è¡¨å·²ä¿å­˜: {png_filename}")
            
        except ImportError:
            print(f"âš ï¸ matplotlibæœªå®‰è£ï¼Œè·³éPNGåœ–è¡¨ç”Ÿæˆ")
        except Exception as e:
            print(f"âš ï¸ PNGåœ–è¡¨ç”Ÿæˆå¤±æ•—: {e}")
        
        # 3. ç”Ÿæˆçµ±è¨ˆæ‘˜è¦
        print(f"\nğŸ“Š è¨“ç·´çµ±è¨ˆæ‘˜è¦:")
        print(f"   ç¸½è¨“ç·´æ­¥æ•¸: {len(steps)}")
        print(f"   æœ€çµ‚Loss: {losses[-1]:.6f}")
        print(f"   æœ€ä½³Loss: {min(losses):.6f}")
        print(f"   æœ€å·®Loss: {max(losses):.6f}")
        print(f"   Lossæ”¹å–„: {((losses[0] - losses[-1]) / losses[0] * 100):.2f}%" if len(losses) > 1 else "N/A")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå ±å‘Šæ™‚å‡ºéŒ¯: {e}")
        return False

# æª¢æŸ¥ä¾è³´é …çš„å‡½æ•¸
def main():
    """ä¸»å‡½æ•¸ - è™•ç†å‘½ä»¤è¡Œåƒæ•¸"""
    # ğŸ”§ FIX: é–‹å§‹æ™‚å°±æª¢æŸ¥ä¾è³´é …
    print("ğŸ” æª¢æŸ¥ç³»çµ±ä¾è³´...")
    if not check_dependencies():
        print("âŒ ä¾è³´æª¢æŸ¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
        sys.exit(1)
    print("âœ… ä¾è³´æª¢æŸ¥é€šé")
    
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬")
    parser.add_argument("--continue", "-c", action="store_true", 
                       dest="continue_training",
                       help="å¾ç¾æœ‰çš„ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´")
    parser.add_argument("--new", "-n", action="store_true",
                       dest="new_training", 
                       help="é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
    parser.add_argument("--steps", "-s", type=int,
                       help="æŒ‡å®šè¨“ç·´æ­¥æ•¸ (è·³éäº¤äº’å¼è©¢å•)")
    
    args = parser.parse_args()
    
    # ğŸ”§ FIX: å¢å¼·æ¨¡å¼æ±ºå®šé‚è¼¯
    print("\nğŸ” æª¢æŸ¥ç¾æœ‰æ¨¡å‹å’Œç‹€æ…‹...")
    existing_lora = find_latest_lora()
    existing_state = find_latest_state_dir()
    
    if existing_lora:
        print(f"ğŸ“„ ç™¼ç¾ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
    else:
        print("âŒ æ²’æœ‰ç™¼ç¾ç¾æœ‰ LoRA æ¨¡å‹")
        
    if existing_state:
        print(f"ğŸ“ ç™¼ç¾è¨“ç·´ç‹€æ…‹: {os.path.basename(existing_state)}")
    else:
        print("âŒ æ²’æœ‰ç™¼ç¾è¨“ç·´ç‹€æ…‹")
    
    # æ±ºå®šè¨“ç·´æ¨¡å¼
    if args.continue_training and args.new_training:
        print("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        sys.exit(1)
    elif args.continue_training:
        if not existing_lora and not existing_state:
            print("âŒ éŒ¯èª¤ï¼šæŒ‡å®šç¹¼çºŒè¨“ç·´ä½†æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹æˆ–ç‹€æ…‹")
            sys.exit(1)
        print("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        continue_from_checkpoint = True
    elif args.new_training:
        print("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        continue_from_checkpoint = False
    else:
        # ğŸ”§ FIX: æ”¹é€²äº¤äº’å¼é¸æ“‡é‚è¼¯
        if existing_lora or existing_state:
            print(f"\nè«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
            print("1. å¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´ (ç´¯ç©èª¿æ•™)")
            print("2. é–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´ (é‡æ–°é–‹å§‹)")
            
            while True:
                choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
                if choice == "1":
                    continue_from_checkpoint = True
                    print("ğŸ”„ å·²é¸æ“‡ï¼šç¹¼çºŒè¨“ç·´æ¨¡å¼")
                    break
                elif choice == "2":
                    continue_from_checkpoint = False
                    print("ğŸ†• å·²é¸æ“‡ï¼šæ–°è¨“ç·´æ¨¡å¼")
                    break
                else:
                    print("âŒ è«‹è¼¸å…¥ 1 æˆ– 2")
        else:
            print("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            continue_from_checkpoint = False
    
    # ğŸ”§ FIX: é©—è­‰æ­¥æ•¸åƒæ•¸
    if args.steps is not None:
        if args.steps <= 0:
            print(f"âŒ éŒ¯èª¤ï¼šæ­¥æ•¸å¿…é ˆå¤§æ–¼0ï¼Œæ‚¨è¼¸å…¥çš„æ˜¯ {args.steps}")
            sys.exit(1)
        print(f"ğŸ“Š å°‡ä½¿ç”¨æŒ‡å®šæ­¥æ•¸: {args.steps}")
    
    # åŸ·è¡Œè¨“ç·´
    success = train_lora(continue_from_checkpoint, custom_steps=args.steps)
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()