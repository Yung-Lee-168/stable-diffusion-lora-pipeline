#!/usr/bin/env python3
"""
çµ±ä¸€ä¸‰å€‹æ€§èƒ½æŒ‡æ¨™å¯¦ç¾ - ä¿®å¾©è…³æœ¬
ç¢ºä¿ analyze_results.py å’Œ day3_fashion_training.py ä½¿ç”¨å®Œå…¨ç›¸åŒçš„å‡½æ•¸å’Œå…¬å¼
"""

import cv2
import numpy as np
from skimage.metrics import structural_similarity as ssim
from PIL import Image
import torch
from sklearn.metrics.pairwise import cosine_similarity

class UnifiedPerformanceMetrics:
    """çµ±ä¸€çš„æ€§èƒ½æŒ‡æ¨™å¯¦ç¾"""
    
    @staticmethod
    def calculate_ssim_similarity(img1_path_or_array, img2_path_or_array):
        """
        çµ±ä¸€çš„ SSIM çµæ§‹ç›¸ä¼¼åº¦è¨ˆç®—
        å…©å€‹è…³æœ¬éƒ½æ‡‰è©²ä½¿ç”¨é€™å€‹å¯¦ç¾
        """
        try:
            # è™•ç†è¼¸å…¥ - æ”¯æŒæ–‡ä»¶è·¯å¾‘æˆ–åœ–ç‰‡æ•¸çµ„
            if isinstance(img1_path_or_array, str):
                img1 = cv2.imread(img1_path_or_array)
            else:
                img1 = np.array(img1_path_or_array)
                
            if isinstance(img2_path_or_array, str):
                img2 = cv2.imread(img2_path_or_array)  
            else:
                img2 = np.array(img2_path_or_array)
            
            if img1 is None or img2 is None:
                return None
            
            # ç¢ºä¿æ˜¯ BGR æ ¼å¼ (OpenCV æ¨™æº–)
            if len(img1.shape) == 3 and img1.shape[2] == 3:
                if isinstance(img1_path_or_array, str):
                    # å¾æ–‡ä»¶è®€å–ï¼Œå·²ç¶“æ˜¯ BGR
                    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                else:
                    # å¾æ•¸çµ„è½‰æ›ï¼Œå‡è¨­æ˜¯ RGB
                    gray1 = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)
            else:
                gray1 = img1
                
            if len(img2.shape) == 3 and img2.shape[2] == 3:
                if isinstance(img2_path_or_array, str):
                    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
                else:
                    gray2 = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)
            else:
                gray2 = img2
            
            # å°ºå¯¸å°é½Š - ä½¿ç”¨è¼ƒå°å°ºå¯¸
            if gray1.shape != gray2.shape:
                target_shape = (min(gray1.shape[0], gray2.shape[0]), 
                              min(gray1.shape[1], gray2.shape[1]))
                gray1 = cv2.resize(gray1, (target_shape[1], target_shape[0]))
                gray2 = cv2.resize(gray2, (target_shape[1], target_shape[0]))
            
            # ğŸ¯ çµ±ä¸€ä½¿ç”¨ skimage.metrics.ssim
            similarity = ssim(gray1, gray2)
            return similarity
            
        except Exception as e:
            print(f"âŒ SSIMè¨ˆç®—å¤±æ•—: {e}")
            return None
    
    @staticmethod 
    def calculate_color_similarity(img1_path_or_array, img2_path_or_array):
        """
        çµ±ä¸€çš„è‰²å½©åˆ†å¸ƒç›¸ä¼¼åº¦è¨ˆç®—
        å…©å€‹è…³æœ¬éƒ½æ‡‰è©²ä½¿ç”¨é€™å€‹å¯¦ç¾
        """
        try:
            # è™•ç†è¼¸å…¥
            if isinstance(img1_path_or_array, str):
                img1 = cv2.imread(img1_path_or_array)
                img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)
            else:
                img1_rgb = np.array(img1_path_or_array)
                
            if isinstance(img2_path_or_array, str):
                img2 = cv2.imread(img2_path_or_array)
                img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)
            else:
                img2_rgb = np.array(img2_path_or_array)
            
            if img1_rgb is None or img2_rgb is None:
                return None
            
            # ğŸ¯ çµ±ä¸€çš„ç›´æ–¹åœ–è¨ˆç®— - 32x32x32 RGB bins
            hist1 = cv2.calcHist([img1_rgb], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])
            hist2 = cv2.calcHist([img2_rgb], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])
            
            # ğŸ¯ çµ±ä¸€æ­£è¦åŒ–æ­¥é©Ÿ
            hist1 = cv2.normalize(hist1, hist1).flatten()
            hist2 = cv2.normalize(hist2, hist2).flatten()
            
            # ğŸ¯ çµ±ä¸€ç›¸é—œä¿‚æ•¸è¨ˆç®—
            correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
            
            return correlation
            
        except Exception as e:
            print(f"âŒ è‰²å½©ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return None
    
    @staticmethod
    def calculate_fashionclip_feature_similarity(fashion_clip_model, fashion_clip_processor, img1, img2):
        """
        çµ±ä¸€çš„ FashionCLIP ç‰¹å¾µå‘é‡ç›¸ä¼¼åº¦è¨ˆç®—
        ä½¿ç”¨ç‰¹å¾µå‘é‡é¤˜å¼¦ç›¸ä¼¼åº¦ (æ›´æº–ç¢ºçš„èªç¾©æ¯”è¼ƒ)
        """
        try:
            if not fashion_clip_model or not fashion_clip_processor:
                return None
                
            device = next(fashion_clip_model.parameters()).device
            model_dtype = next(fashion_clip_model.parameters()).device
            
            # è™•ç†åœ–ç‰‡è¼¸å…¥
            inputs = fashion_clip_processor(
                images=[img1, img2], 
                return_tensors="pt"
            )
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç¢ºä¿æ•¸æ“šé¡å‹ä¸€è‡´
            if model_dtype == torch.float16:
                for key in inputs:
                    if inputs[key].dtype == torch.float32:
                        inputs[key] = inputs[key].half()
            
            with torch.no_grad():
                image_features = fashion_clip_model.get_image_features(**inputs)
                # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                fashion_similarity = cosine_similarity(
                    image_features[0:1].cpu().numpy(), 
                    image_features[1:2].cpu().numpy()
                )[0][0]
                
            return float(fashion_similarity)
            
        except Exception as e:
            print(f"âŒ FashionCLIPç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return None
    
    @staticmethod
    def calculate_fashionclip_label_similarity(orig_analysis, gen_analysis):
        """
        çµ±ä¸€çš„ FashionCLIP æ¨™ç±¤æ¯”è¼ƒç›¸ä¼¼åº¦è¨ˆç®—
        ä½¿ç”¨æ¨™ç±¤åŒ¹é…å’Œä¿¡å¿ƒåº¦æ¯”è¼ƒ (é©ç”¨æ–¼å·²åˆ†æçš„ç‰¹å¾µ)
        """
        try:
            if not orig_analysis or not gen_analysis:
                return None
            
            similarities = []
            
            # æ¯”è¼ƒæ¯å€‹é¡åˆ¥
            for category in orig_analysis.keys():
                if category in gen_analysis:
                    orig_top = orig_analysis[category]["top_label"]
                    gen_top = gen_analysis[category]["top_label"]
                    orig_conf = orig_analysis[category]["confidence"]
                    gen_conf = gen_analysis[category]["confidence"]
                    
                    # æ¨™ç±¤åŒ¹é…åº¦
                    label_match = 1.0 if orig_top == gen_top else 0.0
                    
                    # ä¿¡å¿ƒåº¦ç›¸ä¼¼æ€§
                    conf_similarity = 1.0 - abs(orig_conf - gen_conf)
                    
                    # ğŸ¯ çµ±ä¸€çš„é¡åˆ¥ç›¸ä¼¼åº¦å…¬å¼
                    category_similarity = 0.7 * label_match + 0.3 * conf_similarity
                    similarities.append(category_similarity)
            
            # è¨ˆç®—å¹³å‡ç›¸ä¼¼åº¦
            average_similarity = sum(similarities) / len(similarities) if similarities else 0.0
            
            return average_similarity
            
        except Exception as e:
            print(f"âŒ FashionCLIPæ¨™ç±¤ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
            return None
    
    @staticmethod
    def calculate_combined_loss(visual_similarity, fashion_similarity, color_similarity, 
                              weights=None):
        """
        çµ±ä¸€çš„çµ„åˆæå¤±å‡½æ•¸
        å…©å€‹è…³æœ¬éƒ½æ‡‰è©²ä½¿ç”¨é€™å€‹å¯¦ç¾
        """
        if weights is None:
            # ğŸ¯ çµ±ä¸€çš„é»˜èªæ¬Šé‡é…ç½®
            weights = {
                "visual": 0.2,      # SSIM çµæ§‹ç›¸ä¼¼åº¦
                "fashion_clip": 0.6, # FashionCLIP èªç¾©ç›¸ä¼¼åº¦ (ä¸»è¦æŒ‡æ¨™)
                "color": 0.2        # è‰²å½©åˆ†å¸ƒç›¸ä¼¼åº¦
            }
        
        # å°‡ç›¸ä¼¼åº¦è½‰æ›ç‚ºæå¤± (1 - similarity)
        visual_loss = 1.0 - (visual_similarity if visual_similarity is not None else 0.0)
        fashion_clip_loss = 1.0 - (fashion_similarity if fashion_similarity is not None else 0.0)
        color_loss = 1.0 - (color_similarity if color_similarity is not None else 0.0)
        
        # ğŸ¯ çµ±ä¸€çš„çµ„åˆæå¤±å…¬å¼
        total_loss = (
            weights["visual"] * visual_loss +
            weights["fashion_clip"] * fashion_clip_loss +
            weights["color"] * color_loss
        )
        
        return {
            "total_loss": total_loss,
            "visual_loss": visual_loss,
            "fashion_clip_loss": fashion_clip_loss,
            "color_loss": color_loss,
            "weights": weights,
            "similarities": {
                "visual": visual_similarity,
                "fashion_clip": fashion_similarity,
                "color": color_similarity
            }
        }

def demonstrate_unified_implementation():
    """æ¼”ç¤ºçµ±ä¸€å¯¦ç¾çš„ä½¿ç”¨æ–¹æ³•"""
    print("ğŸ¯ çµ±ä¸€æ€§èƒ½æŒ‡æ¨™å¯¦ç¾æ¼”ç¤º")
    print("=" * 60)
    
    print("\nğŸ“Š ä¸‰å€‹æ ¸å¿ƒæŒ‡æ¨™çš„çµ±ä¸€å¯¦ç¾:")
    print("1. SSIM çµæ§‹ç›¸ä¼¼åº¦:")
    print("   å‡½æ•¸: UnifiedPerformanceMetrics.calculate_ssim_similarity()")
    print("   å¯¦ç¾: skimage.metrics.ssim + å°ºå¯¸å°é½Š")
    print("   ç¯„åœ: [-1, 1], 1=å®Œå…¨ç›¸åŒ")
    
    print("\n2. è‰²å½©åˆ†å¸ƒç›¸ä¼¼åº¦:")
    print("   å‡½æ•¸: UnifiedPerformanceMetrics.calculate_color_similarity()")
    print("   å¯¦ç¾: 32Ã—32Ã—32 RGBç›´æ–¹åœ– + normalize + ç›¸é—œä¿‚æ•¸")
    print("   ç¯„åœ: [-1, 1], 1=å®Œå…¨ç›¸é—œ")
    
    print("\n3. FashionCLIP èªç¾©ç›¸ä¼¼åº¦:")
    print("   ç‰¹å¾µå‘é‡ç‰ˆæœ¬: calculate_fashionclip_feature_similarity()")
    print("   æ¨™ç±¤æ¯”è¼ƒç‰ˆæœ¬: calculate_fashionclip_label_similarity()")
    print("   ç¯„åœ: [0, 1], 1=å®Œå…¨ç›¸ä¼¼")
    
    print("\n4. çµ„åˆæå¤±å‡½æ•¸:")
    print("   å‡½æ•¸: UnifiedPerformanceMetrics.calculate_combined_loss()")
    print("   å…¬å¼: 0.2Ã—visual_loss + 0.6Ã—fashion_loss + 0.2Ã—color_loss")
    print("   ç¯„åœ: [0, 1], 0=å®Œå…¨ç›¸åŒ")
    
    print("\nâœ… ä½¿ç”¨å»ºè­°:")
    print("1. å°‡æ­¤é¡å°å…¥åˆ° analyze_results.py å’Œ day3_fashion_training.py")
    print("2. æ›¿æ›ç¾æœ‰çš„åˆ†æ•£å¯¦ç¾")
    print("3. ç¢ºä¿å…©å€‹è…³æœ¬èª¿ç”¨å®Œå…¨ç›¸åŒçš„å‡½æ•¸")
    print("4. çµ±ä¸€è™•ç†åœ–ç‰‡è¼¸å…¥æ ¼å¼ (è·¯å¾‘æˆ–æ•¸çµ„)")

def generate_implementation_guide():
    """ç”Ÿæˆå¯¦ç¾æŒ‡å—"""
    print("\n" + "=" * 60)
    print("ğŸ”§ ä¿®å¾©æŒ‡å— - å¦‚ä½•çµ±ä¸€ä¸‰å€‹æ€§èƒ½æŒ‡æ¨™")
    print("=" * 60)
    
    print("\næ­¥é©Ÿ 1: åœ¨ analyze_results.py ä¸­æ›¿æ›å‡½æ•¸")
    print("â”" * 40)
    print("æ›¿æ› calculate_image_similarity() ç‚º:")
    print("  similarity = UnifiedPerformanceMetrics.calculate_ssim_similarity(img1_path, img2_path)")
    
    print("\næ›¿æ› calculate_color_similarity() ç‚º:")
    print("  correlation = UnifiedPerformanceMetrics.calculate_color_similarity(img1_path, img2_path)")
    
    print("\nä¿ç•™ compare_fashion_features() æˆ–ä½¿ç”¨:")
    print("  similarity = UnifiedPerformanceMetrics.calculate_fashionclip_label_similarity(orig, gen)")
    
    print("\næ­¥é©Ÿ 2: åœ¨ day3_fashion_training.py ä¸­æ›¿æ›å‡½æ•¸")
    print("â”" * 40)
    print("åœ¨ calculate_image_similarity() ä¸­æ›¿æ›:")
    print("  similarities['visual_ssim'] = UnifiedPerformanceMetrics.calculate_ssim_similarity(gen_img, src_img)")
    print("  similarities['color_distribution'] = UnifiedPerformanceMetrics.calculate_color_similarity(gen_img, src_img)")
    print("  similarities['fashion_clip'] = UnifiedPerformanceMetrics.calculate_fashionclip_feature_similarity(...)")
    
    print("\næ­¥é©Ÿ 3: çµ±ä¸€çµ„åˆæå¤±è¨ˆç®—")
    print("â”" * 40)
    print("å…©å€‹è…³æœ¬éƒ½ä½¿ç”¨:")
    print("  loss_result = UnifiedPerformanceMetrics.calculate_combined_loss(visual_sim, fashion_sim, color_sim)")
    
    print("\næ­¥é©Ÿ 4: é©—è­‰ä¸€è‡´æ€§")
    print("â”" * 40)
    print("1. ä½¿ç”¨ç›¸åŒæ¸¬è©¦åœ–ç‰‡å°æ¯”å…©å€‹è…³æœ¬çš„è¼¸å‡º")
    print("2. ç¢ºä¿ä¸‰å€‹æŒ‡æ¨™å€¼å®Œå…¨ç›¸åŒ")
    print("3. ç¢ºä¿çµ„åˆæå¤±å€¼å®Œå…¨ç›¸åŒ")
    print("4. é‹è¡Œå®Œæ•´æ¸¬è©¦é©—è­‰")

if __name__ == "__main__":
    demonstrate_unified_implementation()
    generate_implementation_guide()
    
    print(f"\nğŸ¯ ç¸½çµ:")
    print(f"æ­¤çµ±ä¸€å¯¦ç¾ç¢ºä¿äº†ä¸‰å€‹æ€§èƒ½æŒ‡æ¨™åœ¨æ‰€æœ‰è…³æœ¬ä¸­ä½¿ç”¨å®Œå…¨ç›¸åŒçš„:")
    print(f"âœ… ç®—æ³• (SSIM, è‰²å½©ç›´æ–¹åœ–, FashionCLIP)")
    print(f"âœ… é è™•ç†æ­¥é©Ÿ (å°ºå¯¸å°é½Š, è‰²å½©ç©ºé–“è½‰æ›)")
    print(f"âœ… æ¬Šé‡é…ç½® (0.2:0.6:0.2)")
    print(f"âœ… æå¤±è½‰æ›å…¬å¼ (1.0 - similarity)")
    print(f"\nè«‹æŒ‰ç…§ä¿®å¾©æŒ‡å—æ›´æ–°ä»£ç¢¼ä»¥ç¢ºä¿å®Œå…¨ä¸€è‡´æ€§!")
