#!/usr/bin/env python3
"""
LoRA è¨“ç·´è…³æœ¬ - åŸºæ–¼ train_lora.py æ·»åŠ ç›£æ§åŠŸèƒ½
èˆ‡ train_lora.py ä¿æŒç›¸åŒçš„æ ¸å¿ƒé‚è¼¯ï¼Œåªæ·»åŠ åŸºæœ¬çš„æ—¥èªŒç›£æ§
"""

import subprocess
import os
import sys
import warnings
import argparse
import datetime
import logging
import json
import shutil
from typing import Tuple, Dict, Optional
from PIL import Image

# æ¸›å°‘è­¦å‘Šè¨Šæ¯
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message=".*xformers.*")
warnings.filterwarnings("ignore", message=".*triton.*")

# æ™ºæ…§è·¯å¾‘æª¢æ¸¬ - æ”¯æ´å¾ä»»ä½•ç›®éŒ„åŸ·è¡Œ
def setup_working_directory():
    """è¨­å®šå·¥ä½œç›®éŒ„ï¼Œæ”¯æ´å¾æ ¹ç›®éŒ„æˆ–auto_test_pipelineç›®éŒ„åŸ·è¡Œ"""
    current_dir = os.getcwd()
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æª¢æŸ¥ç•¶å‰ç›®éŒ„çµæ§‹
    if os.path.basename(current_dir) == "stable-diffusion-webui":
        # å¾æ ¹ç›®éŒ„åŸ·è¡Œ
        print(f"ğŸ“ å¾æ ¹ç›®éŒ„åŸ·è¡Œ: {current_dir}")
        return "root"
    elif os.path.basename(current_dir) == "auto_test_pipeline":
        # å¾auto_test_pipelineç›®éŒ„åŸ·è¡Œ
        print(f"ğŸ“ å¾auto_test_pipelineç›®éŒ„åŸ·è¡Œ: {current_dir}")
        return "auto_test_pipeline"
    else:
        # åˆ‡æ›åˆ°è…³æœ¬æ‰€åœ¨ç›®éŒ„
        os.chdir(script_dir)
        print(f"ğŸ“ åˆ‡æ›åˆ°è…³æœ¬ç›®éŒ„: {script_dir}")
        return "auto_test_pipeline"

# è¨­å®šå·¥ä½œç›®éŒ„ä¸¦ç²å–åŸ·è¡Œæ¨¡å¼
execution_mode = setup_working_directory()

def get_path_config(execution_mode: str) -> Dict[str, str]:
    """æ ¹æ“šåŸ·è¡Œæ¨¡å¼ç²å–è·¯å¾‘é…ç½®"""
    if execution_mode == "root":
        # å¾stable-diffusion-webuiæ ¹ç›®éŒ„åŸ·è¡Œ
        return {
            "train_data_dir": "auto_test_pipeline/lora_train_set",
            "output_dir": "auto_test_pipeline/lora_output", 
            "pretrained_model_path": "models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
            "train_network_script": "auto_test_pipeline/train_network.py",
            "log_dir": "auto_test_pipeline/training_logs"
        }
    else:
        # å¾auto_test_pipelineç›®éŒ„åŸ·è¡Œ
        return {
            "train_data_dir": "lora_train_set",
            "output_dir": "lora_output",
            "pretrained_model_path": "../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors", 
            "train_network_script": "train_network.py",
            "log_dir": "training_logs"
        }

# ç²å–è·¯å¾‘é…ç½®
path_config = get_path_config(execution_mode)
print(f"ğŸ“‹ è·¯å¾‘é…ç½®:")
for key, value in path_config.items():
    print(f"  {key}: {value}")
print("=" * 60)
# èˆ‡ train_lora.py ä¿æŒä¸€è‡´çš„å›ºå®šåƒæ•¸
FIXED_TRAINING_PARAMS = {
    "max_train_steps": 100,         # å›ºå®š 100 æ­¥
    "learning_rate": 5e-5,          # å›ºå®šå­¸ç¿’ç‡
    "network_dim": 32,              # ç¶²è·¯ç¶­åº¦
    "save_every_n_epochs": 50,      # å„²å­˜é »ç‡
}

print("ğŸ“‹ å›ºå®šåƒæ•¸è¨­å®š:")
for key, value in FIXED_TRAINING_PARAMS.items():
    print(f"  {key}: {value}")
print("=" * 60)
# ==================== åƒæ•¸è¨­å®šçµæŸ ====================

def setup_logging():
    """è¨­å®šæ—¥èªŒç³»çµ± - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
    log_dir = path_config["log_dir"]
    os.makedirs(log_dir, exist_ok=True)
    
    log_file = os.path.join(log_dir, f"lora_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ“‹ æ—¥èªŒæª”æ¡ˆ: {log_file}")
    return logger

def find_latest_lora():
    """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
    lora_path = path_config["output_dir"]
    if not os.path.exists(lora_path):
        return None
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    if not lora_files:
        return None
    
    # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
    return os.path.join(lora_path, latest_lora)

def backup_existing_lora():
    """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
    existing_lora = find_latest_lora()
    if existing_lora and os.path.exists(existing_lora):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"lora_backup_{timestamp}.safetensors"
        backup_path = os.path.join(path_config["output_dir"], backup_name)
        
        import shutil
        shutil.copy2(existing_lora, backup_path)
        print(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
        return backup_path
    return None

def check_image_size(data_folder, target_size=512):
    """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
    print(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
    
    files = os.listdir(data_folder)
    img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
    
    valid_count = 0
    invalid_files = []
    
    for img_file in img_files:
        img_path = os.path.join(data_folder, img_file)
        
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                
                # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                if width <= target_size and height <= target_size:
                    valid_count += 1
                    print(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                else:
                    invalid_files.append((img_file, width, height))
                    print(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                
        except Exception as e:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
            invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
    
    print(f"\nğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
    print(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
    print(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
    
    if invalid_files:
        print(f"\nğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
        for img_file, width, height in invalid_files:
            print(f"   - {img_file}: {width}x{height}")
        print(f"\nğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
    
    if valid_count == 0:
        print(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        return False
    else:
        print(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
        return True

def train_lora_with_monitoring(continue_from_checkpoint=False, logger=None):
    """åŸ·è¡Œ LoRA è¨“ç·´ - åŸºæ–¼åŸå§‹ train_lora.pyï¼Œæ·»åŠ ç›£æ§ï¼Œæ”¯æ´å‹•æ…‹è·¯å¾‘"""
    
    if logger:
        logger.info("ğŸ¯ é–‹å§‹ LoRA è¨“ç·´æµç¨‹ï¼ˆå«ç›£æ§ï¼‰")
        logger.info(f"ğŸƒ åŸ·è¡Œæ¨¡å¼: {execution_mode}")
    
    # ä½¿ç”¨å‹•æ…‹è·¯å¾‘é…ç½®
    train_dir = path_config["train_data_dir"]
    sub_folder = "10_test" 
    data_folder = os.path.join(train_dir, sub_folder)

    # è‡ªå‹•è™•ç†è³‡æ–™å¤¾æ”¹å - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    old_folder = os.path.join(train_dir, "test_10")
    if os.path.exists(old_folder) and not os.path.exists(data_folder):
        os.rename(old_folder, data_folder)
        print(f"å·²è‡ªå‹•å°‡ {old_folder} æ”¹åç‚º {data_folder}")
        if logger:
            logger.info(f"å·²è‡ªå‹•å°‡ {old_folder} æ”¹åç‚º {data_folder}")

    # è©³ç´°æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹ - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    print(f"ğŸ” æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹...")
    if logger:
        logger.info("ğŸ” æª¢æŸ¥è³‡æ–™å¤¾çµæ§‹...")
        
    if not os.path.isdir(train_dir):
        print(f"âŒ æ‰¾ä¸åˆ°çˆ¶è³‡æ–™å¤¾ï¼š{train_dir}")
        if logger:
            logger.error(f"âŒ æ‰¾ä¸åˆ°çˆ¶è³‡æ–™å¤¾ï¼š{train_dir}")
        sys.exit(1)

    if not os.path.isdir(data_folder):
        print(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™å¤¾ï¼š{data_folder}")
        if logger:
            logger.error(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è³‡æ–™å¤¾ï¼š{data_folder}")
        print(f"ğŸ“ {train_dir} å…§å®¹ï¼š")
        if os.path.exists(train_dir):
            for item in os.listdir(train_dir):
                print(f"  {item}")
        sys.exit(1)

    # æª¢æŸ¥åœ–ç‰‡å’Œæ–‡å­—æª”æ¡ˆ - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    files = os.listdir(data_folder)
    jpg_files = [f for f in files if f.lower().endswith('.jpg')]
    txt_files = [f for f in files if f.lower().endswith('.txt')]

    print(f"ğŸ“‚ {data_folder} å…§å®¹ï¼š")
    print(f"  åœ–ç‰‡æª”æ¡ˆæ•¸é‡ï¼š{len(jpg_files)}")
    print(f"  æ–‡å­—æª”æ¡ˆæ•¸é‡ï¼š{len(txt_files)}")
    
    if logger:
        logger.info(f"ğŸ“‚ {data_folder} å…§å®¹ï¼šåœ–ç‰‡ {len(jpg_files)} å¼µï¼Œæ–‡å­— {len(txt_files)} å€‹")

    if len(jpg_files) == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .jpg æª”æ¡ˆï¼")
        if logger:
            logger.error("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• .jpg æª”æ¡ˆï¼")
        sys.exit(1)

    # ğŸ¯ é—œéµæ­¥é©Ÿï¼šæª¢æŸ¥åœ–ç‰‡å¤§å° - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    if not check_image_size(data_folder, target_size=512):
        print("âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        print("ğŸ’¡ è«‹ä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        if logger:
            logger.error("âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
        sys.exit(1)

    # è‡ªå‹•å°‡ .JPG å‰¯æª”åæ”¹æˆ .jpg - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    for fname in files:
        if fname.lower().endswith('.jpg') and not fname.endswith('.jpg'):
            src = os.path.join(data_folder, fname)
            dst = os.path.join(data_folder, fname[:-4] + '.jpg')
            os.rename(src, dst)
            print(f"å·²è‡ªå‹•å°‡ {src} æ”¹åç‚º {dst}")
            if logger:
                logger.info(f"å·²è‡ªå‹•å°‡ {fname} æ”¹å")

    # è™•ç†ç¹¼çºŒè¨“ç·´é¸é … - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    resume_from = None
    if continue_from_checkpoint:
        existing_lora = find_latest_lora()
        if existing_lora:
            print(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
            if logger:
                logger.info(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
            resume_from = existing_lora
            # å‚™ä»½ç¾æœ‰æ¨¡å‹
            backup_existing_lora()
        else:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            if logger:
                logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
    else:
        print("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        if logger:
            logger.info("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
        # å¦‚æœå­˜åœ¨èˆŠæ¨¡å‹ï¼Œå‚™ä»½å®ƒ
        backup_existing_lora()

    # åŸºæœ¬è¨“ç·´æŒ‡ä»¤ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘é…ç½®
    cmd_parts = [
        f"python {path_config['train_network_script']}",
        f"--pretrained_model_name_or_path={path_config['pretrained_model_path']}",
        f"--train_data_dir={path_config['train_data_dir']}",
        f"--output_dir={path_config['output_dir']}",
        "--resolution=512,512",
        "--network_module=networks.lora",
        f"--network_dim={FIXED_TRAINING_PARAMS['network_dim']}",        # ä½¿ç”¨å›ºå®šåƒæ•¸
        "--train_batch_size=1",
        f"--max_train_steps={FIXED_TRAINING_PARAMS['max_train_steps']}",   # ä½¿ç”¨å›ºå®šåƒæ•¸
        "--mixed_precision=fp16",
        "--cache_latents",
        f"--learning_rate={FIXED_TRAINING_PARAMS['learning_rate']}",    # ä½¿ç”¨å›ºå®šåƒæ•¸
        f"--save_every_n_epochs={FIXED_TRAINING_PARAMS['save_every_n_epochs']}",
        "--save_model_as=safetensors"
    ]
    
    # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸ - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
    if resume_from:
        cmd_parts.extend([
            f"--resume={resume_from}",
            "--save_state"  # ä¿å­˜è¨“ç·´ç‹€æ…‹
        ])
    
    cmd = " ".join(cmd_parts)
    
    print("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
    print(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    if logger:
        logger.info("ğŸš€ é–‹å§‹ LoRA å¾®èª¿ ...")
        logger.info(f"ğŸ“‹ è¨“ç·´å‘½ä»¤: {cmd}")
    
    # è¨˜éŒ„è¨“ç·´é–‹å§‹æ™‚é–“
    start_time = datetime.datetime.now()
    if logger:
        logger.info(f"â° è¨“ç·´é–‹å§‹æ™‚é–“: {start_time}")
    
    # åŸ·è¡Œè¨“ç·´ - èˆ‡åŸå§‹ä»£ç¢¼å®Œå…¨ä¸€è‡´ï¼ˆä¸ä½¿ç”¨ capture_outputï¼‰
    result = subprocess.run(cmd, shell=True)
    
    # è¨˜éŒ„è¨“ç·´çµæŸæ™‚é–“
    end_time = datetime.datetime.now()
    duration = end_time - start_time
    if logger:
        logger.info(f"â° è¨“ç·´çµæŸæ™‚é–“: {end_time}")
        logger.info(f"â±ï¸  è¨“ç·´æŒçºŒæ™‚é–“: {duration}")
    
    if result.returncode == 0:
        print("âœ… LoRA è¨“ç·´å®Œæˆ")
        if logger:
            logger.info("âœ… LoRA è¨“ç·´å®Œæˆ")
        
        # é¡¯ç¤ºè¨“ç·´çµæœ - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
        final_lora = find_latest_lora()
        if final_lora:
            file_size = os.path.getsize(final_lora) / (1024*1024)
            print(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
            print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
            if logger:
                logger.info(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
                logger.info(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
            
            if continue_from_checkpoint:
                print("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
                if logger:
                    logger.info("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
            else:
                print("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
                if logger:
                    logger.info("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        # ä¿å­˜è¨“ç·´çµæœå ±å‘Š - æ–°å¢çš„ç›£æ§åŠŸèƒ½
        if logger:
            training_report = {
                "success": True,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration.total_seconds(),
                "final_model": os.path.basename(final_lora) if final_lora else None,
                "model_size_mb": file_size if final_lora else 0,
                "continue_from_checkpoint": continue_from_checkpoint,
                "training_params": FIXED_TRAINING_PARAMS
            }
            
            report_file = f"training_result_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(training_report, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“‹ è¨“ç·´å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return True
    else:
        print("âŒ LoRA è¨“ç·´å¤±æ•—")
        if logger:
            logger.error(f"âŒ LoRA è¨“ç·´å¤±æ•—ï¼Œè¿”å›ç¢¼: {result.returncode}")
        return False

def main():
    """ä¸»å‡½æ•¸ - åŸºæ–¼åŸå§‹ train_lora.py çš„é‚è¼¯"""
    
    # è¨­å®šæ—¥èªŒç³»çµ± - æ–°å¢çš„ç›£æ§åŠŸèƒ½
    logger = setup_logging()
    logger.info("ğŸ¯ é–‹å§‹ LoRA è¨“ç·´è…³æœ¬ï¼ˆç›£æ§ç‰ˆæœ¬ï¼‰")
    
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬ - å«ç›£æ§åŠŸèƒ½")
    parser.add_argument("--continue", "-c", action="store_true", 
                       dest="continue_training",
                       help="å¾ç¾æœ‰çš„ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´")
    parser.add_argument("--new", "-n", action="store_true",
                       dest="new_training", 
                       help="é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
    
    args = parser.parse_args()
    
    # æ±ºå®šè¨“ç·´æ¨¡å¼ - èˆ‡åŸå§‹ä»£ç¢¼å®Œå…¨ä¸€è‡´
    if args.continue_training and args.new_training:
        print("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        logger.error("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        sys.exit(1)
    elif args.continue_training:
        print("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        logger.info("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        continue_from_checkpoint = True
    elif args.new_training:
        print("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        logger.info("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        continue_from_checkpoint = False
    else:
        # å¦‚æœæ²’æœ‰æŒ‡å®šåƒæ•¸ï¼Œè©¢å•ç”¨æˆ¶ - èˆ‡åŸå§‹ä»£ç¢¼ä¸€è‡´
        existing_lora = find_latest_lora()
        if existing_lora:
            print(f"ğŸ” ç™¼ç¾ç¾æœ‰çš„ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
            logger.info(f"ğŸ” ç™¼ç¾ç¾æœ‰çš„ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
            print("è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ï¼š")
            print("1. å¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´ (ç´¯ç©èª¿æ•™)")
            print("2. é–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´ (é‡æ–°é–‹å§‹)")
            
            while True:
                choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
                if choice == "1":
                    continue_from_checkpoint = True
                    logger.info("ç”¨æˆ¶é¸æ“‡ï¼šå¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´")
                    break
                elif choice == "2":
                    continue_from_checkpoint = False
                    logger.info("ç”¨æˆ¶é¸æ“‡ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
                    break
                else:
                    print("è«‹è¼¸å…¥ 1 æˆ– 2")
        else:
            print("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            logger.info("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            continue_from_checkpoint = False
    
    # åŸ·è¡Œè¨“ç·´ - èª¿ç”¨ç›£æ§ç‰ˆæœ¬çš„å‡½æ•¸
    success = train_lora_with_monitoring(continue_from_checkpoint, logger)
    
    if success:
        logger.info("âœ… è¨“ç·´è…³æœ¬åŸ·è¡ŒæˆåŠŸ")
    else:
        logger.error("âŒ è¨“ç·´è…³æœ¬åŸ·è¡Œå¤±æ•—")
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

class LoRATrainer:
    """LoRA è¨“ç·´å™¨ - æ•´åˆç›£æ§åŠŸèƒ½"""
    
    def __init__(self, continue_from_checkpoint: bool = False, custom_params: dict = None):
        """
        åˆå§‹åŒ– LoRA è¨“ç·´å™¨
        
        Args:
            continue_from_checkpoint: æ˜¯å¦å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´
            custom_params: è‡ªå®šç¾©è¨“ç·´åƒæ•¸å­—å…¸
        """
        # åˆ‡æ›åˆ°è…³æœ¬æ‰€åœ¨ç›®éŒ„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        if os.getcwd() != script_dir:
            os.chdir(script_dir)
            print(f"ğŸ”„ åˆ‡æ›å·¥ä½œç›®éŒ„åˆ°: {script_dir}")
        
        self.training_dir = os.path.dirname(os.path.abspath(__file__))
        self.monitor = None
        self.continue_from_checkpoint = continue_from_checkpoint
        self.custom_params = custom_params or {}
        
        # è¨­å®šæ—¥èªŒ
        self.setup_logging()
        
        # å¦‚æœæœ‰è‡ªå®šç¾©åƒæ•¸ï¼Œè¨˜éŒ„åˆ°æ—¥èªŒ
        if self.custom_params:
            self.logger.info(f"ğŸ”§ ä½¿ç”¨è‡ªå®šç¾©åƒæ•¸: {self.custom_params}")
    
    def setup_logging(self):
        """è¨­å®šæ—¥èªŒç³»çµ± - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
        log_dir = path_config["log_dir"]
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f"lora_training_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def find_latest_lora(self):
        """æ‰¾åˆ°æœ€æ–°çš„ LoRA æ¨¡å‹æª”æ¡ˆ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
        lora_path = path_config["output_dir"]
        if not os.path.exists(lora_path):
            return None
        
        lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
        if not lora_files:
            return None
        
        # æ‰¾æœ€æ–°çš„æª”æ¡ˆ
        latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
        return os.path.join(lora_path, latest_lora)

    def backup_existing_lora(self):
        """å‚™ä»½ç¾æœ‰çš„ LoRA æ¨¡å‹ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
        existing_lora = self.find_latest_lora()
        if existing_lora and os.path.exists(existing_lora):
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"lora_backup_{timestamp}.safetensors"
            backup_path = os.path.join(path_config["output_dir"], backup_name)
            
            shutil.copy2(existing_lora, backup_path)
            self.logger.info(f"ğŸ“¦ å‚™ä»½ç¾æœ‰æ¨¡å‹: {backup_name}")
            return backup_path
        return None
        
    def check_image_sizes(self, data_folder: str, target_size: int = 512) -> bool:
        """æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼Œè·³éè¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡"""
        self.logger.info(f"ğŸ” æª¢æŸ¥åœ–ç‰‡å¤§å°æ˜¯å¦ç¬¦åˆ {target_size}x{target_size} è¦æ±‚...")
        
        files = os.listdir(data_folder)
        img_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
        valid_count = 0
        invalid_files = []
        
        for img_file in img_files:
            img_path = os.path.join(data_folder, img_file)
            
            try:
                with Image.open(img_path) as img:
                    width, height = img.size
                    
                    # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸æ˜¯å¦ç¬¦åˆè¦æ±‚
                    if width <= target_size and height <= target_size:
                        valid_count += 1
                        self.logger.info(f"  âœ… {img_file}: {width}x{height} (ç¬¦åˆè¦æ±‚)")
                    else:
                        invalid_files.append((img_file, width, height))
                        self.logger.warning(f"  âš ï¸  {img_file}: {width}x{height} (è¶…å‡º {target_size}x{target_size}ï¼Œå°‡è·³é)")
                    
            except Exception as e:
                self.logger.error(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡ {img_file}: {str(e)}")
                invalid_files.append((img_file, "è®€å–å¤±æ•—", ""))
        
        self.logger.info(f"ğŸ“Š åœ–ç‰‡å°ºå¯¸æª¢æŸ¥çµæœï¼š")
        self.logger.info(f"âœ… ç¬¦åˆè¦æ±‚çš„åœ–ç‰‡ï¼š{valid_count} å¼µ")
        self.logger.info(f"âš ï¸  è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡ï¼š{len(invalid_files)} å¼µ")
        
        if invalid_files:
            self.logger.warning(f"ğŸ“‹ è¶…å‡ºå°ºå¯¸çš„åœ–ç‰‡å°‡è¢«è·³éï¼š")
            for img_file, width, height in invalid_files:
                self.logger.warning(f"   - {img_file}: {width}x{height}")
            self.logger.info(f"ğŸ’¡ å»ºè­°ï¼šä½¿ç”¨ generate_caption_fashionclip.py é è™•ç†åœ–ç‰‡")
        
        if valid_count == 0:
            self.logger.error(f"âŒ æ²’æœ‰ä»»ä½•åœ–ç‰‡ç¬¦åˆè¦æ±‚ï¼")
            return False
        else:
            self.logger.info(f"ğŸ¯ å°‡ä½¿ç”¨ {valid_count} å¼µç¬¦åˆè¦æ±‚çš„åœ–ç‰‡é€²è¡Œè¨“ç·´")
            return True

    def check_training_requirements(self) -> bool:
        """æª¢æŸ¥è¨“ç·´éœ€æ±‚ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
        self.logger.info("ğŸ” æª¢æŸ¥è¨“ç·´éœ€æ±‚...")
        self.logger.info(f"ğŸƒ åŸ·è¡Œæ¨¡å¼: {execution_mode}")
        
        # æª¢æŸ¥è¨“ç·´æ•¸æ“š
        train_data_dir = os.path.join(path_config["train_data_dir"], "10_test")
        if not os.path.exists(train_data_dir):
            self.logger.error(f"âŒ è¨“ç·´æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {train_data_dir}")
            self.logger.info(f"ğŸ“ ç•¶å‰å·¥ä½œç›®éŒ„: {os.getcwd()}")
            return False
            
        train_images = [f for f in os.listdir(train_data_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        if len(train_images) == 0:
            self.logger.error("âŒ æ²’æœ‰æ‰¾åˆ°è¨“ç·´åœ–ç‰‡")
            return False
            
        self.logger.info(f"âœ… æ‰¾åˆ° {len(train_images)} å¼µè¨“ç·´åœ–ç‰‡")
        
        # æª¢æŸ¥åœ–ç‰‡å°ºå¯¸
        if not self.check_image_sizes(train_data_dir):
            return False
        
        # æª¢æŸ¥è¼¸å‡ºç›®éŒ„
        output_dir = path_config["output_dir"]
        os.makedirs(output_dir, exist_ok=True)
        self.logger.info(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
        
        return True
        
    def calculate_training_timeout(self) -> int:
        """æ ¹æ“šå¯¦éš›æ¸¬è©¦çµæœå‹•æ…‹è¨ˆç®—è¶…æ™‚æ™‚é–“
        
        å¯¦éš›æ¸¬è©¦åŸºæº–ï¼š10å¼µåœ–ç‰‡ + 200æ­¥ = 30åˆ†é˜
        """
        train_data_dir = "lora_train_set/10_test"
        
        if not os.path.exists(train_data_dir):
            return 1800  # é»˜èª30åˆ†é˜
        
        # è¨ˆç®—åœ–ç‰‡æ•¸é‡
        train_images = [f for f in os.listdir(train_data_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]
        image_count = len(train_images)
        
        # ç²å–ç•¶å‰è¨“ç·´æ­¥æ•¸ï¼ˆä½¿ç”¨å›ºå®šåƒæ•¸ï¼‰
        current_steps = FIXED_TRAINING_PARAMS['max_train_steps']
        
        # å¯¦éš›æ¸¬è©¦åŸºæº–ï¼š10å¼µåœ–ç‰‡ + 200æ­¥ = 30åˆ†é˜
        base_images = 10
        base_steps = 200
        base_time_minutes = 30
        
        # è¨ˆç®—æ¯å¼µåœ–ç‰‡æ¯æ­¥çš„æ™‚é–“
        time_per_image_per_step = base_time_minutes / (base_images * base_steps)
        
        # è¨ˆç®—åŸºç¤æ™‚é–“
        base_time_minutes_calc = image_count * current_steps * time_per_image_per_step
        
        # åŠ ä¸Š50%ç·©è¡æ™‚é–“
        estimated_time_minutes = base_time_minutes_calc * 1.5
        
        # è½‰æ›ç‚ºç§’
        estimated_time_seconds = int(estimated_time_minutes * 60)
        
        # è¨­å®šæœ€å°å’Œæœ€å¤§è¶…æ™‚æ™‚é–“
        min_timeout = 1800  # æœ€å°‘30åˆ†é˜
        max_timeout = 14400  # æœ€å¤š4å°æ™‚
        
        timeout = max(min_timeout, min(estimated_time_seconds, max_timeout))
        
        self.logger.info(f"ğŸ“Š è¨“ç·´æ™‚é–“åˆ†æ (åŸºæ–¼å¯¦éš›æ¸¬è©¦: 10å¼µåœ–ç‰‡+200æ­¥=30åˆ†é˜):")
        self.logger.info(f"   åœ–ç‰‡æ•¸é‡: {image_count} å¼µ")
        self.logger.info(f"   è¨“ç·´æ­¥æ•¸: {current_steps} æ­¥")
        self.logger.info(f"   åŸºç¤æ™‚é–“: {base_time_minutes_calc:.1f} åˆ†é˜")
        self.logger.info(f"   ç·©è¡æ™‚é–“: {estimated_time_minutes:.1f} åˆ†é˜")
        self.logger.info(f"   è¶…æ™‚è¨­å®š: {timeout/60:.1f} åˆ†é˜")
        
        if estimated_time_seconds > max_timeout:
            self.logger.warning(f"âš ï¸  é ä¼°æ™‚é–“ ({estimated_time_minutes:.1f}åˆ†é˜) è¶…éè¶…æ™‚é™åˆ¶ ({max_timeout/60:.1f}åˆ†é˜)")
            self.logger.warning(f"ğŸ”§ å»ºè­°æ¸›å°‘è¨“ç·´æ­¥æ•¸æˆ–åˆ†æ‰¹è¨“ç·´")
            
            # è¨ˆç®—å»ºè­°çš„æ‰¹æ¬¡å¤§å°
            max_time_per_batch = (max_timeout / 60) / 1.5  # å»æ‰ç·©è¡æ™‚é–“
            max_images_per_batch = int(max_time_per_batch / (current_steps * time_per_image_per_step))
            self.logger.info(f"ğŸ’¡ å»ºè­°æ¯æ‰¹æœ€å¤š {max_images_per_batch} å¼µåœ–ç‰‡")
        
        return timeout
        
    def get_training_params(self) -> dict:
        """ç²å–è¨“ç·´åƒæ•¸ï¼Œæ”¯æ´è‡ªå®šç¾©è¦†è“‹"""
        base_params = {
            "pretrained_model_name_or_path": "runwayml/stable-diffusion-v1-5",
            "output_dir": "lora_output",
            "train_data_dir": "lora_train_set",
            "resolution": 512,
            "train_batch_size": 1,
            "gradient_accumulation_steps": 1,
            "dataloader_num_workers": 0,
            "num_train_epochs": 1,
            "max_train_steps": 100,  # é»˜èª100æ­¥ï¼Œé©åˆ100å¼µåœ–ç‰‡
            "learning_rate": 5e-5,  # èª¿æ•´ç‚ºé©åˆå¤§æ•¸æ“šé›†çš„å­¸ç¿’ç‡
            "scale_lr": False,
            "lr_scheduler": "cosine",
            "lr_warmup_steps": 0,
            "snr_gamma": 5.0,
            "use_8bit_adam": True,
            "mixed_precision": "fp16",
            "save_precision": "fp16",
            "enable_xformers_memory_efficient_attention": True,
            "cache_latents": True,
            "save_model_as": "safetensors",
            "network_module": "networks.lora",
            "network_dim": 32,
            "network_alpha": 32,
            "network_train_unet_only": True,
            "network_train_text_encoder_only": False,
            "save_every_n_epochs": 1
        }
        
        # ä½¿ç”¨è‡ªå®šç¾©åƒæ•¸è¦†è“‹åŸºç¤åƒæ•¸
        if self.custom_params:
            base_params.update(self.custom_params)
            self.logger.info(f"âœ… åƒæ•¸å·²æ›´æ–°: {list(self.custom_params.keys())}")
        
        return base_params
    
    def build_training_command(self) -> str:
        """å»ºç«‹è¨“ç·´å‘½ä»¤"""
        
        # ç²å–è¨“ç·´åƒæ•¸
        params = self.get_training_params()
        
        # è™•ç†ç¹¼çºŒè¨“ç·´é¸é …
        resume_from = None
        if self.continue_from_checkpoint:
            existing_lora = self.find_latest_lora()
            if existing_lora:
                self.logger.info(f"ğŸ”„ å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´: {os.path.basename(existing_lora)}")
                resume_from = existing_lora
                # å‚™ä»½ç¾æœ‰æ¨¡å‹
                self.backup_existing_lora()
            else:
                self.logger.warning("âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¾æœ‰çš„ LoRA æª”æ¡ˆï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
        else:
            self.logger.info("ğŸ†• é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
            # å¦‚æœå­˜åœ¨èˆŠæ¨¡å‹ï¼Œå‚™ä»½å®ƒ
            self.backup_existing_lora()
        
        # åŸºæœ¬è¨“ç·´å‘½ä»¤éƒ¨åˆ†ï¼ˆä½¿ç”¨å›ºå®šåƒæ•¸ï¼‰
        cmd_parts = [
            "python train_network.py",
            f"--pretrained_model_name_or_path={params.get('pretrained_model_name_or_path', 'runwayml/stable-diffusion-v1-5')}",
            f"--train_data_dir={params.get('train_data_dir', 'lora_train_set')}",
            f"--output_dir={params.get('output_dir', 'lora_output')}",
            f"--resolution={FIXED_TRAINING_PARAMS['resolution']}",
            f"--train_batch_size={FIXED_TRAINING_PARAMS['train_batch_size']}",
            f"--max_train_steps={FIXED_TRAINING_PARAMS['max_train_steps']}",
            f"--learning_rate={FIXED_TRAINING_PARAMS['learning_rate']}",
            f"--mixed_precision={FIXED_TRAINING_PARAMS['mixed_precision']}",
            f"--save_model_as={FIXED_TRAINING_PARAMS['save_model_as']}",
            f"--network_module={params.get('network_module', 'networks.lora')}",
            f"--network_dim={FIXED_TRAINING_PARAMS['network_dim']}",
            f"--network_alpha={FIXED_TRAINING_PARAMS['network_alpha']}",
            f"--sample_every_n_steps={FIXED_TRAINING_PARAMS['sample_every_n_steps']}",
            f"--sample_sampler={FIXED_TRAINING_PARAMS['sample_sampler']}",
            f"--save_every_n_epochs={FIXED_TRAINING_PARAMS['save_every_n_epochs']}",
        ]
        
        # æ·»åŠ å¸ƒçˆ¾åƒæ•¸
        if params.get('cache_latents', True):
            cmd_parts.append("--cache_latents")
        if params.get('use_8bit_adam', True):
            cmd_parts.append("--use_8bit_adam")
        if params.get('enable_xformers_memory_efficient_attention', True):
            cmd_parts.append("--enable_xformers_memory_efficient_attention")
        if params.get('network_train_unet_only', True):
            cmd_parts.append("--network_train_unet_only")
        
        # å¦‚æœå¾æª¢æŸ¥é»ç¹¼çºŒï¼Œæ·»åŠ ç›¸æ‡‰åƒæ•¸
        if resume_from:
            cmd_parts.extend([
                f"--resume={resume_from}",
                "--save_state"  # ä¿å­˜è¨“ç·´ç‹€æ…‹
            ])
        
        cmd = " ".join(cmd_parts)
        return cmd
        
    def run_training_with_monitoring(self) -> Tuple[bool, dict]:
        """åŸ·è¡Œè¨“ç·´ä¸¦ç›£æ§é€²åº¦"""
        training_command = self.build_training_command()
        self.logger.info(f"ğŸš€ é–‹å§‹ LoRA è¨“ç·´...")
        self.logger.info(f"ğŸ“‹ å‘½ä»¤: {training_command}")
        
        # å¼·åˆ¶ä½¿ç”¨åŸºæœ¬æ¨¡å¼ï¼Œé¿å…ç›£æ§æ¨¡å¼å¡ä½
        self.logger.info("âš ï¸ ä½¿ç”¨åŸºæœ¬æ¨¡å¼åŸ·è¡Œè¨“ç·´ (é¿å…ç›£æ§æ¨¡å¼å¡ä½)")
        self.logger.info("ğŸ”„ å·²ç§»é™¤è¶…æ™‚é™åˆ¶ï¼Œè¨“ç·´å°‡è‡ªç„¶å®Œæˆ")
        try:
            # è¨­ç½®ç’°å¢ƒè®Šé‡ä»¥é¿å…ç·¨ç¢¼å•é¡Œ
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONUTF8'] = '1'
            
            # ç§»é™¤è¶…æ™‚é™åˆ¶ï¼Œè®“è¨“ç·´è‡ªç„¶å®Œæˆ - ä¿®å¾©è¼¸å‡ºé˜»å¡å•é¡Œ
            try:
                result = subprocess.run(
                    training_command, 
                    shell=True,
                    env=env
                )
                
            except Exception as e:
                self.logger.error(f"âŒ è¨“ç·´åŸ·è¡ŒéŒ¯èª¤: {e}")
                result = type('Result', (), {'returncode': -1, 'stderr': f'è¨“ç·´åŸ·è¡ŒéŒ¯èª¤: {e}'})()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æª”æ¡ˆç”Ÿæˆï¼Œå³ä½¿é€²ç¨‹è¿”å›éŒ¯èª¤ç¢¼
            model_generated = False
            model_size_ok = False
            output_dir = path_config["output_dir"]
            if os.path.exists(output_dir):
                lora_files = [f for f in os.listdir(output_dir) if f.endswith('.safetensors')]
                if lora_files:
                    model_generated = True
                    # æª¢æŸ¥æœ€æ–°æ¨¡å‹çš„å¤§å°æ˜¯å¦åˆç† (è‡³å°‘ 10MB)
                    latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(output_dir, x)))
                    latest_lora_path = os.path.join(output_dir, latest_lora)
                    file_size_mb = os.path.getsize(latest_lora_path) / (1024*1024)
                    model_size_ok = file_size_mb > 10  # è‡³å°‘ 10MB æ‰ç®—æ­£å¸¸
                    self.logger.info(f"âœ… æˆåŠŸç”Ÿæˆ LoRA æ¨¡å‹: {lora_files}")
            
            # å¦‚æœæ¨¡å‹å·²ç”Ÿæˆä¸”å¤§å°åˆç†ï¼Œèªç‚ºè¨“ç·´æˆåŠŸï¼ˆå¿½ç•¥è¿”å›ç¢¼ï¼‰
            success = model_generated and model_size_ok
            
            if success:
                self.logger.info("âœ… è¨“ç·´æˆåŠŸå®Œæˆ")
                
                # é¡¯ç¤ºè¨“ç·´çµæœ
                final_lora = self.find_latest_lora()
                if final_lora:
                    file_size = os.path.getsize(final_lora) / (1024*1024)
                    self.logger.info(f"ğŸ“ æœ€çµ‚æ¨¡å‹: {os.path.basename(final_lora)}")
                    self.logger.info(f"ğŸ“Š æª”æ¡ˆå¤§å°: {file_size:.2f} MB")
                    
                    if self.continue_from_checkpoint:
                        self.logger.info("ğŸ”„ æª¢æŸ¥é»è¨“ç·´å®Œæˆ - æ¨¡å‹å·²æ›´æ–°")
                    else:
                        self.logger.info("ğŸ†• æ–°æ¨¡å‹è¨“ç·´å®Œæˆ")
            else:
                if model_generated and not model_size_ok:
                    self.logger.warning("âš ï¸ æ¨¡å‹å·²ç”Ÿæˆä½†å¤§å°ç•°å¸¸ï¼Œå¯èƒ½è¨“ç·´ä¸å®Œæ•´")
                elif result.returncode != 0:
                    self.logger.error(f"âŒ è¨“ç·´å¤±æ•—ï¼Œè¿”å›ç¢¼: {result.returncode}")
                else:
                    self.logger.error("âŒ è¨“ç·´å¤±æ•—ï¼Œæœªç”Ÿæˆæ¨¡å‹æª”æ¡ˆ")
                
            return success, {}
            
        except Exception as e:
            self.logger.error(f"âŒ åŸ·è¡Œè¨“ç·´æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            # æª¢æŸ¥æ˜¯å¦ä»æœ‰æ¨¡å‹ç”Ÿæˆ
            output_dir = path_config["output_dir"]
            if os.path.exists(output_dir):
                lora_files = [f for f in os.listdir(output_dir) if f.endswith('.safetensors')]
                if lora_files:
                    self.logger.info(f"âœ… å„˜ç®¡æœ‰éŒ¯èª¤ï¼Œä½†æˆåŠŸç”Ÿæˆ LoRA æ¨¡å‹: {lora_files}")
                    return True, {}
            return False, {}
                
    def evaluate_training_success(self, report: dict) -> dict:
        """è©•ä¼°è¨“ç·´æˆåŠŸç¨‹åº¦"""
        evaluation = {
            "should_continue_inference": False,
            "training_quality": "unknown",
            "recommendations": [],
            "decision_reason": ""
        }
        
        if not report:
            # æ²’æœ‰è©³ç´°å ±å‘Šï¼Œæª¢æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æª”æ¡ˆ
            output_dir = path_config["output_dir"]
            if os.path.exists(output_dir):
                lora_files = [f for f in os.listdir(output_dir) if f.endswith('.safetensors')]
                if lora_files:
                    evaluation["should_continue_inference"] = True
                    evaluation["training_quality"] = "basic"
                    evaluation["decision_reason"] = "æ‰¾åˆ° LoRA æ¨¡å‹æª”æ¡ˆï¼Œå»ºè­°é€²è¡ŒåŸºæœ¬æ¸¬è©¦"
                else:
                    evaluation["decision_reason"] = "æ²’æœ‰æ‰¾åˆ° LoRA æ¨¡å‹æª”æ¡ˆ"
            else:
                evaluation["decision_reason"] = "è¼¸å‡ºç›®éŒ„ä¸å­˜åœ¨"
            return evaluation
            
        # åŸºæ–¼ç›£æ§å ±å‘Šè©•ä¼°
        training_summary = report.get("training_summary", {})
        training_metrics = report.get("training_metrics", {})
        training_evaluation = report.get("training_evaluation", {})
        
        best_loss = training_summary.get("best_loss", float('inf'))
        loss_improvement = training_metrics.get("loss_improvement", 0)
        performance_grade = training_evaluation.get("performance_grade", "poor")
        
        # æ±ºç­–é‚è¼¯
        if performance_grade == "excellent":
            evaluation["should_continue_inference"] = True
            evaluation["training_quality"] = "excellent"
            evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾å„ªç§€ (æå¤±: {best_loss:.4f})"
        elif performance_grade == "good":
            evaluation["should_continue_inference"] = True
            evaluation["training_quality"] = "good"
            evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾è‰¯å¥½ (æå¤±æ”¹å–„: {loss_improvement:.4f})"
        elif performance_grade == "average" and loss_improvement >= 0.05:
            evaluation["should_continue_inference"] = True
            evaluation["training_quality"] = "average"
            evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾ä¸€èˆ¬ä½†æœ‰æ”¹å–„ (æ”¹å–„: {loss_improvement:.4f})"
        else:
            evaluation["should_continue_inference"] = False
            evaluation["training_quality"] = "poor"
            evaluation["decision_reason"] = f"è¨“ç·´è¡¨ç¾ä¸ä½³ (ç­‰ç´š: {performance_grade}, æ”¹å–„: {loss_improvement:.4f})"
            evaluation["recommendations"] = [
                "ğŸ”§ å»ºè­°èª¿æ•´å­¸ç¿’ç‡ (å˜—è©¦ 5e-5 æˆ– 2e-4)",
                "ğŸ“Š å»ºè­°å¢åŠ è¨“ç·´æ­¥æ•¸ (1500-2000æ­¥)",
                "ğŸ¯ æª¢æŸ¥è¨“ç·´æ•¸æ“šå“è³ª",
                "âš™ï¸ å˜—è©¦ä¸åŒçš„å„ªåŒ–å™¨ (AdamW æˆ– Lion)"
            ]
            
        return evaluation
        
    def train(self) -> dict:
        """åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹"""
        self.logger.info("ğŸ¯ é–‹å§‹ LoRA è¨“ç·´æµç¨‹")
        
        # æª¢æŸ¥è¨“ç·´éœ€æ±‚
        if not self.check_training_requirements():
            return {
                "success": False,
                "message": "è¨“ç·´éœ€æ±‚æª¢æŸ¥å¤±æ•—",
                "should_continue_inference": False
            }
            
        # åŸ·è¡Œè¨“ç·´
        training_success, training_report = self.run_training_with_monitoring()
        
        # è©•ä¼°è¨“ç·´çµæœ
        evaluation = self.evaluate_training_success(training_report)
        
        # å»ºç«‹å®Œæ•´å ±å‘Š
        result = {
            "success": training_success,
            "training_report": training_report,
            "evaluation": evaluation,
            "should_continue_inference": evaluation["should_continue_inference"],
            "training_quality": evaluation["training_quality"],
            "decision_reason": evaluation["decision_reason"],
            "recommendations": evaluation.get("recommendations", [])
        }
        
        # è¨˜éŒ„çµæœ
        if training_success:
            self.logger.info("âœ… è¨“ç·´æˆåŠŸå®Œæˆ")
            self.logger.info(f"ğŸ¯ è©•ä¼°çµæœ: {evaluation['training_quality'].upper()}")
            self.logger.info(f"ğŸ“Š æ±ºç­–: {'ç¹¼çºŒæ¨ç†' if evaluation['should_continue_inference'] else 'å»ºè­°é‡æ–°è¨“ç·´'}")
            self.logger.info(f"ğŸ’¡ åŸå› : {evaluation['decision_reason']}")
        else:
            self.logger.error("âŒ è¨“ç·´å¤±æ•—")
            
        # å¦‚æœæœ‰å»ºè­°ï¼Œè¼¸å‡ºå»ºè­°
        if evaluation.get("recommendations"):
            self.logger.info("ğŸ’¡ æ”¹å–„å»ºè­°:")
            for rec in evaluation["recommendations"]:
                self.logger.info(f"   {rec}")
                
        # ä¿å­˜çµæœåˆ°æª”æ¡ˆ
        result_file = f"training_result_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
            
        self.logger.info(f"ğŸ“‹ çµæœå·²ä¿å­˜: {result_file}")
        
        return result

def find_existing_lora_models():
    """æŸ¥æ‰¾ç¾æœ‰çš„ LoRA æ¨¡å‹ - ä½¿ç”¨å‹•æ…‹è·¯å¾‘"""
    lora_path = path_config["output_dir"]
    if not os.path.exists(lora_path):
        return []
    
    lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
    return lora_files

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="LoRA è¨“ç·´è…³æœ¬ - æ•´åˆç›£æ§åŠŸèƒ½")
    parser.add_argument("--no-monitor", action="store_true", help="ç¦ç”¨è¨“ç·´ç›£æ§")
    parser.add_argument("--force-inference", action="store_true", help="å¼·åˆ¶ç¹¼çºŒæ¨ç†")
    parser.add_argument("--continue", "-c", action="store_true", 
                       dest="continue_training",
                       help="å¾ç¾æœ‰çš„ LoRA æª”æ¡ˆç¹¼çºŒè¨“ç·´")
    parser.add_argument("--new", "-n", action="store_true",
                       dest="new_training", 
                       help="é–‹å§‹æ–°çš„ç¨ç«‹ LoRA è¨“ç·´")
    parser.add_argument("--params", type=str, help="è‡ªå®šç¾©åƒæ•¸ JSON æ–‡ä»¶è·¯å¾‘")
    
    args = parser.parse_args()
    
    print("âš ï¸  æ‰€æœ‰è¨“ç·´åƒæ•¸å·²å›ºå®šåœ¨è…³æœ¬é ‚éƒ¨ï¼Œä¸æ¥å—å‘½ä»¤è¡Œåƒæ•¸è¦†è“‹")
    
    # è™•ç†è‡ªå®šç¾©åƒæ•¸ï¼ˆåƒ…é™éæ ¸å¿ƒåƒæ•¸ï¼‰
    custom_params = {}
    
    # å¾JSONæ–‡ä»¶è®€å–åƒæ•¸ï¼ˆåƒ…é™éæ ¸å¿ƒåƒæ•¸ï¼‰
    if args.params:
        if os.path.exists(args.params):
            with open(args.params, 'r', encoding='utf-8') as f:
                loaded_params = json.load(f)
            # åªå…è¨±éæ ¸å¿ƒåƒæ•¸
            allowed_params = ['pretrained_model_name_or_path', 'train_data_dir', 'output_dir', 'network_module']
            for key, value in loaded_params.items():
                if key in allowed_params:
                    custom_params[key] = value
                else:
                    print(f"âš ï¸  å¿½ç•¥è¢«ç¦æ­¢çš„åƒæ•¸: {key} (ä½¿ç”¨å›ºå®šå€¼)")
            print(f"ğŸ“„ å¾ {args.params} è¼‰å…¥å…è¨±çš„åƒæ•¸")
        else:
            print(f"âš ï¸  åƒæ•¸æ–‡ä»¶ä¸å­˜åœ¨: {args.params}")
    
    # å¦‚æœç¦ç”¨ç›£æ§ï¼Œç§»é™¤ç›£æ§åŠŸèƒ½
    global MONITOR_AVAILABLE
    if args.no_monitor:
        MONITOR_AVAILABLE = False
    
    # æ±ºå®šè¨“ç·´æ¨¡å¼
    continue_from_checkpoint = False
    if args.continue_training and args.new_training:
        print("âŒ éŒ¯èª¤ï¼šä¸èƒ½åŒæ™‚æŒ‡å®š --continue å’Œ --new")
        sys.exit(1)
    elif args.continue_training:
        print("ğŸ”„ æ¨¡å¼ï¼šå¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´")
        continue_from_checkpoint = True
    elif args.new_training:
        print("ğŸ†• æ¨¡å¼ï¼šé–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´")
        continue_from_checkpoint = False
    else:
        # å¦‚æœæ²’æœ‰æŒ‡å®šåƒæ•¸ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰ç¾æœ‰æ¨¡å‹
        lora_path = path_config["output_dir"]
        existing_lora = None
        if os.path.exists(lora_path):
            lora_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
            if lora_files:
                latest_lora = max(lora_files, key=lambda x: os.path.getmtime(os.path.join(lora_path, x)))
                existing_lora = os.path.join(lora_path, latest_lora)
        
        if existing_lora:
            print(f"ğŸ” ç™¼ç¾ç¾æœ‰çš„ LoRA æ¨¡å‹: {os.path.basename(existing_lora)}")
            print("è«‹é¸æ“‡æ“ä½œï¼š")
            print("1. å¾ç¾æœ‰æ¨¡å‹ç¹¼çºŒè¨“ç·´ (ç´¯ç©èª¿æ•™)")
            print("2. é–‹å§‹æ–°çš„ç¨ç«‹è¨“ç·´ (é‡æ–°é–‹å§‹)")
            
            while True:
                choice = input("è«‹è¼¸å…¥é¸æ“‡ (1 æˆ– 2): ").strip()
                if choice == "1":
                    continue_from_checkpoint = True
                    break
                elif choice == "2":
                    continue_from_checkpoint = False
                    break
                else:
                    print("è«‹è¼¸å…¥ 1 æˆ– 2")
        else:
            print("ğŸ†• æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ¨¡å‹ï¼Œå°‡é–‹å§‹æ–°çš„è¨“ç·´")
            continue_from_checkpoint = False
        
    # å»ºç«‹è¨“ç·´å™¨
    trainer = LoRATrainer(continue_from_checkpoint=continue_from_checkpoint, custom_params=custom_params)
    
    # åŸ·è¡Œè¨“ç·´
    result = trainer.train()
    
    # æ±ºå®šè¿”å›ç¢¼ - ç°¡åŒ–é‚è¼¯ï¼Œèˆ‡ train_lora.py ä¿æŒä¸€è‡´
    if result["success"]:
        return 0  # è¨“ç·´æˆåŠŸ
    else:
        return 1  # è¨“ç·´å¤±æ•—

if __name__ == "__main__":
    sys.exit(main())
