#!/usr/bin/env python3
"""
æ¯”è¼ƒå…©å€‹ LoRA è¨“ç·´è…³æœ¬çš„åƒæ•¸è¨­å®š
"""

import re

# å¾ train_lora.py æå–çš„åƒæ•¸
train_lora_params = {
    "pretrained_model_name_or_path": "../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
    "train_data_dir": "lora_train_set",
    "output_dir": "lora_output",
    "resolution": "512,512",
    "network_module": "networks.lora",
    "network_dim": "8",
    "train_batch_size": "1",
    "max_train_steps": "200",
    "mixed_precision": "fp16",
    "cache_latents": True,
    "learning_rate": "1e-4",
    "sample_every_n_steps": None,
    "sample_sampler": None
}

# å¾ train_lora_monitored.py æå–çš„åƒæ•¸
train_lora_monitored_params = {
    "pretrained_model_name_or_path": "../models/Stable-diffusion/v1-5-pruned-emaonly.safetensors",
    "train_data_dir": "lora_train_set",
    "output_dir": "lora_output",
    "resolution": "512,512",
    "network_module": "networks.lora",
    "network_dim": "8",
    "train_batch_size": "1",
    "max_train_steps": "200",
    "mixed_precision": "fp16",
    "cache_latents": True,
    "learning_rate": "1e-4",
    "sample_every_n_steps": "100",
    "sample_sampler": "euler_a"
}

def compare_params():
    """æ¯”è¼ƒå…©å€‹è…³æœ¬çš„åƒæ•¸"""
    print("ğŸ” LoRA è¨“ç·´è…³æœ¬åƒæ•¸æ¯”è¼ƒ")
    print("=" * 80)
    
    all_keys = set(train_lora_params.keys()) | set(train_lora_monitored_params.keys())
    
    print(f"{'åƒæ•¸åç¨±':<30} {'train_lora.py':<25} {'train_lora_monitored.py':<25} {'ç‹€æ…‹':<10}")
    print("-" * 90)
    
    differences = []
    
    for key in sorted(all_keys):
        val1 = train_lora_params.get(key)
        val2 = train_lora_monitored_params.get(key)
        
        if val1 == val2:
            status = "âœ… ç›¸åŒ"
        elif val1 is None and val2 is not None:
            status = "â• æ–°å¢"
            differences.append(f"{key}: monitored ç‰ˆæœ¬æ–°å¢ {val2}")
        elif val1 is not None and val2 is None:
            status = "â– ç§»é™¤"
            differences.append(f"{key}: monitored ç‰ˆæœ¬ç§»é™¤")
        else:
            status = "âŒ ä¸åŒ"
            differences.append(f"{key}: {val1} -> {val2}")
        
        print(f"{key:<30} {str(val1):<25} {str(val2):<25} {status}")
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ å·®ç•°ç¸½çµ:")
    
    if not differences:
        print("ğŸ‰ å…©å€‹è…³æœ¬çš„æ ¸å¿ƒåƒæ•¸å®Œå…¨ä¸€è‡´ï¼")
    else:
        print(f"ç™¼ç¾ {len(differences)} å€‹å·®ç•°:")
        for diff in differences:
            print(f"  - {diff}")
    
    # æª¢æŸ¥é¡å¤–åŠŸèƒ½
    print("\nğŸ“Š é¡å¤–åŠŸèƒ½æ¯”è¼ƒ:")
    print("train_lora.py:")
    print("  - åŸºæœ¬è¨“ç·´åŠŸèƒ½")
    print("  - åœ–ç‰‡å°ºå¯¸æª¢æŸ¥")
    print("  - ç°¡å–®çš„è¨“ç·´æŒ‡ä»¤åŸ·è¡Œ")
    
    print("\ntrain_lora_monitored.py:")
    print("  - åŸºæœ¬è¨“ç·´åŠŸèƒ½")
    print("  - åœ–ç‰‡å°ºå¯¸æª¢æŸ¥")
    print("  - è¨“ç·´é€²åº¦ç›£æ§")
    print("  - è¨“ç·´çµæœè©•ä¼°")
    print("  - è©³ç´°æ—¥èªŒè¨˜éŒ„")
    print("  - è¨“ç·´å ±å‘Šç”Ÿæˆ")
    print("  - è‡ªå‹•æ±ºç­–æ˜¯å¦ç¹¼çºŒæ¨ç†")
    print("  - æ¡æ¨£é è¦½åŠŸèƒ½ (sample_every_n_steps)")

if __name__ == "__main__":
    compare_params()
