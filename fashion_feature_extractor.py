#!/usr/bin/env python3
"""
Fashion Feature Extraction Pipeline
ä½¿ç”¨ FashionCLIP åˆ†ææœè£é›œèªŒåœ–ç‰‡ä¸¦æå–ç‰¹å¾µ
"""

import os
import json
import torch
import clip
from PIL import Image
import pandas as pd
from datetime import datetime
import numpy as np

class FashionFeatureExtractor:
    """æ™‚å°šç‰¹å¾µæå–å™¨"""
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ”§ ä½¿ç”¨è¨­å‚™: {self.device}")
        
        # è¼‰å…¥ CLIP æ¨¡å‹ (å¯ä»¥æ›¿æ›ç‚º FashionCLIP)
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        
        # å®šç¾©æ™‚å°šç›¸é—œçš„ç‰¹å¾µé¡åˆ¥
        self.feature_categories = {
            "gender": ["male", "female", "unisex"],
            "age_group": ["children", "teenager", "young adult", "middle aged", "elderly"],
            "top_clothing": [
                "t-shirt", "shirt", "blouse", "sweater", "jacket", "coat", 
                "hoodie", "tank top", "cardigan", "blazer", "vest"
            ],
            "bottom_clothing": [
                "jeans", "trousers", "shorts", "skirt", "dress", "leggings",
                "pants", "chinos", "joggers", "overalls"
            ],
            "style": [
                "casual", "formal", "business", "sporty", "elegant", "vintage",
                "streetwear", "bohemian", "minimalist", "punk", "gothic"
            ],
            "color_scheme": [
                "monochrome", "colorful", "pastel", "bright", "dark", "neutral",
                "warm tones", "cool tones", "earth tones"
            ],
            "season": ["spring", "summer", "autumn", "winter"],
            "occasion": [
                "everyday", "work", "party", "wedding", "beach", "gym",
                "date", "travel", "home", "outdoor"
            ]
        }
    
    def extract_features_from_image(self, image_path):
        """å¾å–®å¼µåœ–ç‰‡æå–ç‰¹å¾µ"""
        try:
            # è¼‰å…¥å’Œé è™•ç†åœ–ç‰‡
            image = Image.open(image_path).convert('RGB')
            image_input = self.preprocess(image).unsqueeze(0).to(self.device)
            
            features = {}
            
            # å°æ¯å€‹ç‰¹å¾µé¡åˆ¥é€²è¡Œåˆ†æ
            for category, options in self.feature_categories.items():
                print(f"   åˆ†æ {category}...")
                
                # å‰µå»ºæ–‡å­—æç¤º
                text_prompts = [f"a photo of {option} clothing" for option in options]
                text_inputs = clip.tokenize(text_prompts).to(self.device)
                
                # è¨ˆç®—ç›¸ä¼¼åº¦
                with torch.no_grad():
                    image_features = self.model.encode_image(image_input)
                    text_features = self.model.encode_text(text_inputs)
                    
                    # è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
                    similarities = torch.cosine_similarity(
                        image_features, text_features, dim=1
                    )
                    
                    # è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒ
                    probs = torch.softmax(similarities, dim=0)
                    
                    # å„²å­˜çµæœ
                    category_features = {}
                    for i, option in enumerate(options):
                        category_features[option] = float(probs[i])
                    
                    features[category] = category_features
            
            return features
            
        except Exception as e:
            print(f"âŒ è™•ç†åœ–ç‰‡ {image_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def process_fashion_magazine_dataset(self, dataset_dir, output_file="fashion_features.json"):
        """è™•ç†æ•´å€‹æ™‚å°šé›œèªŒè³‡æ–™é›†"""
        
        print(f"ğŸ” æƒæè³‡æ–™é›†ç›®éŒ„: {dataset_dir}")
        
        # æ”¯æ´çš„åœ–ç‰‡æ ¼å¼
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        
        # æ”¶é›†æ‰€æœ‰åœ–ç‰‡æª”æ¡ˆ
        image_files = []
        for root, dirs, files in os.walk(dataset_dir):
            for file in files:
                if any(file.lower().endswith(ext) for ext in image_extensions):
                    image_files.append(os.path.join(root, file))
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡")
        
        # è™•ç†æ¯å¼µåœ–ç‰‡
        all_features = []
        
        for i, image_path in enumerate(image_files, 1):
            print(f"\nğŸ¨ è™•ç†åœ–ç‰‡ {i}/{len(image_files)}: {os.path.basename(image_path)}")
            
            features = self.extract_features_from_image(image_path)
            
            if features:
                # æ·»åŠ å…ƒæ•¸æ“š
                image_data = {
                    "image_path": image_path,
                    "image_name": os.path.basename(image_path),
                    "processing_time": datetime.now().isoformat(),
                    "features": features
                }
                
                all_features.append(image_data)
                print(f"âœ… ç‰¹å¾µæå–å®Œæˆ")
            else:
                print(f"âŒ ç‰¹å¾µæå–å¤±æ•—")
        
        # å„²å­˜çµæœ
        print(f"\nğŸ’¾ å„²å­˜ç‰¹å¾µè³‡æ–™åˆ° {output_file}")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "dataset_info": {
                    "total_images": len(image_files),
                    "processed_images": len(all_features),
                    "processing_date": datetime.now().isoformat(),
                    "dataset_directory": dataset_dir
                },
                "features": all_features
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ‰ è™•ç†å®Œæˆ! æˆåŠŸè™•ç† {len(all_features)}/{len(image_files)} å¼µåœ–ç‰‡")
        return all_features
    
    def generate_sd_prompts(self, features_data, output_file="sd_prompts.json"):
        """å°‡ç‰¹å¾µè½‰æ›ç‚º Stable Diffusion æç¤ºè©"""
        
        print("ğŸ”„ è½‰æ›ç‰¹å¾µç‚º SD æç¤ºè©...")
        
        sd_prompts = []
        
        for item in features_data:
            features = item["features"]
            
            # æ§‹å»ºæç¤ºè©
            prompt_parts = []
            negative_parts = []
            
            # æ€§åˆ¥
            gender_scores = features.get("gender", {})
            top_gender = max(gender_scores.items(), key=lambda x: x[1])
            if top_gender[1] > 0.3:
                prompt_parts.append(top_gender[0])
            
            # å¹´é½¡å±¤
            age_scores = features.get("age_group", {})
            top_age = max(age_scores.items(), key=lambda x: x[1])
            if top_age[1] > 0.3:
                prompt_parts.append(f"{top_age[0]} person")
            
            # æœè£é¡å‹
            top_scores = features.get("top_clothing", {})
            top_top = max(top_scores.items(), key=lambda x: x[1])
            if top_top[1] > 0.3:
                prompt_parts.append(top_top[0])
            
            bottom_scores = features.get("bottom_clothing", {})
            top_bottom = max(bottom_scores.items(), key=lambda x: x[1])
            if top_bottom[1] > 0.3:
                prompt_parts.append(top_bottom[0])
            
            # é¢¨æ ¼
            style_scores = features.get("style", {})
            top_style = max(style_scores.items(), key=lambda x: x[1])
            if top_style[1] > 0.3:
                prompt_parts.append(f"{top_style[0]} style")
            
            # å ´åˆ
            occasion_scores = features.get("occasion", {})
            top_occasion = max(occasion_scores.items(), key=lambda x: x[1])
            if top_occasion[1] > 0.3:
                prompt_parts.append(f"{top_occasion[0]} outfit")
            
            # çµ„åˆæç¤ºè©
            main_prompt = ", ".join(prompt_parts)
            main_prompt += ", fashion photography, high quality, detailed"
            
            negative_prompt = "blurry, low quality, distorted, deformed, ugly"
            
            sd_prompt_data = {
                "original_image": item["image_path"],
                "prompt": main_prompt,
                "negative_prompt": negative_prompt,
                "features_scores": features
            }
            
            sd_prompts.append(sd_prompt_data)
        
        # å„²å­˜ SD æç¤ºè©
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "generation_info": {
                    "total_prompts": len(sd_prompts),
                    "generation_date": datetime.now().isoformat()
                },
                "prompts": sd_prompts
            }, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… å·²ç”Ÿæˆ {len(sd_prompts)} å€‹ SD æç¤ºè©")
        return sd_prompts

def main():
    """ä¸»å‡½æ•¸ - ç¤ºç¯„ä½¿ç”¨"""
    
    print("ğŸ‘— Fashion Feature Extraction Pipeline")
    print("=" * 60)
    
    # å‰µå»ºç‰¹å¾µæå–å™¨
    extractor = FashionFeatureExtractor()
    
    # è¨­å®šè³‡æ–™é›†è·¯å¾‘ (è«‹ä¿®æ”¹ç‚ºæ‚¨çš„è³‡æ–™é›†è·¯å¾‘)
    dataset_directory = "fashion_magazine_images"
    
    # æª¢æŸ¥è³‡æ–™é›†æ˜¯å¦å­˜åœ¨
    if not os.path.exists(dataset_directory):
        print(f"âš ï¸ è³‡æ–™é›†ç›®éŒ„ä¸å­˜åœ¨: {dataset_directory}")
        print("è«‹å‰µå»ºç›®éŒ„ä¸¦æ”¾å…¥æ™‚å°šé›œèªŒåœ–ç‰‡")
        
        # å‰µå»ºç¤ºä¾‹ç›®éŒ„çµæ§‹
        os.makedirs(dataset_directory, exist_ok=True)
        print(f"ğŸ“ å·²å‰µå»ºç›®éŒ„: {dataset_directory}")
        print("è«‹å°‡æ™‚å°šé›œèªŒåœ–ç‰‡æ”¾å…¥æ­¤ç›®éŒ„ä¸­")
        return
    
    # è™•ç†è³‡æ–™é›†
    features_data = extractor.process_fashion_magazine_dataset(
        dataset_directory, 
        "fashion_features.json"
    )
    
    if features_data:
        # ç”Ÿæˆ SD æç¤ºè©
        sd_prompts = extractor.generate_sd_prompts(
            features_data, 
            "sd_prompts.json"
        )
        
        print(f"\nğŸ‰ Pipeline å®Œæˆ!")
        print(f"ğŸ“Š è™•ç†äº† {len(features_data)} å¼µåœ–ç‰‡")
        print(f"ğŸ”¤ ç”Ÿæˆäº† {len(sd_prompts)} å€‹æç¤ºè©")
        print(f"ğŸ“ ç‰¹å¾µæª”æ¡ˆ: fashion_features.json")
        print(f"ğŸ“ æç¤ºè©æª”æ¡ˆ: sd_prompts.json")
    
    print("\nä¸‹ä¸€æ­¥: ä½¿ç”¨é€™äº›æç¤ºè©è¨“ç·´ SD æ¨¡å‹")

if __name__ == "__main__":
    main()
