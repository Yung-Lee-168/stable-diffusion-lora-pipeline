#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Day 3: Fashion AI Training - Google Colab ç‰ˆæœ¬
å°ˆç‚º Google Colab ç’°å¢ƒå„ªåŒ–çš„ SD v1.5 çœŸæ­£å¾®èª¿ç³»çµ±

ğŸ¯ ç‰¹è‰²:
- é‡å° Colab çš„ T4/V100 GPU å„ªåŒ–
- è‡ªå‹•è¨˜æ†¶é«”ç®¡ç†å’Œæ¸…ç†
- LoRA é«˜æ•ˆå¾®èª¿ (ç¯€çœè¨˜æ†¶é«”)
- è‡ªå‹•ä¸Šå‚³/ä¸‹è¼‰ Google Drive
- æ¼¸é€²å¼è¨“ç·´å’Œæª¢æŸ¥é»ä¿å­˜
- å¯¦æ™‚è¨“ç·´ç›£æ§

ğŸ”§ è¨˜æ†¶é«”éœ€æ±‚:
- T4 (16GB): æ”¯æ´ LoRA + æ··åˆç²¾åº¦
- V100 (16GB): æ”¯æ´å®Œæ•´è¨“ç·´
- è‡ªå‹•åµæ¸¬å’Œèª¿æ•´é…ç½®
"""

import os
import json
import torch
import gc
import zipfile
import shutil
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# Colab ç’°å¢ƒæª¢æŸ¥å’Œä¾è³´å®‰è£
def setup_colab_environment():
    """Colab ç’°å¢ƒåˆå§‹åŒ–å’Œä¾è³´å®‰è£ - ä¿®å¾©è¤‡é›œä¾è³´è¡çª"""
    print("ï¿½ Google Colab ç’°å¢ƒè¨­ç½®ä¸­...")
    print("ï¿½ æª¢æ¸¬åˆ°è¤‡é›œçš„ä¾è³´è¡çªï¼Œæ­£åœ¨é€²è¡Œæ·±åº¦ä¿®å¾©...")
    
    # æª¢æŸ¥å’Œè§£æ±ºä¾è³´è¡çª
    import subprocess
    import sys
    
    try:
        # 1. å…ˆå¸è¼‰æ‰€æœ‰å¯èƒ½è¡çªçš„å¥—ä»¶
        print("ğŸ—‘ï¸ æ­¥é©Ÿ 1: æ·±åº¦æ¸…ç†è¡çªå¥—ä»¶...")
        conflicting_packages = [
            "sentence-transformers", 
            "transformers", 
            "torch", 
            "torchvision", 
            "torchaudio",
            "fastai"
        ]
        
        for package in conflicting_packages:
            try:
                subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", package], 
                              capture_output=True, text=True)
                print(f"   ğŸ—‘ï¸ å·²å¸è¼‰ {package}")
            except:
                pass
        
        # 2. å®‰è£å…¼å®¹çš„ PyTorch ç”Ÿæ…‹ç³»çµ±
        print("ğŸ“¦ æ­¥é©Ÿ 2: å®‰è£å…¼å®¹çš„ PyTorch ç‰ˆæœ¬...")
        
        # ä½¿ç”¨ CUDA 11.8 çš„ç©©å®šç‰ˆæœ¬çµ„åˆ
        torch_install_cmd = [
            sys.executable, "-m", "pip", "install", 
            "torch==2.1.0+cu118", 
            "torchvision==0.16.0+cu118", 
            "torchaudio==2.1.0+cu118",
            "--index-url", "https://download.pytorch.org/whl/cu118",
            "--force-reinstall"
        ]
        
        result = subprocess.run(torch_install_cmd, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… PyTorch ç”Ÿæ…‹ç³»çµ±å®‰è£æˆåŠŸ")
        else:
            print("âš ï¸ PyTorch å®‰è£å¯èƒ½æœ‰å•é¡Œï¼Œç¹¼çºŒ...")
        
        # 3. å®‰è£å…¼å®¹ç‰ˆæœ¬çš„ transformers
        print("ğŸ“¦ æ­¥é©Ÿ 3: å®‰è£å…¼å®¹ç‰ˆæœ¬çš„ transformers...")
        subprocess.run([sys.executable, "-m", "pip", "install", 
                       "transformers>=4.41.0,<5.0.0", "--force-reinstall"], 
                      capture_output=True, text=True)
        
        # 4. å®‰è£å…¶ä»–æ ¸å¿ƒå¥—ä»¶
        print("ğŸ“¦ æ­¥é©Ÿ 4: å®‰è£å°ˆæ¡ˆä¾è³´...")
        core_packages = [
            "diffusers[torch]",
            "accelerate",
            "peft>=0.4.0",
            "packaging",
            "matplotlib",
            "seaborn", 
            "numpy",
            "pillow",
            "scikit-learn"
        ]
        
        for package in core_packages:
            try:
                subprocess.run([sys.executable, "-m", "pip", "install", "-q", package], 
                              capture_output=True, text=True, check=True)
                print(f"âœ… {package}")
            except subprocess.CalledProcessError:
                print(f"âš ï¸ {package} å®‰è£å¤±æ•—")
        
        # 5. é‡æ–°å®‰è£ sentence-transformersï¼ˆæœ€å¾Œå®‰è£ä»¥é¿å…è¡çªï¼‰
        print("ğŸ“¦ æ­¥é©Ÿ 5: é‡æ–°å®‰è£ sentence-transformers...")
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", 
                           "sentence-transformers"], 
                          capture_output=True, text=True, check=True)
            print("âœ… sentence-transformers")
        except subprocess.CalledProcessError:
            print("âš ï¸ sentence-transformers å®‰è£å¤±æ•—ï¼ˆå¯é¸å¥—ä»¶ï¼‰")
        
        # 6. å˜—è©¦å®‰è£ xformersï¼ˆå¯é¸ï¼Œä½†æœ‰åŠ©æ–¼æ€§èƒ½ï¼‰
        print("ğŸ“¦ æ­¥é©Ÿ 6: å®‰è£ xformers...")
        try:
            subprocess.run([sys.executable, "-m", "pip", "install", "-q", 
                           "xformers==0.0.22.post7", 
                           "--index-url", "https://download.pytorch.org/whl/cu118"], 
                          capture_output=True, text=True, check=True)
            print("âœ… xformers")
        except subprocess.CalledProcessError:
            print("âš ï¸ xformers å®‰è£å¤±æ•—ï¼ˆå°‡ä½¿ç”¨æ¨™æº–æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼‰")
        
        # 7. æª¢æŸ¥æœ€çµ‚å®‰è£ç‹€æ…‹
        print("ğŸ“‹ æ­¥é©Ÿ 7: æª¢æŸ¥å®‰è£ç‹€æ…‹...")
        _check_final_installation()
        
        print("âœ… ç’°å¢ƒè¨­ç½®å®Œæˆï¼")
        print("ğŸ”„ å¼·çƒˆå»ºè­°é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚ (Runtime > Restart runtime)")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
        print("\nğŸ”§ æ‰‹å‹•ä¿®å¾©æ–¹æ³•:")
        print("è«‹åœ¨æ–°çš„ cell ä¸­åŸ·è¡Œä»¥ä¸‹å‘½ä»¤:")
        print("!pip uninstall -y torch torchvision torchaudio transformers sentence-transformers fastai")
        print("!pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118")
        print("!pip install transformers>=4.41.0 diffusers[torch] accelerate peft")
        print("ç„¶å¾Œé‡æ–°å•Ÿå‹•é‹è¡Œæ™‚")
        return False

def _check_final_installation():
    """æª¢æŸ¥æœ€çµ‚å®‰è£ç‹€æ…‹"""
    try:
        import torch
        import transformers
        import diffusers
        
        print(f"   torch: {torch.__version__}")
        print(f"   transformers: {transformers.__version__}")
        print(f"   diffusers: {diffusers.__version__}")
        
        # æª¢æŸ¥ CUDA å¯ç”¨æ€§
        if torch.cuda.is_available():
            print(f"   CUDA: å¯ç”¨ (ç‰ˆæœ¬ {torch.version.cuda})")
        else:
            print("   CUDA: ä¸å¯ç”¨")
            
        # æ¸¬è©¦é—œéµå°å…¥
        from diffusers import StableDiffusionPipeline
        from peft import LoraConfig
        print("   âœ… é—œéµå¥—ä»¶å°å…¥æ¸¬è©¦é€šé")
        
    except ImportError as e:
        print(f"   âŒ å°å…¥æ¸¬è©¦å¤±æ•—: {e}")
    except Exception as e:
        print(f"   âš ï¸ æª¢æŸ¥éç¨‹å‡ºéŒ¯: {e}")

# æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒ
try:
    from google.colab import drive, files
    IN_COLAB = True
    print("ğŸŒ åœ¨ Google Colab ç’°å¢ƒä¸­é‹è¡Œ")
    
    # è‡ªå‹•ä¿®å¾©ä¾è³´ï¼ˆä½†ä¸é˜»å¡å¾ŒçºŒå°å…¥ï¼‰
    try:
        setup_success = setup_colab_environment()
        if not setup_success:
            print("âš ï¸ è‡ªå‹•ä¿®å¾©å¤±æ•—ï¼Œè«‹æ‰‹å‹•åŸ·è¡Œä¿®å¾©å‘½ä»¤æˆ–é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚")
    except Exception as e:
        print(f"âš ï¸ ä¾è³´ä¿®å¾©éç¨‹å‡ºéŒ¯: {e}")
        print("è«‹é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚ä¸¦æ‰‹å‹•å®‰è£ä¾è³´")
    
except ImportError:
    IN_COLAB = False
    print("ğŸ’» åœ¨æœ¬åœ°ç’°å¢ƒä¸­é‹è¡Œ")

# å˜—è©¦å°å…¥æ·±åº¦å­¸ç¿’æ¡†æ¶ï¼Œå¦‚æœå¤±æ•—å‰‡æä¾›æŒ‡å°
try:
    from diffusers import (
        StableDiffusionPipeline, 
        UNet2DConditionModel,
        DDPMScheduler,
        AutoencoderKL
    )
    from transformers import CLIPTextModel, CLIPTokenizer, CLIPProcessor, CLIPModel
    from accelerate import Accelerator
    from torch.utils.data import Dataset, DataLoader
    import torch.nn.functional as F
    
    # PEFT å°å…¥æœ‰æ™‚æœƒå¤±æ•—ï¼Œå–®ç¨è™•ç†
    try:
        from peft import LoraConfig, get_peft_model, TaskType
        PEFT_AVAILABLE = True
    except ImportError:
        print("âš ï¸ PEFT ä¸å¯ç”¨ï¼Œå°‡è·³é LoRA åŠŸèƒ½")
        PEFT_AVAILABLE = False
        
        # å‰µå»ºå‡çš„ LoRA é¡åˆ¥ä»¥é¿å…éŒ¯èª¤
        class LoraConfig:
            def __init__(self, **kwargs):
                pass
        
        def get_peft_model(model, config):
            return model
            
        TaskType = type('TaskType', (), {})
    
    print("âœ… æ ¸å¿ƒå¥—ä»¶å°å…¥æˆåŠŸ")
    
except ImportError as e:
    print(f"âŒ æ ¸å¿ƒå¥—ä»¶å°å…¥å¤±æ•—: {e}")
    print("ğŸ”§ è§£æ±ºæ–¹æ¡ˆ:")
    if IN_COLAB:
        print("1. é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚ (Runtime > Restart runtime)")
        print("2. é‡æ–°åŸ·è¡Œæ­¤è…³æœ¬")
        print("3. æˆ–æ‰‹å‹•åŸ·è¡Œä»¥ä¸‹å‘½ä»¤:")
        print("   !pip uninstall -y torch torchvision torchaudio transformers")
        print("   !pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 --index-url https://download.pytorch.org/whl/cu118")
        print("   !pip install transformers>=4.41.0 diffusers[torch] accelerate peft")
    else:
        print("è«‹æª¢æŸ¥æœ¬åœ°ç’°å¢ƒçš„å¥—ä»¶å®‰è£")
    
    # å‰µå»ºç©ºçš„é¡åˆ¥ä»¥é¿å…å¾ŒçºŒéŒ¯èª¤
    class StableDiffusionPipeline: pass
    class UNet2DConditionModel: pass
    class DDPMScheduler: pass
    class AutoencoderKL: pass
    class CLIPTextModel: pass
    class CLIPTokenizer: pass
    class CLIPProcessor: pass
    class CLIPModel: pass
    class Accelerator: pass
    class Dataset: pass
    class DataLoader: pass
    class LoraConfig: pass
    def get_peft_model(model, config): return model
    TaskType = type('TaskType', (), {})
    PEFT_AVAILABLE = False

class ColabEnvironmentSetup:
    """Google Colab ç’°å¢ƒè¨­ç½®å’Œå„ªåŒ–"""
    
    def __init__(self):
        self.gpu_info = self.check_gpu()
        self.drive_mounted = False
        
    def check_gpu(self):
        """æª¢æŸ¥ GPU ç‹€æ…‹å’Œè¨˜æ†¶é«”"""
        if not torch.cuda.is_available():
            print("âŒ æ²’æœ‰å¯ç”¨çš„ GPU")
            return None
            
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"ğŸ”§ GPU: {gpu_name}")
        print(f"ğŸ’¾ VRAM: {gpu_memory:.1f} GB")
        
        return {
            "name": gpu_name,
            "memory_gb": gpu_memory,
            "is_t4": "T4" in gpu_name,
            "is_v100": "V100" in gpu_name,
            "is_a100": "A100" in gpu_name
        }
    
    def mount_drive(self):
        """æ›è¼‰ Google Drive"""
        if IN_COLAB and not self.drive_mounted:
            try:
                drive.mount('/content/drive')
                self.drive_mounted = True
                print("âœ… Google Drive å·²æ›è¼‰")
                
                # å‰µå»ºå·¥ä½œç›®éŒ„
                work_dir = "/content/drive/MyDrive/fashion_ai_training"
                os.makedirs(work_dir, exist_ok=True)
                os.chdir(work_dir)
                print(f"ğŸ“ å·¥ä½œç›®éŒ„: {work_dir}")
                
            except Exception as e:
                print(f"âŒ Drive æ›è¼‰å¤±æ•—: {e}")
        
    def setup_environment(self):
        """è¨­ç½® Colab ç’°å¢ƒ"""
        if IN_COLAB:
            print("ğŸ”§ æ­£åœ¨å®‰è£å’Œæ›´æ–°å¥—ä»¶...")
            
            # å…ˆæ›´æ–° transformers åˆ°å…¼å®¹ç‰ˆæœ¬
            os.system("pip install -q --upgrade transformers>=4.41.0")
            
            # å®‰è£å…¶ä»–å¿…è¦å¥—ä»¶
            os.system("pip install -q diffusers[torch] accelerate peft")
            
            # å˜—è©¦å®‰è£ xformersï¼ˆå¦‚æœå¤±æ•—å‰‡è·³éï¼‰
            try:
                os.system("pip install -q xformers --index-url https://download.pytorch.org/whl/cu118")
                print("âœ… xformers å®‰è£æˆåŠŸ")
            except:
                print("âš ï¸ xformers å®‰è£å¤±æ•—ï¼Œå°‡ä½¿ç”¨æ¨™æº–æ³¨æ„åŠ›æ©Ÿåˆ¶")
            
            # æª¢æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
            self._check_package_compatibility()
            
            # è¨­ç½®ç’°å¢ƒè®Šæ•¸
            os.environ["CUDA_VISIBLE_DEVICES"] = "0"
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
            
        # æ¸…ç†è¨˜æ†¶é«”
        self.cleanup_memory()
    
    def _check_package_compatibility(self):
        """æª¢æŸ¥å¥—ä»¶ç‰ˆæœ¬å…¼å®¹æ€§"""
        try:
            import transformers
            import diffusers
            
            print(f"ğŸ“‹ å¥—ä»¶ç‰ˆæœ¬æª¢æŸ¥:")
            print(f"   transformers: {transformers.__version__}")
            print(f"   diffusers: {diffusers.__version__}")
            
            # æª¢æŸ¥ transformers ç‰ˆæœ¬
            from packaging import version
            transformers_version = version.parse(transformers.__version__)
            required_version = version.parse("4.41.0")
            
            if transformers_version < required_version:
                print(f"âš ï¸ transformers ç‰ˆæœ¬éä½ï¼Œæ­£åœ¨å‡ç´š...")
                os.system("pip install -q --upgrade transformers>=4.41.0")
                print("âœ… transformers å·²å‡ç´šï¼Œè«‹é‡æ–°å•Ÿå‹• runtime")
                
        except ImportError as e:
            print(f"âš ï¸ å°å…¥æª¢æŸ¥å¤±æ•—: {e}")
        except Exception as e:
            print(f"âš ï¸ ç‰ˆæœ¬æª¢æŸ¥å¤±æ•—: {e}")
        
    def cleanup_memory(self):
        """æ¸…ç† GPU è¨˜æ†¶é«”"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ§¹ è¨˜æ†¶é«”æ¸…ç†å®Œæˆï¼Œå¯ç”¨: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

class FashionDataset(Dataset):
    """æ™‚å°šåœ–ç‰‡æ•¸æ“šé›†"""
    
    def __init__(self, image_paths: List[str], captions: List[str], 
                 tokenizer, image_size: int = 512):
        self.image_paths = image_paths
        self.captions = captions
        self.tokenizer = tokenizer
        self.image_size = image_size
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # è¼‰å…¥å’Œé è™•ç†åœ–ç‰‡
        image = Image.open(self.image_paths[idx]).convert("RGB")
        image = image.resize((self.image_size, self.image_size))
        image = np.array(image) / 255.0
        image = torch.tensor(image).permute(2, 0, 1).float()
        
        # è™•ç†æ–‡å­—æè¿°
        caption = self.captions[idx]
        tokens = self.tokenizer(
            caption,
            padding="max_length",
            max_length=77,
            truncation=True,
            return_tensors="pt"
        )
        
        return {
            "image": image,
            "input_ids": tokens.input_ids.squeeze(),
            "attention_mask": tokens.attention_mask.squeeze()
        }

class FashionSDFineTuner:
    """SD v1.5 æ™‚å°šå¾®èª¿å™¨ - Colab å„ªåŒ–ç‰ˆ"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.env_setup = ColabEnvironmentSetup()
        self.config = self._get_colab_optimized_config(config)
        self.accelerator = None
        self.models = {}
        self.fashion_clip_model = None
        
        # è¨­ç½®ç’°å¢ƒ
        self.env_setup.setup_environment()
        self.env_setup.mount_drive()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
    def _get_colab_optimized_config(self, custom_config: Optional[Dict] = None):
        """æ ¹æ“š Colab GPU è‡ªå‹•å„ªåŒ–é…ç½®"""
        gpu_info = self.env_setup.gpu_info
        
        if gpu_info is None:
            raise RuntimeError("éœ€è¦ GPU æ”¯æ´")
        
        # åŸºç¤é…ç½®
        base_config = {
            "model_id": "runwayml/stable-diffusion-v1-5",
            "output_dir": "/content/drive/MyDrive/fashion_ai_training/models",
            "cache_dir": "/content/cache",
            "image_size": 512,
            "train_batch_size": 1,
            "num_epochs": 20,
            "learning_rate": 1e-4,
            "use_lora": True,
            "lora_rank": 4,
            "mixed_precision": "fp16",
            "gradient_accumulation_steps": 4,
            "save_steps": 100,
            "validation_steps": 50,
            "max_grad_norm": 1.0,
            "warmup_steps": 100
        }
        
        # æ ¹æ“š GPU èª¿æ•´é…ç½®
        if gpu_info["is_t4"]:
            # T4 å„ªåŒ– (16GB)
            base_config.update({
                "train_batch_size": 1,
                "gradient_accumulation_steps": 8,
                "lora_rank": 4,
                "image_size": 512,
                "use_xformers": True
            })
            print("ğŸ¯ T4 GPU å„ªåŒ–é…ç½®")
            
        elif gpu_info["is_v100"]:
            # V100 å„ªåŒ– (16GB)
            base_config.update({
                "train_batch_size": 2,
                "gradient_accumulation_steps": 4,
                "lora_rank": 8,
                "image_size": 512,
                "use_xformers": True
            })
            print("ğŸ¯ V100 GPU å„ªåŒ–é…ç½®")
            
        elif gpu_info["is_a100"]:
            # A100 å„ªåŒ– (40GB)
            base_config.update({
                "train_batch_size": 4,
                "gradient_accumulation_steps": 2,
                "lora_rank": 16,
                "image_size": 768,
                "use_xformers": True
            })
            print("ğŸ¯ A100 GPU å„ªåŒ–é…ç½®")
        
        # åˆä½µè‡ªå®šç¾©é…ç½®
        if custom_config:
            base_config.update(custom_config)
            
        return base_config
    
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        print("ğŸ“¦ è¼‰å…¥æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"

        # åˆ¤æ–·æœ¬åœ°æ¨¡å‹è·¯å¾‘
        model_path = self.config.get("base_model_path") or self.config.get("model_id")
        cache_dir = self.config.get("cache_dir")

        # è¼‰å…¥ Stable Diffusion v1.5
        try:
            self.models["tokenizer"] = CLIPTokenizer.from_pretrained(
                model_path,
                subfolder="tokenizer",
                cache_dir=cache_dir
            )
            self.models["text_encoder"] = CLIPTextModel.from_pretrained(
                model_path,
                subfolder="text_encoder",
                cache_dir=cache_dir,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32
            ).to(device)
            self.models["vae"] = AutoencoderKL.from_pretrained(
                model_path,
                subfolder="vae",
                cache_dir=cache_dir,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32
            ).to(device)
            self.models["unet"] = UNet2DConditionModel.from_pretrained(
                model_path,
                subfolder="unet",
                cache_dir=cache_dir,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32
            ).to(device)
            print(f"âœ… Stable Diffusion è¼‰å…¥æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âŒ SD æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
        
        # è¼‰å…¥ FashionCLIP (ç”¨æ–¼ç‰¹å¾µæå–å’Œè©•ä¼°)
        try:
            self.fashion_clip_model = CLIPModel.from_pretrained(
                "patrickjohncyh/fashion-clip",
                cache_dir=self.config["cache_dir"],
                torch_dtype=torch.float16 if device == "cuda" else torch.float32
            ).to(device)
            
            self.fashion_clip_processor = CLIPProcessor.from_pretrained(
                "patrickjohncyh/fashion-clip",
                cache_dir=self.config["cache_dir"]
            )
            
            print("âœ… FashionCLIP è¼‰å…¥æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ FashionCLIP è¼‰å…¥å¤±æ•—: {e}")
            self.fashion_clip_model = None
        
        # è¨­ç½® LoRA
        if self.config["use_lora"]:
            self._setup_lora()
        
        # åˆå§‹åŒ– accelerator
        self.accelerator = Accelerator(
            mixed_precision=self.config["mixed_precision"],
            gradient_accumulation_steps=self.config["gradient_accumulation_steps"]
        )
        
        print(f"ğŸš€ ä½¿ç”¨è¨­å‚™: {device}")
        print(f"ğŸ’¾ æ··åˆç²¾åº¦: {self.config['mixed_precision']}")
    
    def _setup_lora(self):
        """è¨­ç½® LoRA å¾®èª¿"""
        if not PEFT_AVAILABLE:
            print("âš ï¸ PEFT ä¸å¯ç”¨ï¼Œè·³é LoRA è¨­ç½®ï¼Œå°‡ä½¿ç”¨å®Œæ•´å¾®èª¿")
            self.config["use_lora"] = False
            return
            
        print("ğŸ”§ è¨­ç½® LoRA å¾®èª¿...")
        
        try:
            # UNet LoRA é…ç½®
            unet_lora_config = LoraConfig(
                r=self.config["lora_rank"],
                lora_alpha=self.config["lora_rank"],
                target_modules=[
                    "to_k", "to_q", "to_v", "to_out.0",
                    "proj_in", "proj_out",
                    "ff.net.0.proj", "ff.net.2"
                ],
                lora_dropout=0.1,
            )
            
            # æ‡‰ç”¨ LoRA åˆ° UNet
            self.models["unet"] = get_peft_model(self.models["unet"], unet_lora_config)
            
            # å‡çµå…¶ä»–æ¨¡å‹åƒæ•¸
            self.models["vae"].requires_grad_(False)
            self.models["text_encoder"].requires_grad_(False)
            
            # åªè¨“ç·´ LoRA åƒæ•¸
            trainable_params = sum(p.numel() for p in self.models["unet"].parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.models["unet"].parameters())
            
            print(f"ğŸ“Š å¯è¨“ç·´åƒæ•¸: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)")
            
        except Exception as e:
            print(f"âŒ LoRA è¨­ç½®å¤±æ•—: {e}")
            print("âš ï¸ å°‡ä½¿ç”¨å®Œæ•´å¾®èª¿æ¨¡å¼")
            self.config["use_lora"] = False
    
    def upload_training_images(self):
        """ä¸Šå‚³è¨“ç·´åœ–ç‰‡ (Colab å°ˆç”¨)"""
        if not IN_COLAB:
            print("ğŸ’» æœ¬åœ°ç’°å¢ƒï¼Œè·³éä¸Šå‚³")
            return []
            
        print("ğŸ“¤ è«‹ä¸Šå‚³è¨“ç·´åœ–ç‰‡...")
        uploaded = files.upload()
        
        image_paths = []
        upload_dir = "/content/uploaded_images"
        os.makedirs(upload_dir, exist_ok=True)
        
        for filename, content in uploaded.items():
            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                file_path = os.path.join(upload_dir, filename)
                with open(file_path, 'wb') as f:
                    f.write(content)
                image_paths.append(file_path)
                
        print(f"âœ… ä¸Šå‚³äº† {len(image_paths)} å¼µåœ–ç‰‡")
        return image_paths
    
    def extract_features_from_images(self, image_paths: List[str]) -> List[str]:
        """ä½¿ç”¨ FashionCLIP å¾åœ–ç‰‡æå–ç‰¹å¾µä¸¦ç”Ÿæˆæè¿°"""
        print("ğŸ” ä½¿ç”¨ FashionCLIP æå–ç‰¹å¾µ...")
        
        if not self.fashion_clip_model:
            # å¦‚æœæ²’æœ‰ FashionCLIPï¼Œä½¿ç”¨ç°¡å–®æè¿°
            print("âš ï¸ ä½¿ç”¨ç°¡å–®æè¿°")
            return [f"fashion photo {i+1}" for i in range(len(image_paths))]
        
        captions = []
        device = next(self.fashion_clip_model.parameters()).device
        
        # ç‰¹å¾µé¡åˆ¥ (ç°¡åŒ–ç‰ˆ)
        categories = {
            "clothing": ["dress", "shirt", "jacket", "pants", "skirt", "blouse"],
            "style": ["casual", "formal", "elegant", "sporty", "vintage", "modern"],
            "color": ["black", "white", "blue", "red", "green", "pink", "brown"],
            "pattern": ["solid", "striped", "floral", "geometric", "plain"]
        }
        
        for image_path in image_paths:
            try:
                image = Image.open(image_path).convert("RGB")
                
                caption_parts = []
                
                # åˆ†ææ¯å€‹é¡åˆ¥
                for category, labels in categories.items():
                    inputs = self.fashion_clip_processor(
                        text=labels, 
                        images=image, 
                        return_tensors="pt", 
                        padding=True
                    )
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.fashion_clip_model(**inputs)
                        probs = outputs.logits_per_image.softmax(dim=1)
                        
                    # å–æœ€é«˜åˆ†çš„æ¨™ç±¤
                    best_idx = probs.argmax().item()
                    best_label = labels[best_idx]
                    caption_parts.append(best_label)
                
                # çµ„åˆæè¿°
                caption = f"a photo of a person wearing {caption_parts[0]} in {caption_parts[1]} style"
                captions.append(caption)
                
                print(f"   {os.path.basename(image_path)}: {caption}")
                
            except Exception as e:
                print(f"âš ï¸ è™•ç† {image_path} æ™‚å‡ºéŒ¯: {e}")
                captions.append("fashion photo")
        
        return captions
    
    def prepare_dataset(self, image_paths: List[str], captions: List[str]) -> DataLoader:
        """æº–å‚™è¨“ç·´æ•¸æ“šé›†"""
        print("ğŸ“Š æº–å‚™æ•¸æ“šé›†...")
        
        dataset = FashionDataset(
            image_paths=image_paths,
            captions=captions,
            tokenizer=self.models["tokenizer"],
            image_size=self.config["image_size"]
        )
        
        dataloader = DataLoader(
            dataset,
            batch_size=self.config["train_batch_size"],
            shuffle=True,
            num_workers=0,  # Colab å»ºè­°è¨­ç‚º 0
            pin_memory=True
        )
        
        print(f"âœ… æ•¸æ“šé›†æº–å‚™å®Œæˆ: {len(dataset)} å¼µåœ–ç‰‡")
        return dataloader
    
    def train(self, dataloader: DataLoader):
        """åŸ·è¡Œè¨“ç·´"""
        print("ğŸš€ é–‹å§‹è¨“ç·´...")
        
        # å„ªåŒ–å™¨
        optimizer = torch.optim.AdamW(
            self.models["unet"].parameters(),
            lr=self.config["learning_rate"],
            weight_decay=0.01
        )
        
        # å­¸ç¿’ç‡èª¿åº¦å™¨
        num_training_steps = len(dataloader) * self.config["num_epochs"]
        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=num_training_steps
        )
        
        # Accelerator æº–å‚™
        unet, optimizer, dataloader, lr_scheduler = self.accelerator.prepare(
            self.models["unet"], optimizer, dataloader, lr_scheduler
        )
        
        # å™ªè²èª¿åº¦å™¨
        noise_scheduler = DDPMScheduler.from_pretrained(
            self.config["model_id"], 
            subfolder="scheduler"
        )
        
        # è¨“ç·´å¾ªç’°
        global_step = 0
        training_losses = []
        
        for epoch in range(self.config["num_epochs"]):
            unet.train()
            epoch_losses = []
            
            for step, batch in enumerate(dataloader):
                with self.accelerator.accumulate(unet):
                    # ç·¨ç¢¼åœ–ç‰‡
                    images = batch["image"].to(self.accelerator.device)
                    latents = self.models["vae"].encode(images).latent_dist.sample()
                    latents = latents * 0.18215
                    
                    # æ·»åŠ å™ªè²
                    noise = torch.randn_like(latents)
                    timesteps = torch.randint(
                        0, noise_scheduler.config.num_train_timesteps, 
                        (latents.shape[0],), device=latents.device
                    ).long()
                    
                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                    
                    # ç·¨ç¢¼æ–‡å­—
                    encoder_hidden_states = self.models["text_encoder"](
                        batch["input_ids"].to(self.accelerator.device)
                    )[0]
                    
                    # é æ¸¬å™ªè²
                    noise_pred = unet(
                        noisy_latents, 
                        timesteps, 
                        encoder_hidden_states
                    ).sample
                    
                    # è¨ˆç®—æå¤±
                    loss = F.mse_loss(noise_pred, noise, reduction="mean")
                    
                    # åå‘å‚³æ’­
                    self.accelerator.backward(loss)
                    
                    if self.accelerator.sync_gradients:
                        self.accelerator.clip_grad_norm_(unet.parameters(), self.config["max_grad_norm"])
                    
                    optimizer.step()
                    lr_scheduler.step()
                    optimizer.zero_grad()
                
                # è¨˜éŒ„æå¤±
                epoch_losses.append(loss.item())
                training_losses.append(loss.item())
                
                if global_step % 10 == 0:
                    avg_loss = np.mean(epoch_losses[-10:])
                    print(f"Epoch {epoch+1}/{self.config['num_epochs']}, "
                          f"Step {step+1}/{len(dataloader)}, "
                          f"Loss: {avg_loss:.4f}")
                
                # ä¿å­˜æª¢æŸ¥é»
                if global_step % self.config["save_steps"] == 0:
                    self._save_checkpoint(global_step, optimizer, lr_scheduler)
                
                # é©—è­‰
                if global_step % self.config["validation_steps"] == 0:
                    self._validate(global_step)
                
                global_step += 1
                
                # è¨˜æ†¶é«”æ¸…ç†
                if global_step % 50 == 0:
                    self.env_setup.cleanup_memory()
            
            # Epoch çµæŸ
            avg_epoch_loss = np.mean(epoch_losses)
            print(f"âœ… Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡æå¤±: {avg_epoch_loss:.4f}")
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        self._save_final_model()
        
        # ç”Ÿæˆè¨“ç·´åœ–è¡¨
        self._plot_training_progress(training_losses)
        
        print("ğŸ‰ è¨“ç·´å®Œæˆï¼")
    
    def _save_checkpoint(self, step: int, optimizer, lr_scheduler):
        """ä¿å­˜è¨“ç·´æª¢æŸ¥é»"""
        checkpoint_dir = os.path.join(self.config["output_dir"], f"checkpoint-{step}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜ LoRA æ¬Šé‡æˆ–å®Œæ•´æ¨¡å‹
        if self.config["use_lora"] and PEFT_AVAILABLE:
            try:
                self.models["unet"].save_pretrained(checkpoint_dir)
                print(f"ğŸ’¾ LoRA æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_dir}")
            except Exception as e:
                print(f"âš ï¸ LoRA ä¿å­˜å¤±æ•—: {e}")
                # å›é€€åˆ°ä¿å­˜å®Œæ•´ UNet
                torch.save(self.models["unet"].state_dict(), 
                          os.path.join(checkpoint_dir, "unet_state_dict.pt"))
        else:
            # ä¿å­˜å®Œæ•´æ¨¡å‹ç‹€æ…‹
            torch.save(self.models["unet"].state_dict(), 
                      os.path.join(checkpoint_dir, "unet_state_dict.pt"))
            print(f"ğŸ’¾ å®Œæ•´æ¨¡å‹æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_dir}")
        
        # ä¿å­˜è¨“ç·´ç‹€æ…‹
        torch.save({
            "step": step,
            "optimizer_state_dict": optimizer.state_dict(),
            "lr_scheduler_state_dict": lr_scheduler.state_dict(),
            "config": self.config
        }, os.path.join(checkpoint_dir, "training_state.pt"))
    
    def _validate(self, step: int):
        """é©—è­‰ç”Ÿæˆæ•ˆæœ"""
        print(f"ğŸ” ç¬¬ {step} æ­¥é©—è­‰...")
        
        try:
            # å‰µå»ºè‡¨æ™‚ç®¡é“
            pipeline = StableDiffusionPipeline(
                vae=self.models["vae"],
                text_encoder=self.models["text_encoder"],
                tokenizer=self.models["tokenizer"],
                unet=self.accelerator.unwrap_model(self.models["unet"]),
                scheduler=DDPMScheduler.from_pretrained(
                    self.config["model_id"], subfolder="scheduler"
                ),
                safety_checker=None,
                requires_safety_checker=False
            )
            
            # æ¸¬è©¦æç¤ºè©
            test_prompts = [
                "a woman wearing a elegant dress",
                "a man in casual shirt and jeans",
                "person in formal business attire"
            ]
            
            validation_dir = os.path.join(self.config["output_dir"], "validation")
            os.makedirs(validation_dir, exist_ok=True)
            
            for i, prompt in enumerate(test_prompts):
                image = pipeline(
                    prompt,
                    num_inference_steps=20,
                    guidance_scale=7.5,
                    width=512,
                    height=512
                ).images[0]
                
                image.save(os.path.join(validation_dir, f"step_{step}_prompt_{i+1}.png"))
            
            print(f"âœ… é©—è­‰åœ–ç‰‡å·²ä¿å­˜åˆ° {validation_dir}")
            
        except Exception as e:
            print(f"âš ï¸ é©—è­‰å¤±æ•—: {e}")
    
    def _save_final_model(self):
        """ä¿å­˜æœ€çµ‚æ¨¡å‹"""
        final_dir = os.path.join(self.config["output_dir"], "final_model")
        os.makedirs(final_dir, exist_ok=True)
        
        if self.config["use_lora"] and PEFT_AVAILABLE:
            try:
                # ä¿å­˜ LoRA æ¬Šé‡
                self.models["unet"].save_pretrained(final_dir)
                print(f"ğŸ’¾ LoRA æ¨¡å‹å·²ä¿å­˜: {final_dir}")
            except Exception as e:
                print(f"âš ï¸ LoRA ä¿å­˜å¤±æ•—: {e}")
                # å›é€€åˆ°ä¿å­˜å®Œæ•´æ¨¡å‹
                self._save_full_pipeline(final_dir)
        else:
            # ä¿å­˜å®Œæ•´æ¨¡å‹
            self._save_full_pipeline(final_dir)
    
    def _save_full_pipeline(self, save_dir):
        """ä¿å­˜å®Œæ•´çš„ Stable Diffusion ç®¡é“"""
        try:
            pipeline = StableDiffusionPipeline(
                vae=self.models["vae"],
                text_encoder=self.models["text_encoder"],
                tokenizer=self.models["tokenizer"],
                unet=self.accelerator.unwrap_model(self.models["unet"]) if self.accelerator else self.models["unet"],
                scheduler=DDPMScheduler.from_pretrained(
                    self.config["model_id"], subfolder="scheduler"
                ),
                safety_checker=None,
                requires_safety_checker=False
            )
            pipeline.save_pretrained(save_dir)
            print(f"ğŸ’¾ å®Œæ•´æ¨¡å‹å·²ä¿å­˜: {save_dir}")
        except Exception as e:
            print(f"âŒ å®Œæ•´æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")
            # è‡³å°‘ä¿å­˜ UNet æ¬Šé‡
            torch.save(self.models["unet"].state_dict(), 
                      os.path.join(save_dir, "unet_state_dict.pt"))
            print(f"ğŸ’¾ UNet æ¬Šé‡å·²ä¿å­˜: {save_dir}")
    
    def _plot_training_progress(self, losses: List[float]):
        """ç¹ªè£½è¨“ç·´é€²åº¦åœ–"""
        plt.figure(figsize=(12, 6))
        
        # æå¤±æ›²ç·š
        plt.subplot(1, 2, 1)
        plt.plot(losses)
        plt.title("Training Loss")
        plt.xlabel("Step")
        plt.ylabel("Loss")
        plt.grid(True)
        
        # å¹³æ»‘æå¤±æ›²ç·š
        plt.subplot(1, 2, 2)
        window_size = min(50, len(losses) // 10)
        if window_size > 1:
            smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')
            plt.plot(smoothed_losses)
            plt.title(f"Smoothed Training Loss (window={window_size})")
            plt.xlabel("Step")
            plt.ylabel("Loss")
            plt.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜åœ–è¡¨
        plot_path = os.path.join(self.config["output_dir"], "training_progress.png")
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š è¨“ç·´åœ–è¡¨å·²ä¿å­˜: {plot_path}")
    
    def create_download_package(self):
        """å‰µå»ºä¸‹è¼‰åŒ… (Colab å°ˆç”¨)"""
        if not IN_COLAB:
            print("ğŸ’» æœ¬åœ°ç’°å¢ƒï¼Œè·³éæ‰“åŒ…")
            return
            
        print("ğŸ“¦ å‰µå»ºä¸‹è¼‰åŒ…...")
        
        # æ‰“åŒ…æ¨¡å‹å’Œçµæœ
        package_name = f"fashion_ai_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        
        with zipfile.ZipFile(package_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ æ¨¡å‹æ–‡ä»¶
            model_dir = os.path.join(self.config["output_dir"], "final_model")
            if os.path.exists(model_dir):
                for root, dirs, files in os.walk(model_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        arcname = os.path.relpath(file_path, self.config["output_dir"])
                        zipf.write(file_path, arcname)
            
            # æ·»åŠ è¨“ç·´åœ–è¡¨
            plot_path = os.path.join(self.config["output_dir"], "training_progress.png")
            if os.path.exists(plot_path):
                zipf.write(plot_path, "training_progress.png")
            
            # æ·»åŠ é©—è­‰åœ–ç‰‡
            validation_dir = os.path.join(self.config["output_dir"], "validation")
            if os.path.exists(validation_dir):
                for file in os.listdir(validation_dir):
                    file_path = os.path.join(validation_dir, file)
                    zipf.write(file_path, f"validation/{file}")
        
        print(f"ğŸ“¦ æ‰“åŒ…å®Œæˆ: {package_name}")
        
        # æä¾›ä¸‹è¼‰
        files.download(package_name)

def demo_quick_test():
    """å¿«é€Ÿæ¸¬è©¦ - ä½¿ç”¨ç¯„ä¾‹æ•¸æ“š"""
    print("ğŸš€ å¿«é€Ÿæ¸¬è©¦æ¨¡å¼")
    print("=" * 40)
    
    try:
        # å‰µå»ºç°¡å–®çš„æ¸¬è©¦é…ç½®
        test_config = {
            "num_epochs": 2,
            "train_batch_size": 1,
            "save_steps": 10,
            "validation_steps": 10,
            "learning_rate": 5e-5
        }
        
        trainer = FashionSDFineTuner(test_config)
        
        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        test_prompts = [
            "a woman wearing an elegant black dress",
            "a man in casual blue jeans and white shirt",
            "person in formal business suit"
        ]
        
        print("âœ… å¿«é€Ÿæ¸¬è©¦å®Œæˆï¼Œç³»çµ±é‹è¡Œæ­£å¸¸ï¼")
        print("ğŸ’¡ ç¾åœ¨æ‚¨å¯ä»¥é‹è¡Œå®Œæ•´ç‰ˆæœ¬")
        
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        print("è«‹æª¢æŸ¥ä¾è³´å®‰è£æˆ–é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚")

def main():
    """ä¸»å‡½æ•¸ - Colab äº’å‹•å¼åŸ·è¡Œ"""
    print("ğŸ¨ Day 3: Fashion AI Training - Google Colab ç‰ˆæœ¬")
    print("=" * 60)
    
    # é¸æ“‡åŸ·è¡Œæ¨¡å¼
    print("è«‹é¸æ“‡åŸ·è¡Œæ¨¡å¼:")
    print("1. å¿«é€Ÿæ¸¬è©¦ (æª¢æŸ¥ç’°å¢ƒ)")
    print("2. å®Œæ•´è¨“ç·´ (ä¸Šå‚³åœ–ç‰‡)")
    print("3. ä½¿ç”¨ç¯„ä¾‹æ•¸æ“šè¨“ç·´")
    
    if IN_COLAB:
        # åœ¨ Colab ä¸­è‡ªå‹•é¸æ“‡æ¨¡å¼
        mode = input("è«‹è¼¸å…¥é¸é … (1/2/3ï¼Œé è¨­=1): ").strip() or "1"
    else:
        mode = "1"  # æœ¬åœ°ç’°å¢ƒé è¨­å¿«é€Ÿæ¸¬è©¦
    
    if mode == "1":
        demo_quick_test()
        return
    
    elif mode == "3":
        # ä½¿ç”¨ç¯„ä¾‹æ•¸æ“š
        print("\nğŸ¯ ä½¿ç”¨ç¯„ä¾‹æ•¸æ“šè¨“ç·´")
        trainer = FashionSDFineTuner()
        
        # å‰µå»ºç¯„ä¾‹æ•¸æ“š
        sample_captions = [
            "a woman wearing an elegant black dress",
            "a man in casual blue jeans and white shirt", 
            "person in formal business suit",
            "woman in floral summer dress",
            "man wearing leather jacket"
        ]
        
        # ç”±æ–¼æ²’æœ‰çœŸå¯¦åœ–ç‰‡ï¼Œæˆ‘å€‘è·³éå¯¦éš›è¨“ç·´
        print("ğŸ“ ç¯„ä¾‹æè¿°:")
        for i, caption in enumerate(sample_captions, 1):
            print(f"  {i}. {caption}")
        
        print("\nğŸ’¡ é€™æ˜¯ç¯„ä¾‹æ¨¡å¼ï¼Œå¦‚éœ€å®Œæ•´è¨“ç·´è«‹é¸æ“‡æ¨¡å¼ 2")
        return
    
    # å®Œæ•´è¨“ç·´æ¨¡å¼
    try:
        # åˆå§‹åŒ–è¨“ç·´å™¨
        trainer = FashionSDFineTuner()
        
        # ä¸Šå‚³åœ–ç‰‡
        print("\nğŸ“¤ æ­¥é©Ÿ 1: ä¸Šå‚³è¨“ç·´åœ–ç‰‡")
        image_paths = trainer.upload_training_images()
        
        if not image_paths:
            print("âŒ æ²’æœ‰æœ‰æ•ˆçš„åœ–ç‰‡ï¼Œè«‹é‡æ–°ä¸Šå‚³")
            return
        
        # æå–ç‰¹å¾µ
        print("\nğŸ” æ­¥é©Ÿ 2: æå–åœ–ç‰‡ç‰¹å¾µ")
        captions = trainer.extract_features_from_images(image_paths)
        
        # æº–å‚™æ•¸æ“šé›†
        print("\nğŸ“Š æ­¥é©Ÿ 3: æº–å‚™è¨“ç·´æ•¸æ“š")
        dataloader = trainer.prepare_dataset(image_paths, captions)
        
        # é–‹å§‹è¨“ç·´
        print("\nğŸš€ æ­¥é©Ÿ 4: é–‹å§‹å¾®èª¿è¨“ç·´")
        trainer.train(dataloader)
        
        # å‰µå»ºä¸‹è¼‰åŒ…
        print("\nğŸ“¦ æ­¥é©Ÿ 5: æº–å‚™ä¸‹è¼‰")
        trainer.create_download_package()
        
        print("\nğŸ‰ æ‰€æœ‰æ­¥é©Ÿå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        print("\nğŸ”§ å»ºè­°è§£æ±ºæ–¹æ¡ˆ:")
        print("1. é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚ (Runtime > Restart runtime)")
        print("2. é‡æ–°åŸ·è¡Œæ­¤è…³æœ¬")
        print("3. æª¢æŸ¥ GPU æ˜¯å¦å·²å•Ÿç”¨")

def run_colab_setup_only():
    """åªé‹è¡Œç’°å¢ƒè¨­ç½® - ç”¨æ–¼æ’é™¤æ•…éšœ"""
    print("ğŸ”§ åªé‹è¡Œç’°å¢ƒè¨­ç½®...")
    setup_success = setup_colab_environment()
    
    if setup_success:
        print("âœ… ç’°å¢ƒè¨­ç½®å®Œæˆ")
        print("ğŸ’¡ ç¾åœ¨å¯ä»¥é‡æ–°å•Ÿå‹•é‹è¡Œæ™‚ä¸¦åŸ·è¡Œå®Œæ•´è…³æœ¬")
    else:
        print("âŒ ç’°å¢ƒè¨­ç½®å¤±æ•—")
        print("è«‹æ‰‹å‹•å®‰è£ä¾è³´æˆ–è¯ç¹«æ”¯æ´")

if __name__ == "__main__":
    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "setup":
        run_colab_setup_only()
    else:
        main()
